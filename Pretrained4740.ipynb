{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igumiao/nlp_G60/blob/xiaohong/Group60__COMP90042_Project_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\OneDrive - The University of Melbourne\\25S1\\90042 Natural Language Processing\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\10762\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\10762\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\10762\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\10762\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
            "Loading ./data\\train-claims.json...\n",
            "Successfully loaded 1228 items.\n",
            "Loading ./data\\dev-claims.json...\n",
            "Successfully loaded 154 items.\n",
            "Loading ./data\\test-claims-unlabelled.json...\n",
            "Successfully loaded 153 items.\n",
            "Loading ./data\\evidence.json...\n",
            "Successfully loaded 1208827 items.\n",
            "\n",
            "=== Sample Data Structure ===\n",
            "Sample claim ID: claim-1937\n",
            "{\n",
            "  \"claim_text\": \"Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\",\n",
            "  \"claim_label\": \"DISPUTED\",\n",
            "  \"evidences\": [\n",
            "    \"evidence-442946\",\n",
            "    \"evidence-1194317\",\n",
            "    \"evidence-12171\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Sample evidence ID: evidence-0\n",
            "Evidence text: John Bennet Lawes, English entrepreneur and agricultural scientist\n",
            "Original claim: Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
            "Basic preprocessing: not only is there no scientific evidence that co2 is a pollutant higher co2 concentrations actually help ecosystems support more plant and animal life\n",
            "With stopword removal: scientific evidence co2 pollutant higher co2 concentrations actually help ecosystems support plant animal life\n",
            "With lemmatization: not only is there no scientific evidence that co2 is a pollutant higher co2 concentration actually help ecosystem support more plant and animal life\n",
            "With stemming: not onli is there no scientif evid that co2 is a pollut higher co2 concentr actual help ecosystem support more plant and anim life\n",
            "\n",
            "Original evidence: John Bennet Lawes, English entrepreneur and agricultural scientist\n",
            "Basic preprocessing: john bennet lawes english entrepreneur and agricultural scientist\n",
            "With stopword removal: john bennet lawes english entrepreneur agricultural scientist\n",
            "\n",
            "Data processing complete!\n"
          ]
        }
      ],
      "source": [
        "# 1. DataSet Processing\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_DATA_PATH = './data'  # Path to the data directory\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# File paths\n",
        "TRAIN_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'train-claims.json')\n",
        "DEV_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'dev-claims.json')\n",
        "TEST_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'test-claims-unlabelled.json')\n",
        "EVIDENCE_PATH = os.path.join(DRIVE_DATA_PATH, 'evidence.json')\n",
        "DEV_PREDICTIONS_PATH = os.path.join(DRIVE_DATA_PATH, 'dev-claims-predictions.json')\n",
        "TEST_PREDICTIONS_PATH = os.path.join(DRIVE_DATA_PATH, 'test-claims-predictions.json')\n",
        "EVAL_SCRIPT_PATH = os.path.join('eval.py')\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_json(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"Successfully loaded {len(data)} items.\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text, remove_stop_words=False, lemmatize=False, stem=False):\n",
        "    \"\"\"\n",
        "    Preprocess text with multiple options:\n",
        "    - Lowercase\n",
        "    - Remove special characters\n",
        "    - Optional: Remove stopwords\n",
        "    - Optional: Lemmatization\n",
        "    - Optional: Stemming\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove special characters and extra spaces\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords if requested\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "    \n",
        "    # Apply lemmatization if requested\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    \n",
        "    # Apply stemming if requested\n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "    \n",
        "    # Join the tokens back to a string\n",
        "    processed_text = ' '.join(tokens)\n",
        "    \n",
        "    return processed_text\n",
        "\n",
        "def basic_dataset_analysis(data_dict, name):\n",
        "    \"\"\"Basic analysis of a dataset (claims or evidence).\"\"\"\n",
        "    print(f\"\\n=== Basic Analysis of {name} Dataset ===\")\n",
        "    print(f\"Number of items: {len(data_dict)}\")\n",
        "    \n",
        "    # For claims, check label distribution\n",
        "    if name == 'Claims' and 'claim_label' in next(iter(data_dict.values())):\n",
        "        labels = [item.get('claim_label') for item in data_dict.values() if 'claim_label' in item]\n",
        "        label_counts = Counter(labels)\n",
        "        print(\"\\nLabel Distribution:\")\n",
        "        for label, count in label_counts.items():\n",
        "            print(f\"  {label}: {count} ({count/len(labels)*100:.2f}%)\")\n",
        "    \n",
        "    # Check text lengths\n",
        "    if 'claim_text' in next(iter(data_dict.values()), {}):\n",
        "        # For claims\n",
        "        text_field = 'claim_text'\n",
        "        text_lengths = [len(item[text_field].split()) for item in data_dict.values()]\n",
        "    else:\n",
        "        # For evidence\n",
        "        text_lengths = [len(text.split()) for i, text in enumerate(data_dict.values()) if i < 1000]\n",
        "        text_field = 'evidence text'\n",
        "    \n",
        "    print(f\"\\n{text_field.capitalize()} Length Statistics (in words):\")\n",
        "    print(f\"  Average length: {sum(text_lengths)/len(text_lengths):.2f}\")\n",
        "    print(f\"  Maximum length: {max(text_lengths)}\")\n",
        "    print(f\"  Minimum length: {min(text_lengths)}\")\n",
        "    \n",
        "    return text_lengths\n",
        "\n",
        "# --- 1. Load and Explore Datasets ---\n",
        "train_claims = load_json(TRAIN_CLAIMS_PATH)\n",
        "dev_claims = load_json(DEV_CLAIMS_PATH)\n",
        "test_claims = load_json(TEST_CLAIMS_PATH)\n",
        "evidence = load_json(EVIDENCE_PATH)\n",
        "\n",
        "# Print a sample claim to understand structure\n",
        "print(\"\\n=== Sample Data Structure ===\")\n",
        "sample_claim_id = next(iter(train_claims))\n",
        "print(f\"Sample claim ID: {sample_claim_id}\")\n",
        "print(json.dumps(train_claims[sample_claim_id], indent=2))\n",
        "\n",
        "# Print a sample evidence\n",
        "sample_evidence_id = next(iter(evidence))\n",
        "print(f\"\\nSample evidence ID: {sample_evidence_id}\")\n",
        "print(f\"Evidence text: {evidence[sample_evidence_id]}\")\n",
        "\n",
        "# # --- 2. Basic Dataset Analysis ---\n",
        "# train_lengths = basic_dataset_analysis(train_claims, \"Training Claims\")\n",
        "# dev_lengths = basic_dataset_analysis(dev_claims, \"Development Claims\")\n",
        "# test_lengths = basic_dataset_analysis(test_claims, \"Test Claims\")\n",
        "\n",
        "# # Sample evidence for analysis (avoid analyzing all 1.2M items)\n",
        "# sample_evidence = {k: evidence[k] for k in list(evidence.keys())[:1000]}\n",
        "# evidence_lengths = basic_dataset_analysis(sample_evidence, \"Evidence (Sample)\")\n",
        "\n",
        "# # --- 3. Create a simple visualization ---\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(train_lengths, bins=30, alpha=0.5, label='Claims')\n",
        "# plt.hist(evidence_lengths, bins=30, alpha=0.5, label='Evidence')\n",
        "# plt.xlabel('Word Count')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.title('Distribution of Text Lengths: Claims vs Evidence')\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.savefig('text_length_distribution.png')\n",
        "# plt.show()\n",
        "\n",
        "# # --- 4. Preprocessing Examples ---\n",
        "# print(\"\\n=== Preprocessing Examples ===\")\n",
        "\n",
        "# Sample a claim\n",
        "sample_claim = train_claims[sample_claim_id]['claim_text']\n",
        "print(f\"Original claim: {sample_claim}\")\n",
        "print(f\"Basic preprocessing: {preprocess_text(sample_claim)}\")\n",
        "print(f\"With stopword removal: {preprocess_text(sample_claim, remove_stop_words=True)}\")\n",
        "print(f\"With lemmatization: {preprocess_text(sample_claim, lemmatize=True)}\")\n",
        "print(f\"With stemming: {preprocess_text(sample_claim, stem=True)}\")\n",
        "\n",
        "# Sample an evidence\n",
        "sample_evidence_text = evidence[sample_evidence_id]\n",
        "print(f\"\\nOriginal evidence: {sample_evidence_text}\")\n",
        "print(f\"Basic preprocessing: {preprocess_text(sample_evidence_text)}\")\n",
        "print(f\"With stopword removal: {preprocess_text(sample_evidence_text, remove_stop_words=True)}\")\n",
        "\n",
        "print(\"\\nData processing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "# --- Global Configuration Variables ---\n",
        "# TF-IDF settings\n",
        "TFIDF_MAX_FEATURES = 20000\n",
        "TFIDF_TOP_K = 500\n",
        "\n",
        "# Model settings\n",
        "TRANSFORMER_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "TRANSFORMER_BATCH_SIZE = 32\n",
        "TRANSFORMER_MAX_LENGTH = 512\n",
        "\n",
        "# Retrieval settings\n",
        "EVIDENCE_FINAL_TOP_K = 4  # The value to use for evidence retrieval\n",
        "EXPERIMENT_WITH_MULTIPLE_K = False  # Set to True to experiment with different k values\n",
        "EVIDENCE_TOP_K_VALUES = [3, 4, 5 ,6] if EXPERIMENT_WITH_MULTIPLE_K else [EVIDENCE_FINAL_TOP_K]\n",
        "\n",
        "# Memory management\n",
        "CUDA_CACHE_CLEAR_FREQUENCY = 20  # Clear CUDA cache every N claims\n",
        "PROGRESS_REPORT_FREQUENCY = 50  # Report progress every N claims\n",
        "\n",
        "# Text Processing settings\n",
        "REMOVE_STOP_WORDS = False\n",
        "LEMMATIZE = True\n",
        "\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Retrieval "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Hugging Face Embedder Class ---\n",
        "class HuggingFaceEmbedder:\n",
        "    def __init__(self, model_name=TRANSFORMER_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Initialize the Hugging Face embedder.\n",
        "        \"\"\"\n",
        "        print(f\"Loading model {model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        \n",
        "        # Move model to GPU if available\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()  # Set model to evaluation mode\n",
        "        \n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        \"\"\"\n",
        "        Perform mean pooling on model outputs using attention mask.\n",
        "        \"\"\"\n",
        "        # Mean pooling - take average of all token embeddings\n",
        "        token_embeddings = model_output[0]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        \n",
        "    def encode(self, texts, batch_size=TRANSFORMER_BATCH_SIZE, show_progress=False):\n",
        "        \"\"\"\n",
        "        Encode texts to embeddings using the model.\n",
        "        \"\"\"\n",
        "        all_embeddings = []\n",
        "        \n",
        "        # Process texts in batches\n",
        "        iterator = range(0, len(texts), batch_size)\n",
        "        if show_progress:\n",
        "            iterator = tqdm(iterator, desc=\"Encoding texts\")\n",
        "            \n",
        "        for i in iterator:\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            \n",
        "            # Tokenize\n",
        "            encoded_input = self.tokenizer(batch_texts, padding=True, truncation=True, \n",
        "                                          max_length=TRANSFORMER_MAX_LENGTH, return_tensors='pt')\n",
        "            \n",
        "            # Move to device\n",
        "            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
        "            \n",
        "            # Get model output\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoded_input)\n",
        "            \n",
        "            # Perform pooling and get embeddings\n",
        "            batch_embeddings = self.mean_pooling(outputs, encoded_input['attention_mask'])\n",
        "            \n",
        "            # Normalize embeddings\n",
        "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
        "            \n",
        "            # Move to CPU to free up GPU memory\n",
        "            all_embeddings.append(batch_embeddings.cpu())\n",
        "            \n",
        "        # Concatenate all embeddings\n",
        "        if len(all_embeddings) > 0:\n",
        "            all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "            \n",
        "        return all_embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Two-Stage Retriever "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Two-Stage Retriever Class ---\n",
        "class TwoStageRetriever:\n",
        "    def __init__(self, evidence_corpus, tfidf_preprocessing=None, model_name=TRANSFORMER_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Initialize the two-stage retriever with TF-IDF and Hugging Face Transformers.\n",
        "        \n",
        "        Args:\n",
        "            evidence_corpus (dict): Dictionary of evidence texts\n",
        "            tfidf_preprocessing (function, optional): Function to preprocess text for TF-IDF\n",
        "            model_name (str): Name of the Hugging Face model to use\n",
        "        \"\"\"\n",
        "        self.evidence_corpus = evidence_corpus\n",
        "        self.evidence_ids = list(evidence_corpus.keys())\n",
        "        self.tfidf_preprocessing = tfidf_preprocessing if tfidf_preprocessing else preprocess_text\n",
        "        \n",
        "        # Prepare evidence texts for TF-IDF\n",
        "        print(\"Preparing evidence texts for TF-IDF...\")\n",
        "        start_time = time.time()\n",
        "        self.evidence_texts = [self.tfidf_preprocessing(evidence_corpus[eid]) for eid in tqdm(self.evidence_ids)]\n",
        "        print(f\"Evidence preparation took {time.time() - start_time:.2f} seconds\")\n",
        "        \n",
        "        # Initialize TF-IDF vectorizer\n",
        "        print(\"Fitting TF-IDF vectorizer...\")\n",
        "        start_time = time.time()\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES)\n",
        "        self.tfidf_evidence_matrix = self.tfidf_vectorizer.fit_transform(self.evidence_texts)\n",
        "        print(f\"TF-IDF fitting took {time.time() - start_time:.2f} seconds\")\n",
        "        \n",
        "        # Initialize Hugging Face embedder\n",
        "        self.embedder = HuggingFaceEmbedder(model_name)\n",
        "        \n",
        "        print(\"Two-stage retriever initialized successfully.\")\n",
        "    \n",
        "    def retrieve(self, claim, tfidf_top_k=TFIDF_TOP_K, final_top_k=EVIDENCE_FINAL_TOP_K):\n",
        "        \"\"\"\n",
        "        Retrieve evidence for a claim using the two-stage approach.\n",
        "        \n",
        "        Args:\n",
        "            claim (str): The claim text\n",
        "            tfidf_top_k (int): Number of candidates to retrieve with TF-IDF\n",
        "            final_top_k (int): Number of final candidates to return after re-ranking\n",
        "            \n",
        "        Returns:\n",
        "            list: Top k evidence IDs\n",
        "            list: Corresponding similarity scores\n",
        "        \"\"\"\n",
        "        # Stage 1: TF-IDF retrieval\n",
        "        processed_claim = self.tfidf_preprocessing(claim)\n",
        "        claim_vector = self.tfidf_vectorizer.transform([processed_claim])\n",
        "        \n",
        "        # Calculate cosine similarity with all evidence\n",
        "        tfidf_similarities = cosine_similarity(claim_vector, self.tfidf_evidence_matrix)[0]\n",
        "        \n",
        "        # Get top-k candidate indices\n",
        "        tfidf_top_indices = np.argsort(-tfidf_similarities)[:tfidf_top_k]\n",
        "        tfidf_top_evidence_ids = [self.evidence_ids[idx] for idx in tfidf_top_indices]\n",
        "        tfidf_top_evidence_texts = [self.evidence_corpus[eid] for eid in tfidf_top_evidence_ids]\n",
        "        \n",
        "        # Stage 2: Transformer embedding re-ranking\n",
        "        # Encode claim and candidates\n",
        "        with torch.no_grad():\n",
        "            claim_embedding = self.embedder.encode([claim])\n",
        "            candidate_embeddings = self.embedder.encode(tfidf_top_evidence_texts)\n",
        "        \n",
        "        # Calculate similarities\n",
        "        similarities = torch.matmul(candidate_embeddings, claim_embedding.t()).squeeze()\n",
        "            \n",
        "        # Get top-k final indices\n",
        "        top_k_indices = torch.argsort(similarities, descending=True)[:final_top_k].tolist()\n",
        "        \n",
        "        # Get final evidence IDs and scores\n",
        "        final_evidence_ids = [tfidf_top_evidence_ids[idx] for idx in top_k_indices]\n",
        "        final_scores = [similarities[idx].item() for idx in top_k_indices]\n",
        "        \n",
        "        return final_evidence_ids, final_scores\n",
        "\n",
        "    def batch_retrieve(self, claims_dict, tfidf_top_k=TFIDF_TOP_K, final_top_k=EVIDENCE_FINAL_TOP_K):\n",
        "        \"\"\"\n",
        "        Retrieve evidence for multiple claims in batch mode.\n",
        "        \n",
        "        Args:\n",
        "            claims_dict (dict): Dictionary of claims\n",
        "            tfidf_top_k (int): Number of candidates to retrieve with TF-IDF\n",
        "            final_top_k (int): Number of final candidates to return after re-ranking\n",
        "            \n",
        "        Returns:\n",
        "            dict: Dictionary mapping claim IDs to lists of evidence IDs\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        total_claims = len(claims_dict)\n",
        "        \n",
        "        print(f\"Processing {total_claims} claims...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for idx, (claim_id, claim_data) in enumerate(tqdm(claims_dict.items())):\n",
        "            claim_text = claim_data['claim_text']\n",
        "            evidence_ids, _ = self.retrieve(claim_text, tfidf_top_k, final_top_k)\n",
        "            results[claim_id] = evidence_ids\n",
        "            \n",
        "            # # Periodically clear CUDA cache to prevent memory leaks\n",
        "            # if torch.cuda.is_available() and (idx % CUDA_CACHE_CLEAR_FREQUENCY == 0):\n",
        "            #     torch.cuda.empty_cache()\n",
        "            #     gc.collect()\n",
        "            \n",
        "            # Periodically report progress\n",
        "            if (idx + 1) % PROGRESS_REPORT_FREQUENCY == 0 or (idx + 1) == total_claims:\n",
        "                elapsed = time.time() - start_time\n",
        "                claims_per_second = (idx + 1) / elapsed\n",
        "                print(f\"Processed {idx + 1}/{total_claims} claims \" \n",
        "                      f\"({(idx + 1)/total_claims*100:.1f}%) \"\n",
        "                      f\"at {claims_per_second:.2f} claims/second\")\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Total retrieval time: {total_time:.2f} seconds \"\n",
        "              f\"({total_time/60:.2f} minutes)\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "def evaluate_retrieval_fscore(retrieved_evidences, groundtruth_claims):\n",
        "    \"\"\"\n",
        "    Calculate F-score for evidence retrieval.\n",
        "    \"\"\"\n",
        "    f_scores = []\n",
        "    \n",
        "    for claim_id, claim_data in groundtruth_claims.items():\n",
        "        if claim_id in retrieved_evidences:\n",
        "            # Get retrieved and ground truth evidence sets\n",
        "            retrieved_set = set(retrieved_evidences[claim_id])\n",
        "            groundtruth_set = set(claim_data[\"evidences\"])\n",
        "            \n",
        "            # Calculate number of correct retrievals\n",
        "            correct = len(retrieved_set.intersection(groundtruth_set))\n",
        "            \n",
        "            # Calculate precision, recall, and F-score\n",
        "            precision = correct / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "            recall = correct / len(groundtruth_set) if len(groundtruth_set) > 0 else 0\n",
        "            \n",
        "            # Calculate F-score\n",
        "            fscore = 0\n",
        "            if precision > 0 and recall > 0:\n",
        "                fscore = 2 * precision * recall / (precision + recall)\n",
        "            \n",
        "            f_scores.append(fscore)\n",
        "    \n",
        "    # Return average F-score\n",
        "    return np.mean(f_scores) if f_scores else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Main Retrieval Function ---\n",
        "def run_evidence_retrieval(train_claims, dev_claims, test_claims, evidence):\n",
        "    \"\"\"\n",
        "    Run the evidence retrieval pipeline and evaluate results.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Retrieval Hyperparameters ===\")\n",
        "    print(f\"TFIDF_MAX_FEATURES: {TFIDF_MAX_FEATURES}\")\n",
        "    print(f\"TFIDF_TOP_K: {TFIDF_TOP_K}\")\n",
        "    print(f\"TRANSFORMER_MODEL: {TRANSFORMER_MODEL_NAME}\")\n",
        "    print(f\"EVIDENCE_TOP_K_VALUES: {EVIDENCE_TOP_K_VALUES}\")\n",
        "    \n",
        "    # Use your existing preprocessing function\n",
        "    def preprocess_for_retrieval(text):\n",
        "        return preprocess_text(text, remove_stop_words=REMOVE_STOP_WORDS, lemmatize=LEMMATIZE)\n",
        "    \n",
        "    # Results container\n",
        "    all_results = {}\n",
        "    \n",
        "    # Initialize the retriever (only once)\n",
        "    print(\"Initializing two-stage retriever...\")\n",
        "    start_time = time.time()\n",
        "    retriever = TwoStageRetriever(\n",
        "        evidence_corpus=evidence,\n",
        "        tfidf_preprocessing=preprocess_for_retrieval,\n",
        "        model_name=TRANSFORMER_MODEL_NAME\n",
        "    )\n",
        "    print(f\"Retriever initialization took {time.time() - start_time:.2f} seconds\")\n",
        "    \n",
        "    # Run only with selected top_k value(s)\n",
        "    for top_k in EVIDENCE_TOP_K_VALUES:\n",
        "        print(f\"\\n--- Evidence Retrieval with Top-K = {top_k} ---\")\n",
        "        \n",
        "        # Retrieve evidence for dev set\n",
        "        print(f\"Retrieving evidence for development set...\")\n",
        "        dev_evidence_results = retriever.batch_retrieve(\n",
        "            dev_claims, \n",
        "            tfidf_top_k=TFIDF_TOP_K, \n",
        "            final_top_k=top_k\n",
        "        )\n",
        "        \n",
        "        # Create dev predictions format\n",
        "        dev_predictions = {}\n",
        "        for claim_id, claim_data in dev_claims.items():\n",
        "            dev_predictions[claim_id] = {\n",
        "                \"claim_text\": claim_data[\"claim_text\"],\n",
        "                \"claim_label\": claim_data[\"claim_label\"],\n",
        "                \"evidences\": dev_evidence_results[claim_id]\n",
        "            }\n",
        "        \n",
        "        # Save dev predictions \n",
        "        dev_predictions_path = DEV_PREDICTIONS_PATH\n",
        "        if EXPERIMENT_WITH_MULTIPLE_K:\n",
        "            # If experimenting, save with specific filename\n",
        "            dev_predictions_path = os.path.join(DRIVE_DATA_PATH, f'dev-claims-predictions-top{top_k}.json')\n",
        "            \n",
        "        print(f\"Saving development predictions to {dev_predictions_path}\")\n",
        "        with open(dev_predictions_path, 'w') as f:\n",
        "            json.dump(dev_predictions, f, indent=2)\n",
        "        \n",
        "        # # Evaluate dev results\n",
        "        print(f\"Evaluating development results...\")\n",
        "        fscore = evaluate_retrieval_fscore(dev_evidence_results, dev_claims)\n",
        "        print(f\"Evidence Retrieval F-score = {fscore:.4f}\")\n",
        "        \n",
        "        # Store results\n",
        "        all_results[f\"dev_top{top_k}\"] = dev_evidence_results\n",
        "        \n",
        "        # Also retrieve for test set if this is the final top_k\n",
        "        if top_k == EVIDENCE_FINAL_TOP_K or not EXPERIMENT_WITH_MULTIPLE_K:\n",
        "            print(f\"\\nRetrieving evidence for test set...\")\n",
        "            test_evidence_results = retriever.batch_retrieve(\n",
        "                test_claims, \n",
        "                tfidf_top_k=TFIDF_TOP_K, \n",
        "                final_top_k=top_k\n",
        "            )\n",
        "            \n",
        "            # Create test predictions\n",
        "            test_predictions = {}\n",
        "            for claim_id, claim_data in test_claims.items():\n",
        "                test_predictions[claim_id] = {\n",
        "                    \"claim_text\": claim_data[\"claim_text\"],\n",
        "                    \"claim_label\": None,  # Will be filled by classification\n",
        "                    \"evidences\": test_evidence_results[claim_id]\n",
        "                }\n",
        "            \n",
        "            # Save test predictions\n",
        "            print(f\"Saving test predictions to {TEST_PREDICTIONS_PATH}\")\n",
        "            with open(TEST_PREDICTIONS_PATH, 'w') as f:\n",
        "                json.dump(test_predictions, f, indent=2)\n",
        "            \n",
        "            all_results[\"test\"] = test_evidence_results\n",
        "    \n",
        "    all_results[\"retriever\"] = retriever\n",
        "    print(\"\\nEvidence retrieval completed!\")\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running Evidence Retrieval Pipeline ===\n",
            "Running evidence retrieval pipeline...\n",
            "\n",
            "=== Retrieval Hyperparameters ===\n",
            "TFIDF_MAX_FEATURES: 20000\n",
            "TFIDF_TOP_K: 500\n",
            "TRANSFORMER_MODEL: sentence-transformers/all-MiniLM-L6-v2\n",
            "EVIDENCE_TOP_K_VALUES: [4]\n",
            "Initializing two-stage retriever...\n",
            "Preparing evidence texts for TF-IDF...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1208827/1208827 [03:33<00:00, 5673.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence preparation took 213.08 seconds\n",
            "Fitting TF-IDF vectorizer...\n",
            "TF-IDF fitting took 20.09 seconds\n",
            "Loading model sentence-transformers/all-MiniLM-L6-v2...\n",
            "Two-stage retriever initialized successfully.\n",
            "Retriever initialization took 237.93 seconds\n",
            "\n",
            "--- Evidence Retrieval with Top-K = 4 ---\n",
            "Retrieving evidence for development set...\n",
            "Processing 154 claims...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 50/154 [00:39<01:21,  1.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 50/154 claims (32.5%) at 1.26 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▍   | 100/154 [01:18<00:41,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 100/154 claims (64.9%) at 1.27 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 150/154 [01:57<00:03,  1.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 150/154 claims (97.4%) at 1.27 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [02:00<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 154/154 claims (100.0%) at 1.27 claims/second\n",
            "Total retrieval time: 120.90 seconds (2.02 minutes)\n",
            "Saving development predictions to ./data\\dev-claims-predictions.json\n",
            "Evaluating development results...\n",
            "Evidence Retrieval F-score = 0.1734\n",
            "\n",
            "Retrieving evidence for test set...\n",
            "Processing 153 claims...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 50/153 [00:38<01:17,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 50/153 claims (32.7%) at 1.28 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 100/153 [01:17<00:41,  1.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 100/153 claims (65.4%) at 1.29 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 150/153 [01:56<00:02,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 150/153 claims (98.0%) at 1.29 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 153/153 [01:58<00:00,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 153/153 claims (100.0%) at 1.29 claims/second\n",
            "Total retrieval time: 118.56 seconds (1.98 minutes)\n",
            "Saving test predictions to ./data\\test-claims-predictions.json\n",
            "\n",
            "Evidence retrieval completed!\n",
            "Saving retrieval results to ./data\\retrieval_results.pt\n",
            "\n",
            "Evidence retrieval step completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Run Evidence Retrieval Pipeline ---\n",
        "print(\"\\n=== Running Evidence Retrieval Pipeline ===\")\n",
        "\n",
        "# Check if retrieval results already exist to avoid recomputation\n",
        "retrieval_results_path = os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')\n",
        "\n",
        "if os.path.exists(retrieval_results_path) and False:  # Set to True to use cached results\n",
        "    print(f\"Loading cached retrieval results from {retrieval_results_path}\")\n",
        "    retrieval_results = torch.load(retrieval_results_path,weights_only=True)\n",
        "    print(\"Loaded retrieval results successfully.\")\n",
        "else:\n",
        "    print(\"Running evidence retrieval pipeline...\")\n",
        "    retrieval_results = run_evidence_retrieval(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims, \n",
        "        test_claims=test_claims,\n",
        "        evidence=evidence\n",
        "    )\n",
        "    \n",
        "    # Save retrieval results (excluding the retriever object which might be large)\n",
        "    results_to_save = {k: v for k, v in retrieval_results.items() if k != 'retriever'}\n",
        "    print(f\"Saving retrieval results to {retrieval_results_path}\")\n",
        "    torch.save(results_to_save, retrieval_results_path)\n",
        "\n",
        "print(\"\\nEvidence retrieval step completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Classification with \"distilbert-base-uncased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Task 2: Claim Verification Configuration ---\n",
        "# Model settings\n",
        "VERIFICATION_MODEL_NAME = \"distilbert-base-uncased\"\n",
        "VERIFICATION_BATCH_SIZE = 16\n",
        "VERIFICATION_MAX_LENGTH = 512\n",
        "VERIFICATION_LEARNING_RATE = 2e-5\n",
        "VERIFICATION_NUM_EPOCHS = 6\n",
        "VERIFICATION_NUM_LABELS = 4\n",
        "\n",
        "# Class management\n",
        "USE_CLASS_WEIGHTS = False\n",
        "USE_MIXED_EVIDENCE = False  # Whether to use both gold and retrieved evidence\n",
        "MIX_RATIO=0.5  # Ratio of gold evidence to retrieved evidence\n",
        "\n",
        "# Training phases\n",
        "PHASE1_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_phase1.pt')\n",
        "PHASE2_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_phase2.pt')\n",
        "FINAL_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_final.pt')\n",
        "\n",
        "# Label mapping\n",
        "LABEL_MAP = {\n",
        "    \"SUPPORTS\": 0,\n",
        "    \"REFUTES\": 1,\n",
        "    \"NOT_ENOUGH_INFO\": 2,\n",
        "    \"DISPUTED\": 3\n",
        "}\n",
        "LABEL_MAP_REVERSE = {v: k for k, v in LABEL_MAP.items()}\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def prepare_input_sequence(\n",
        "    claim, evidence, tokenizer, max_length=VERIFICATION_MAX_LENGTH\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepare input sequence for the model by tokenizing and formatting claim + evidence.\n",
        "    Implements smart truncation to handle long evidence passages.\n",
        "    \"\"\"\n",
        "    # Tokenize the claim and evidence pair\n",
        "    if \"distilbert\" in VERIFICATION_MODEL_NAME.lower():\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text=claim,\n",
        "            text_pair=evidence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=None,  # Return Python lists\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,  # No token_type_ids for DistilBERT\n",
        "        )\n",
        "    else:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text=claim,\n",
        "            text_pair=evidence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=None,  # Return Python lists\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "\n",
        "class ClaimVerificationDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        claims,\n",
        "        evidence,\n",
        "        tokenizer,\n",
        "        max_length=VERIFICATION_MAX_LENGTH,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Dataset for claim verification.\n",
        "\n",
        "        Args:\n",
        "            claims: Dictionary of claims\n",
        "            evidence: Dictionary of evidence passages\n",
        "            tokenizer: Tokenizer for preprocessing\n",
        "            max_length: Maximum sequence length\n",
        "            use_ground_truth: Whether to use ground truth evidence\n",
        "            retrieved_evidence: Dictionary of retrieved evidence IDs (from Task 1)\n",
        "            mix_ratio: Ratio of retrieved evidence to mix in (0.0-1.0)\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "\n",
        "        print(\n",
        "            f\"Creating dataset (use_ground_truth={use_ground_truth}, mix_ratio={mix_ratio})...\"\n",
        "        )\n",
        "\n",
        "        # Set random seed for reproducibility when mixing evidence\n",
        "        random.seed(42)\n",
        "\n",
        "        # Process each claim\n",
        "        for claim_id, claim_data in tqdm(claims.items(), desc=\"Processing claims\"):\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "\n",
        "            # Skip claims without labels (e.g., test set)\n",
        "            if \"claim_label\" not in claim_data:\n",
        "                continue\n",
        "\n",
        "            label = LABEL_MAP.get(claim_data[\"claim_label\"])\n",
        "\n",
        "            # Use ground truth evidence if available and requested\n",
        "            if use_ground_truth and \"evidences\" in claim_data:\n",
        "                for ev_id in claim_data[\"evidences\"]:\n",
        "                    if ev_id in evidence:\n",
        "                        self.samples.append(\n",
        "                            {\n",
        "                                \"claim_id\": claim_id,\n",
        "                                \"claim_text\": claim_text,\n",
        "                                \"evidence_id\": ev_id,\n",
        "                                \"evidence_text\": evidence[ev_id],\n",
        "                                \"label\": label,\n",
        "                                \"is_ground_truth\": True,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "            # Mix in retrieved evidence if requested\n",
        "            if retrieved_evidence and claim_id in retrieved_evidence and mix_ratio > 0:\n",
        "                for ev_id in retrieved_evidence[claim_id]:\n",
        "                    # Skip if already included as ground truth\n",
        "                    if (\n",
        "                        use_ground_truth\n",
        "                        and \"evidences\" in claim_data\n",
        "                        and ev_id in claim_data[\"evidences\"]\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    # Only include with probability equal to mix_ratio\n",
        "                    if random.random() < mix_ratio and ev_id in evidence:\n",
        "                        self.samples.append(\n",
        "                            {\n",
        "                                \"claim_id\": claim_id,\n",
        "                                \"claim_text\": claim_text,\n",
        "                                \"evidence_id\": ev_id,\n",
        "                                \"evidence_text\": evidence[ev_id],\n",
        "                                \"label\": label,\n",
        "                                \"is_ground_truth\": False,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "        print(f\"Created dataset with {len(self.samples)} samples\")\n",
        "\n",
        "        # Print ground truth vs retrieved evidence statistics\n",
        "        ground_truth_count = sum(\n",
        "            1 for sample in self.samples if sample[\"is_ground_truth\"]\n",
        "        )\n",
        "        retrieved_count = len(self.samples) - ground_truth_count\n",
        "        print(f\"Ground truth evidence: {ground_truth_count}\")\n",
        "        print(f\"Retrieved evidence: {retrieved_count}\")\n",
        "        print(f\"Ratio of retrieved to total: {retrieved_count/len(self.samples):.2f}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Tokenize and prepare input sequence\n",
        "        encoded = prepare_input_sequence(\n",
        "            sample[\"claim_text\"],\n",
        "            sample[\"evidence_text\"],\n",
        "            self.tokenizer,\n",
        "            self.max_length,\n",
        "        )\n",
        "\n",
        "        # Convert to tensors\n",
        "        item = {\n",
        "            \"input_ids\": torch.tensor(encoded[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(encoded[\"attention_mask\"]),\n",
        "            \"label\": torch.tensor(sample[\"label\"]),\n",
        "        }\n",
        "\n",
        "        # Add token_type_ids only if not using DistilBERT\n",
        "        if \"token_type_ids\" in encoded:\n",
        "            item[\"token_type_ids\"] = torch.tensor(encoded[\"token_type_ids\"])\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClaimVerificationModel(nn.Module):\n",
        "    def __init__(self, num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Transformer-based model for claim verification.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
        "        self.evidence_attention = nn.Linear(self.transformer.config.hidden_size, 1)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Token IDs\n",
        "            attention_mask: Attention mask\n",
        "            token_type_ids: Optional token type IDs\n",
        "            labels: Optional labels for loss calculation\n",
        "            \n",
        "        Returns:\n",
        "            If labels provided: (loss, logits)\n",
        "            If labels not provided: logits\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        if 'distilbert' in VERIFICATION_MODEL_NAME.lower():\n",
        "            outputs = self.transformer(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "        else:\n",
        "            outputs = self.transformer(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "        # Get CLS token representation\n",
        "        cls_output = outputs[0][:, 0, :]  # CLS token output\n",
        "        # Calculate attention scores - apply consistently regardless of training mode\n",
        "        attention_scores = self.evidence_attention(cls_output)\n",
        "        # Apply the attention as a weighting factor\n",
        "        cls_output = cls_output * torch.sigmoid(attention_scores)\n",
        "    \n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        \n",
        "        # Calculate loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "        \n",
        "        return (loss, logits) if loss is not None else logits\n",
        "    \n",
        "    def get_attention_score(self, cls_output):\n",
        "        \"\"\"Calculate attention score for evidence weighting\"\"\"\n",
        "        return torch.sigmoid(self.evidence_attention(cls_output))\n",
        "    \n",
        "    def predict_with_evidence_set(self, claim_text, evidence_texts, tokenizer):\n",
        "        \"\"\"\n",
        "        Process multiple evidence passages for one claim and make a prediction.\n",
        "        \n",
        "        Args:\n",
        "            claim_text: The claim text\n",
        "            evidence_texts: List of evidence texts\n",
        "            tokenizer: Tokenizer for encoding\n",
        "            \n",
        "        Returns:\n",
        "            Probability distribution over the 4 classes\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        all_logits = []\n",
        "        all_attention = []\n",
        "        \n",
        "        # Default if no evidence\n",
        "        if not evidence_texts:\n",
        "            print(f\"No evidence passages provided.\")\n",
        "            return torch.tensor([0, 0, 1, 0], device=device)  # Default to NOT_ENOUGH_INFO\n",
        "        \n",
        "        # Process each evidence passage\n",
        "        for evidence_text in evidence_texts:\n",
        "            # Prepare input\n",
        "            encoded = prepare_input_sequence(claim_text, evidence_text, tokenizer, VERIFICATION_MAX_LENGTH)\n",
        "            input_ids = torch.tensor([encoded[\"input_ids\"]]).to(device)\n",
        "            attention_mask = torch.tensor([encoded[\"attention_mask\"]]).to(device)\n",
        "            \n",
        "            # Get predictions\n",
        "            with torch.no_grad():\n",
        "                if 'distilbert' in VERIFICATION_MODEL_NAME.lower():\n",
        "                    outputs = self.transformer(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    token_type_ids = torch.tensor([encoded[\"token_type_ids\"]]).to(device) if \"token_type_ids\" in encoded else None\n",
        "                    outputs = self.transformer(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        token_type_ids=token_type_ids\n",
        "                    )\n",
        "                \n",
        "                cls_output = outputs[0][:, 0, :]\n",
        "                \n",
        "                # Get attention score\n",
        "                attention_score = self.get_attention_score(cls_output)\n",
        "                \n",
        "                # Get logits\n",
        "                logits = self.classifier(self.dropout(cls_output))\n",
        "                \n",
        "                all_logits.append(logits)\n",
        "                all_attention.append(attention_score)\n",
        "        \n",
        "        # Stack results\n",
        "        stacked_logits = torch.cat(all_logits, dim=0)\n",
        "        stacked_attention = torch.cat(all_attention, dim=0)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Normalize attention\n",
        "        norm_attention = F.softmax(stacked_attention.squeeze(-1), dim=0)\n",
        "        \n",
        "        # Weighted average of logits\n",
        "        weighted_logits = torch.sum(stacked_logits * norm_attention.unsqueeze(-1), dim=0)\n",
        "        \n",
        "        # Return probabilities\n",
        "        return F.softmax(weighted_logits, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def get_loss_function(labels=None, use_class_weights=False):\n",
        "    \"\"\"Get loss function with optional class weighting.\"\"\"\n",
        "    if use_class_weights and labels is not None:\n",
        "        # Calculate class weights\n",
        "        class_counts = torch.bincount(labels)\n",
        "        class_weights = len(labels) / (len(class_counts) * class_counts)\n",
        "        class_weights = class_weights.to(device)\n",
        "        \n",
        "        print(f\"Using class weights: {class_weights}\")\n",
        "        return nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler=None):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        # Move batch to device\n",
        "        optimizer.zero_grad()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        \n",
        "        # Forward pass\n",
        "        if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "            loss, logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        else:\n",
        "            loss, logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "    \n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            \n",
        "            # Forward pass\n",
        "            if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "                loss, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            else:\n",
        "                loss, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Get predictions\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "            \n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    \n",
        "    # Option 2: Calculate F1 score (macro-averaged)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    \n",
        "    # Option 3: Calculate balanced accuracy\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "    \n",
        "    report = classification_report(all_labels, all_preds, target_names=list(LABEL_MAP.keys()), output_dict=True)\n",
        "    \n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    \n",
        "    return {\n",
        "        \"loss\": total_loss / len(dataloader),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_score\": f1,  # Added F1 score\n",
        "        \"balanced_accuracy\": balanced_acc,  # Added balanced accuracy\n",
        "        \"report\": report,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, num_epochs=VERIFICATION_NUM_EPOCHS, \n",
        "                batch_size=VERIFICATION_BATCH_SIZE, learning_rate=VERIFICATION_LEARNING_RATE, \n",
        "                use_class_weights=USE_CLASS_WEIGHTS, save_path=FINAL_MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Train the claim verification model.\n",
        "    \n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_dataset: Training dataset\n",
        "        val_dataset: Validation dataset\n",
        "        num_epochs: Number of training epochs\n",
        "        batch_size: Batch size\n",
        "        learning_rate: Learning rate\n",
        "        use_class_weights: Whether to use class weighting\n",
        "        save_path: Path to save the best model\n",
        "        \n",
        "    Returns:\n",
        "        The trained model and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    \n",
        "    # Setup optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # Get loss function\n",
        "    if use_class_weights:\n",
        "        all_labels = torch.tensor([sample[\"label\"] for i, sample in enumerate(train_dataset)])\n",
        "        loss_fn = get_loss_function(all_labels, use_class_weights=True)\n",
        "    else:\n",
        "        loss_fn = get_loss_function()\n",
        "    \n",
        "     # Initialize tracking variables\n",
        "    best_val_accuracy = 0.0\n",
        "    best_val_f1 = 0.0  # For Option 2\n",
        "    best_val_balanced_acc = 0.0  # For Option 3\n",
        "    best_epoch = 0\n",
        "    training_stats = []\n",
        "    \n",
        "      # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer)\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        \n",
        "        # Evaluate\n",
        "        val_metrics = evaluate(model, val_loader)\n",
        "        print(f\"Validation loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"F1 Score (macro): {val_metrics['f1_score']:.4f}, Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
        "        \n",
        "        # Print detailed metrics\n",
        "        print(\"\\nValidation Report:\")\n",
        "        for label, metrics in val_metrics['report'].items():\n",
        "            if label in LABEL_MAP_REVERSE:\n",
        "                print(f\"  {LABEL_MAP_REVERSE[int(label)]}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1-score']:.4f}\")\n",
        "        \n",
        "        # Save stats\n",
        "        training_stats.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_metrics[\"loss\"],\n",
        "            \"val_accuracy\": val_metrics[\"accuracy\"],\n",
        "            \"val_f1_score\": val_metrics[\"f1_score\"],  # Added F1 score\n",
        "            \"val_balanced_accuracy\": val_metrics[\"balanced_accuracy\"],  # Added balanced accuracy\n",
        "            \"val_report\": val_metrics[\"report\"]\n",
        "        })\n",
        "        \n",
        "        # Option 1 : Save best model based on accuracy\n",
        "        # if val_metrics[\"accuracy\"] > best_val_accuracy:\n",
        "        #    best_val_accuracy = val_metrics[\"accuracy\"]\n",
        "        #    best_epoch = epoch + 1\n",
        "        #    torch.save(model.state_dict(), save_path)\n",
        "        #    print(f\"New best model saved with accuracy {best_val_accuracy:.4f}\")\n",
        "        \n",
        "        # Option 2: Save best model based on F1 score\n",
        "        if val_metrics[\"f1_score\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"f1_score\"]\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), save_path)  \n",
        "            print(f\"New best model saved with F1 score {best_val_f1:.4f}\")\n",
        "        \n",
        "        # Option 3: Save best model based on balanced accuracy\n",
        "        # if val_metrics[\"balanced_accuracy\"] > best_val_balanced_acc:\n",
        "        #    best_val_balanced_acc = val_metrics[\"balanced_accuracy\"]\n",
        "        #    best_epoch = epoch + 1\n",
        "        #    torch.save(model.state_dict(), save_path, weights_only=True)\n",
        "        #    print(f\"New best model saved with balanced accuracy {best_val_balanced_acc:.4f}\")\n",
        "    \n",
        "    # Print training summary\n",
        "    if best_val_f1 > 0:  # If using Option 2\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with F1 score {best_val_f1:.4f}\")\n",
        "    elif best_val_balanced_acc > 0:  # If using Option 3\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with balanced accuracy {best_val_balanced_acc:.4f}\")\n",
        "    else:  # If using original Option 1\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with accuracy {best_val_accuracy:.4f}\")\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(save_path, weights_only=True))  # Added weights_only=True\n",
        "    \n",
        "    return model, training_stats\n",
        "\n",
        "def visualize_confusion_matrix(confusion_matrix, classes):\n",
        "    \"\"\"\n",
        "    Visualize the confusion matrix.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_verification_model(train_claims, dev_claims, evidence, retriever=None, \n",
        "                            use_mixed_evidence=USE_MIXED_EVIDENCE, \n",
        "                            use_class_weights=USE_CLASS_WEIGHTS):\n",
        "    \"\"\"\n",
        "    Train the verification model with configurable training phases.\n",
        "    \n",
        "    Args:\n",
        "        train_claims: Training claims\n",
        "        dev_claims: Development claims\n",
        "        evidence: Evidence corpus\n",
        "        retriever: Optional retriever for getting training evidence\n",
        "        use_mixed_evidence: Whether to use Phase 2 (mixed evidence training)\n",
        "        use_class_weights: Whether to use class weighting\n",
        "        \n",
        "    Returns:\n",
        "        Trained model and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    model = ClaimVerificationModel(num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Phase 1: Train on ground truth evidence\n",
        "    print(\"\\n=== Phase 1: Training on ground truth evidence ===\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = ClaimVerificationDataset(\n",
        "        claims=train_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "    \n",
        "    val_dataset = ClaimVerificationDataset(\n",
        "        claims=dev_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "    \n",
        "    # Train for Phase 1\n",
        "    model, phase1_stats = train_model(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        use_class_weights=use_class_weights,\n",
        "        save_path=PHASE1_MODEL_PATH\n",
        "    )\n",
        "    \n",
        "    phase1_accuracy = phase1_stats[-1][\"val_accuracy\"]\n",
        "    print(f\"Phase 1 Model Accuracy: {phase1_accuracy:.4f}\")\n",
        "    \n",
        "    # Phase 2: Mix ground truth and retrieved evidence (optional)\n",
        "    if use_mixed_evidence and retriever is not None:\n",
        "        print(\"\\n=== Phase 2: Training with mixed evidence ===\")\n",
        "        \n",
        "        # Get retrieved evidence for training set\n",
        "        print(\"Retrieving evidence for training claims...\")\n",
        "        train_retrieved = {}\n",
        "        for claim_id, claim_data in tqdm(train_claims.items()):\n",
        "            # Skip if no ground truth label\n",
        "            if \"claim_label\" not in claim_data:\n",
        "                continue\n",
        "            \n",
        "            # Get claim text\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "            \n",
        "            # Retrieve evidence\n",
        "            evidence_ids, _ = retriever.retrieve(claim_text)\n",
        "            train_retrieved[claim_id] = evidence_ids\n",
        "        \n",
        "        # Create mixed dataset\n",
        "        mixed_train_dataset = ClaimVerificationDataset(\n",
        "            claims=train_claims,\n",
        "            evidence=evidence,\n",
        "            tokenizer=tokenizer,\n",
        "            use_ground_truth=True,\n",
        "            retrieved_evidence=train_retrieved,\n",
        "            mix_ratio=0.5 \n",
        "        )\n",
        "        \n",
        "        # Continue training from Phase 1 model\n",
        "        model.load_state_dict(torch.load(PHASE1_MODEL_PATH,weights_only=True))\n",
        "        \n",
        "        # Train for Phase 2\n",
        "        model, phase2_stats = train_model(\n",
        "            model=model,\n",
        "            train_dataset=mixed_train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            num_epochs=2,  # Fewer epochs for Phase 2\n",
        "            use_class_weights=use_class_weights,\n",
        "            save_path=PHASE2_MODEL_PATH\n",
        "        )\n",
        "        \n",
        "        phase2_accuracy = phase2_stats[-1][\"val_accuracy\"]\n",
        "        print(f\"Phase 2 Model Accuracy: {phase2_accuracy:.4f}\")\n",
        "        print(f\"Improvement: {phase2_accuracy - phase1_accuracy:.4f}\")\n",
        "        \n",
        "        return model, {\"phase1\": phase1_stats, \"phase2\": phase2_stats}\n",
        "    \n",
        "    return model, {\"phase1\": phase1_stats}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_claims(model, claims, evidence, retrieved_evidence, tokenizer):\n",
        "    \"\"\"\n",
        "    Make predictions on claims using retrieved evidence.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained verification model\n",
        "        claims: Dictionary of claims\n",
        "        evidence: Evidence corpus\n",
        "        retrieved_evidence: Dictionary mapping claim IDs to lists of evidence IDs\n",
        "        tokenizer: Tokenizer for preprocessing\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary mapping claim IDs to predicted labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = {}\n",
        "    \n",
        "    print(f\"Making predictions for {len(claims)} claims...\")\n",
        "    for claim_id, claim_data in tqdm(claims.items()):\n",
        "        claim_text = claim_data[\"claim_text\"]\n",
        "        \n",
        "        # Get evidence texts\n",
        "        evidence_ids = retrieved_evidence.get(claim_id, [])\n",
        "        evidence_texts = [evidence[eid] for eid in evidence_ids if eid in evidence]\n",
        "        \n",
        "        # Skip if no evidence (default to NOT_ENOUGH_INFO)\n",
        "        if not evidence_texts:\n",
        "            print(f\"No evidence passages provided HOWWWWW.\")\n",
        "            predictions[claim_id] = \"NOT_ENOUGH_INFO\"\n",
        "            continue\n",
        "        \n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            probs = model.predict_with_evidence_set(claim_text, evidence_texts, tokenizer)\n",
        "        \n",
        "        # Get class with highest probability\n",
        "        label_id = torch.argmax(probs).item()\n",
        "        predictions[claim_id] = LABEL_MAP_REVERSE[label_id]\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "def update_predictions_file(predictions_path, predictions):\n",
        "    \"\"\"Update predictions file with claim labels.\"\"\"\n",
        "    # Load current predictions\n",
        "    with open(predictions_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    # Update with new predictions\n",
        "    for claim_id, label in predictions.items():\n",
        "        if claim_id in data:\n",
        "            data[claim_id][\"claim_label\"] = label\n",
        "    \n",
        "    # Save updated predictions\n",
        "    with open(predictions_path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    \n",
        "    print(f\"Updated predictions saved to {predictions_path}\")\n",
        "\n",
        "def evaluate_predictions(predictions, groundtruth):\n",
        "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Collect all predictions and labels\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    for claim_id, claim_data in groundtruth.items():\n",
        "        if claim_id in predictions:\n",
        "            true_label = claim_data[\"claim_label\"]\n",
        "            pred_label = predictions[claim_id]\n",
        "            \n",
        "            y_true.append(LABEL_MAP[true_label])\n",
        "            y_pred.append(LABEL_MAP[pred_label])\n",
        "            \n",
        "            if pred_label == true_label:\n",
        "                correct += 1\n",
        "            \n",
        "            total += 1\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    \n",
        "    # Get detailed report\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    report = classification_report(y_true, y_pred, target_names=list(LABEL_MAP.keys()), output_dict=True)\n",
        "    \n",
        "    # Get confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    for label, metrics in report.items():\n",
        "        if label in LABEL_MAP_REVERSE:\n",
        "            print(f\"  {LABEL_MAP_REVERSE[int(label)]}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1-score']:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"report\": report,\n",
        "        \"confusion_matrix\": cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found retriever in retrieval_results.\n",
            "\n",
            "=== Running Claim Verification (Task 2) ===\n",
            "\n",
            "=== Claim Verification Hyperparameters ===\n",
            "MODEL_NAME: distilbert-base-uncased\n",
            "BATCH_SIZE: 16\n",
            "LEARNING_RATE: 2e-05\n",
            "USE_CLASS_WEIGHTS: False\n",
            "USE_MIXED_EVIDENCE: False\n",
            "Loading ./data\\dev-claims-predictions.json...\n",
            "Successfully loaded 154 items.\n",
            "Loading ./data\\test-claims-predictions.json...\n",
            "Successfully loaded 153 items.\n",
            "Checking retriever functionality...\n",
            "Retriever test successful. Found 4 evidence passages.\n",
            "\n",
            "=== Phase 1: Training on ground truth evidence ===\n",
            "Creating dataset (use_ground_truth=True, mix_ratio=0.0)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing claims: 100%|██████████| 1228/1228 [00:00<00:00, 204754.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created dataset with 4122 samples\n",
            "Ground truth evidence: 4122\n",
            "Retrieved evidence: 0\n",
            "Ratio of retrieved to total: 0.00\n",
            "Creating dataset (use_ground_truth=True, mix_ratio=0.0)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing claims: 100%|██████████| 154/154 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created dataset with 491 samples\n",
            "Ground truth evidence: 491\n",
            "Retrieved evidence: 0\n",
            "Ratio of retrieved to total: 0.00\n",
            "\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [02:09<00:00,  1.99it/s, loss=0.933]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 1.0597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:04<00:00,  6.41it/s]\n",
            "d:\\OneDrive - The University of Melbourne\\25S1\\90042 Natural Language Processing\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "d:\\OneDrive - The University of Melbourne\\25S1\\90042 Natural Language Processing\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "d:\\OneDrive - The University of Melbourne\\25S1\\90042 Natural Language Processing\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.2745, Accuracy: 0.4888\n",
            "F1 Score (macro): 0.3301, Balanced Accuracy: 0.3566\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3301\n",
            "\n",
            "Epoch 2/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [02:01<00:00,  2.13it/s, loss=0.152] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.4826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:04<00:00,  6.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.6226, Accuracy: 0.4705\n",
            "F1 Score (macro): 0.3363, Balanced Accuracy: 0.3432\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3363\n",
            "\n",
            "Epoch 3/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [02:02<00:00,  2.10it/s, loss=0.0455]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.1440\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:04<00:00,  6.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 2.1120, Accuracy: 0.4399\n",
            "F1 Score (macro): 0.3616, Balanced Accuracy: 0.3828\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3616\n",
            "\n",
            "Epoch 4/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [02:10<00:00,  1.98it/s, loss=0.00993]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.0648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:04<00:00,  6.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 2.7444, Accuracy: 0.4236\n",
            "F1 Score (macro): 0.3256, Balanced Accuracy: 0.3454\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Epoch 5/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [02:10<00:00,  1.98it/s, loss=0.017]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.0324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:04<00:00,  6.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 2.6166, Accuracy: 0.4277\n",
            "F1 Score (macro): 0.3710, Balanced Accuracy: 0.3901\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3710\n",
            "\n",
            "Epoch 6/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [02:03<00:00,  2.09it/s, loss=0.00245]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.0186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:04<00:00,  7.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 2.8332, Accuracy: 0.4440\n",
            "F1 Score (macro): 0.3515, Balanced Accuracy: 0.3693\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Training complete. Best model from epoch 5 with F1 score 0.3710\n",
            "Phase 1 Model Accuracy: 0.4440\n",
            "\n",
            "Making predictions on development set...\n",
            "Making predictions for 154 claims...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [00:06<00:00, 23.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating development predictions:\n",
            "Accuracy: 0.4740\n",
            "\n",
            "Classification Report:\n",
            "Updated predictions saved to ./data\\dev-claims-predictions.json\n",
            "\n",
            "Making predictions on test set...\n",
            "Making predictions for 153 claims...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 153/153 [00:06<00:00, 23.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated predictions saved to ./data\\test-claims-predictions.json\n",
            "\n",
            "Claim verification completed!\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5EAAAMWCAYAAABoZwLfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAepZJREFUeJzs3Xt8z/X///H7e2PvHWxj2OacYY6JKFHOhyhKR6GyQoqKD0Xj4xyLTkQJYSh0Qio5lFM51PhQQlFGKksOcxrDvH5/9PP+7m3b22va9nzPbtfP5XX59Hq9nu/X67G3197eD4/H6/lyWJZlCQAAAAAAG3xMBwAAAAAAyD9IIgEAAAAAtpFEAgAAAABsI4kEAAAAANhGEgkAAAAAsI0kEgAAAABgG0kkAAAAAMA2kkgAAAAAgG0kkQAAAAAA20giAcCL/PDDD3rsscdUsWJF+fv7q0iRIrrxxhs1fvx4HT16NFfPvXXrVjVt2lShoaFyOByaMGFCjp/D4XBoxIgROX7cK4mPj5fD4ZDD4dCaNWsy7LcsS5UrV5bD4VCzZs2u6hxvvfWW4uPjs/WaNWvWZBkTAADeqpDpAAAA/5g+fbp69+6tqlWr6vnnn1eNGjV0/vx5bd68WW+//bY2btyoRYsW5dr5H3/8cZ0+fVoLFixQsWLFdN111+X4OTZu3KiyZcvm+HHtCg4O1owZMzIkimvXrtWvv/6q4ODgqz72W2+9pRIlSigmJsb2a2688UZt3LhRNWrUuOrzAgCQ10giAcALbNy4UU899ZRat26txYsXy+l0uva1bt1aAwYM0LJly3I1hh9//FE9e/ZUu3btcu0ct9xyS64d245OnTrpvffe05tvvqmQkBDX9hkzZqhhw4Y6ceJEnsRx/vx5ORwOhYSEGH9PAADILtpZAcALjB07Vg6HQ9OmTXNLIC/x8/PTXXfd5Vq/ePGixo8fr2rVqsnpdCo8PFyPPvqofv/9d7fXNWvWTLVq1VJCQoIaN26swMBARUVF6aWXXtLFixcl/V+r54ULFzRlyhRX26ckjRgxwvXf6V16zb59+1zbVq1apWbNmql48eIKCAhQ+fLldd999yklJcU1JrN21h9//FF33323ihUrJn9/f9WpU0ezZ892G3Op7XP+/PkaMmSISpcurZCQELVq1Uo///yzvTdZUufOnSVJ8+fPd207fvy4Pv74Yz3++OOZvmbkyJFq0KCBwsLCFBISohtvvFEzZsyQZVmuMdddd5127NihtWvXut6/S5XcS7HPnTtXAwYMUJkyZeR0OvXLL79kaGc9fPiwypUrp0aNGun8+fOu4+/cuVNBQUF65JFHbP+sAADkFpJIADAsLS1Nq1atUr169VSuXDlbr3nqqac0aNAgtW7dWkuWLNHo0aO1bNkyNWrUSIcPH3Ybm5SUpK5du+rhhx/WkiVL1K5dO8XGxurdd9+VJN15553auHGjJOn+++/Xxo0bXet27du3T3feeaf8/Pw0c+ZMLVu2TC+99JKCgoJ07ty5LF/3888/q1GjRtqxY4feeOMNLVy4UDVq1FBMTIzGjx+fYfzgwYO1f/9+vfPOO5o2bZr27NmjDh06KC0tzVacISEhuv/++zVz5kzXtvnz58vHx0edOnXK8mfr1auXPvjgAy1cuFD33nuvnnnmGY0ePdo1ZtGiRYqKilLdunVd79/lrcexsbH67bff9Pbbb+vTTz9VeHh4hnOVKFFCCxYsUEJCggYNGiRJSklJ0QMPPKDy5cvr7bfftvVzAgCQm2hnBQDDDh8+rJSUFFWsWNHW+J9++knTpk1T7969NWnSJNf2unXrqkGDBnr99dc1ZswY1/YjR45o6dKluvnmmyVJrVq10po1azRv3jw9+uijKlmypEqWLClJioiIuKr2yi1btujs2bN6+eWXdcMNN7i2d+nSxePrRowYoXPnzmn16tWuBPqOO+5QcnKyRo4cqV69eik0NNQ1vkaNGq7kV5J8fX314IMPKiEhwXbcjz/+uJo3b64dO3aoZs2amjlzph544IEs74ecNWuW678vXryoZs2aybIsTZw4UUOHDpXD4VDdunUVEBDgsT21UqVK+vDDD68Y36233qoxY8Zo0KBBatKkiRYvXqzExER9++23CgoKsvUzAgCQm6hEAkA+s3r1aknKMIHLzTffrOrVq+urr75y2x4ZGelKIC+pXbu29u/fn2Mx1alTR35+fnriiSc0e/Zs7d2719brVq1apZYtW2aowMbExCglJSVDRTR9S6/0z88hKVs/S9OmTVWpUiXNnDlT27dvV0JCQpatrJdibNWqlUJDQ+Xr66vChQtr2LBhOnLkiA4dOmT7vPfdd5/tsc8//7zuvPNOde7cWbNnz9akSZN0/fXX2349AAC5iSQSAAwrUaKEAgMDlZiYaGv8kSNHJEmlSpXKsK906dKu/ZcUL148wzin06kzZ85cRbSZq1Spkr788kuFh4erT58+qlSpkipVqqSJEyd6fN2RI0ey/Dku7U/v8p/l0v2j2flZHA6HHnvsMb377rt6++23FR0drcaNG2c69rvvvlObNm0k/TN77vr165WQkKAhQ4Zk+7yZ/ZyeYoyJidHZs2cVGRnJvZAAAK9CEgkAhvn6+qply5basmVLholxMnMpkTp48GCGfX/++adKlCiRY7H5+/tLklJTU922X37fpSQ1btxYn376qY4fP65NmzapYcOG6tevnxYsWJDl8YsXL57lzyEpR3+W9GJiYnT48GG9/fbbeuyxx7Ict2DBAhUuXFifffaZHnzwQTVq1Ej169e/qnNmNkFRVg4ePKg+ffqoTp06OnLkiJ577rmrOicAALmBJBIAvEBsbKwsy1LPnj0znYjm/Pnz+vTTTyVJLVq0kCS3ewMlKSEhQbt27VLLli1zLK5LM4z+8MMPbtsvxZIZX19fNWjQQG+++aYk6X//+1+WY1u2bKlVq1a5ksZL5syZo8DAwFx7/EWZMmX0/PPPq0OHDurWrVuW4xwOhwoVKiRfX1/XtjNnzmju3LkZxuZUdTctLU2dO3eWw+HQF198obi4OE2aNEkLFy7818cGACAnMLEOAHiBhg0basqUKerdu7fq1aunp556SjVr1tT58+e1detWTZs2TbVq1VKHDh1UtWpVPfHEE5o0aZJ8fHzUrl077du3T0OHDlW5cuX0n//8J8fiuuOOOxQWFqbu3btr1KhRKlSokOLj43XgwAG3cW+//bZWrVqlO++8U+XLl9fZs2ddM6C2atUqy+MPHz5cn332mZo3b65hw4YpLCxM7733nj7//HONHz/ebVKdnPbSSy9dccydd96p1157TV26dNETTzyhI0eO6JVXXsn0MSzXX3+9FixYoPfff19RUVHy9/e/qvsYhw8frq+//lorVqxQZGSkBgwYoLVr16p79+6qW7eu7QmYAADILSSRAOAlevbsqZtvvlmvv/66xo0bp6SkJBUuXFjR0dHq0qWLnn76adfYKVOmqFKlSpoxY4befPNNhYaGqm3btoqLi8v0HsirFRISomXLlqlfv356+OGHVbRoUfXo0UPt2rVTjx49XOPq1KmjFStWaPjw4UpKSlKRIkVUq1YtLVmyxHVPYWaqVq2qDRs2aPDgwerTp4/OnDmj6tWra9asWRkmDjKhRYsWmjlzpsaNG6cOHTqoTJky6tmzp8LDw9W9e3e3sSNHjtTBgwfVs2dPnTx5UhUqVHB7jqYdK1euVFxcnIYOHepWUY6Pj1fdunXVqVMnffPNN/Lz88uJHw8AgKvisNI/LRkAAAAAAA+4JxIAAAAAYBtJJAAAAADANpJIAAAAAIBtJJEAAAAAANtIIgEAAAAAtpFEAgAAAABsI4kEAAAAANhWyHQAuSGg7tNXHgTkc5s/G2c6BCBXJZ08YzoEIFfVKh1qOgQg10WEFDYdwlXxpnzizNbJpkPIgEokAAAAAMA2kkgAAAAAgG3XZDsrAAAAAFw1B7U2T3h3AAAAAAC2kUQCAAAAAGyjnRUAAAAA0nM4TEfg1ahEAgAAAABsI4kEAAAAANhGOysAAAAApMfsrB7x7gAAAAAAbKMSCQAAAADpMbGOR1QiAQAAAAC2kUQCAAAAAGwjiQQAAACA9Bw+3rP8C3FxcXI4HOrXr59rm2VZGjFihEqXLq2AgAA1a9ZMO3bsyNZxSSIBAAAA4BqTkJCgadOmqXbt2m7bx48fr9dee02TJ09WQkKCIiMj1bp1a508edL2sUkiAQAAAOAacurUKXXt2lXTp09XsWLFXNsty9KECRM0ZMgQ3XvvvapVq5Zmz56tlJQUzZs3z/bxSSIBAAAAID2Hw2uW1NRUnThxwm1JTU31GH6fPn105513qlWrVm7bExMTlZSUpDZt2ri2OZ1ONW3aVBs2bLD99pBEAgAAAICXiouLU2hoqNsSFxeX5fgFCxbof//7X6ZjkpKSJEkRERFu2yMiIlz77OA5kQAAAADgpWJjY9W/f3+3bU6nM9OxBw4cUN++fbVixQr5+/tneUzHZc/BtCwrwzZPSCIBAAAAIL1/OStqTnI6nVkmjZfbsmWLDh06pHr16rm2paWlad26dZo8ebJ+/vlnSf9UJEuVKuUac+jQoQzVSU+8590BAAAAAFy1li1bavv27dq2bZtrqV+/vrp27apt27YpKipKkZGRWrlypes1586d09q1a9WoUSPb56ESCQAAAADpZaO105sEBwerVq1abtuCgoJUvHhx1/Z+/fpp7NixqlKliqpUqaKxY8cqMDBQXbp0sX0ekkgAAAAAKCAGDhyoM2fOqHfv3jp27JgaNGigFStWKDg42PYxHJZlWbkYoxEBdZ82HQKQ6zZ/Ns50CECuSjp5xnQIQK6qVTrUdAhArosIKWw6hKsS0PAF0yG4nNn4kukQMqASCQAAAADpedHEOt6IdwcAAAAAYBtJJAAAAADANtpZAQAAACC9fDo7a16hEgkAAAAAsI0kEgAAAABgG+2sAAAAAJAes7N6xLsDAAAAALCNSiQAAAAApMfEOh5RiQQAAAAA2EYSCQAAAACwjXZWAAAAAEiPiXU84t0BAAAAANhGEgkAAAAAsI12VgAAAABIj3ZWj3h3AAAAAAC2kUQCAAAAAGyjnRUAAAAA0vNxmI7Aq1GJBAAAAADYRiUSAAAAANJjYh2PeHcAAAAAALaRRAIAAAAAbKOdFQAAAADSczCxjidUIgEAAAAAtpFEAgAAAABso50VAAAAANJjdlaPeHcAAAAAALaRRAIAAAAAbKOdFQAAAADSY3ZWj7yuErl//37t3LlTFy9eNB0KAAAAAOAyxpLI2bNna8KECW7bnnjiCUVFRen6669XrVq1dODAATPBAQAAAAAyZSyJfPvttxUaGupaX7ZsmWbNmqU5c+YoISFBRYsW1ciRI02FBwAAAKCgcvh4z+KFjN0TuXv3btWvX9+1/sknn+iuu+5S165dJUljx47VY489Zio8AAAAAEAmjKW2Z86cUUhIiGt9w4YNatKkiWs9KipKSUlJJkIDAAAAUJA5HN6zeCFjSWSFChW0ZcsWSdLhw4e1Y8cO3Xbbba79SUlJbu2uAAAAAADzjLWzPvroo+rTp4927NihVatWqVq1aqpXr55r/4YNG1SrVi1T4QEAAAAAMmEsiRw0aJBSUlK0cOFCRUZG6sMPP3Tbv379enXu3NlQdAAAAAAKLC+d0MZbGEsiv/nmGw0fPlyjR4/OdP/lSSUAAAAAwDxjKXbz5s119OhRU6cHAAAAAFwFY5VIy7JMnRoAAAAAsuals6J6C6PNvg7+cAAAAAAgXzFWiZSkoUOHKjAw0OOY1157LY+iAQAAAABcidEkcvv27fLz88tyP5VKAAAAAHmO2Vk9MppELlq0SOHh4SZDAAAAAABkg7EkkiojAAAAAK9EruKRsTqtndlZU1JS8iASAAAAAIBdxpLIWbNmKTQ0NNN9Z8+e1auvvqqoqKg8jgoAAAAA4ImxJLJz584aNWqUbrrpJjVq1EiLFy+W9E9yGRUVpddee019+/Y1FR4AAACAgsrh4z2LFzJ2T+SIESP05ptvqnXr1lq/fr0eeOABPf7441qzZo3i4uLUpUsXFS5c2FR4AAAAAIBMGEsiP/jgA8XHx+uee+7R999/r7p16+rEiRPasWOHChUyOmksAAAAACALxrK1AwcO6KabbpIk3XDDDfLz89OgQYNIIAEAAACY5aVtpN7C2Ltz/vx5+fn5udYLFy6c5UQ7AAAAAADvYLTsN2zYMAUGBkqSzp07pxdffDFDIvnaa6+ZCA0AAAAAkAljSWSTJk30888/u9YbNWqkvXv3uo1x8JBPAAAAAHmNPMQjY0nkmjVrTJ0aAAAAAHCVvGYWm8OHD8vhcKh48eKmQwEAAABQkDGxjkdG353k5GT16dNHJUqUUEREhMLDw1WiRAk9/fTTSk5ONhkaAAAAACATxiqRR48eVcOGDfXHH3+oa9euql69uizL0q5duxQfH6+vvvpKGzZsULFixUyFCAAAAAC4jLEkctSoUfLz89Ovv/6qiIiIDPvatGmjUaNG6fXXXzcUITx57vE2Gv3MXZr83mo9/8rHkqS7W9yg7vfdprrVy6lEsSJq0ClOP+z+w3CkwL9zJuW05s18S99+s1onko+pYuWqevzp51WlWk3ToQHZtmfHNq1cNE8HfvlJx48d0ROxcapzSxPX/s/mz9CWr7/UscOH5FuosMpXqqq7Hn5CFatyvSP/mjntTcVPn+K2LSysuBYvX2soIuQLTKzjkbF21sWLF+uVV17JkEBKUmRkpMaPH69FixYZiAxXUq9GeXW/t5F+2P272/bAAD9t/P5XDZ30iaHIgJz35iuj9MOWb9U3drRen/G+bqh/i0Y+/5SO/H3IdGhAtp07e0Zlr6usB3v1z3R/ROly6vREf/33jTka8NJbKh4eqUkj/qOTx4/lcaRAzqoYVVmLvljjWuIX8B0T+DeMVSIPHjyomjWz/pfNWrVqKSkpKQ8jgh1BAX6aNTZGvUfP1ws92rrtm/95giSpfKkwE6EBOS419aw2rVulF158TTVvqCdJeijmSX23fo2WL/lQXbr3MRwhkD016zVUzXoNs9x/U9M2buv3dX9WG778TH/s+1XVbqif2+EBucbX11fFS5QwHQZwzTBWiSxRooT27duX5f7ExERmavVCE2I7adnXP2r1tz9feTCQz11MS9PFi2ny8/Nz2+7ndGrXj9vMBAXkkQvnz+ub5Z8oIKiIylasbDoc4F/5/cBvuqddcz149+0aMfg5/fn7AdMhwds5fLxn8ULGKpFt27bVkCFDtHLlygxf0FJTUzV06FC1bds2i1fDhAdur6c61crptofHmw4FyBMBgUGqWqO2Ppz7jsqWj1JosTB9s2qZ9uz6UaXKlDcdHpArties18xXhutc6lmFFCuuZ0ZOUJGQoqbDAq5ajZq1NXjkWJUrX0HHjhzRnJlT1bv7w5r9/icKLVrUdHhAvmQsiRw5cqTq16+vKlWqqE+fPqpWrZokaefOnXrrrbeUmpqquXPnXvE4qampSk1NddtmXUyTw8c3V+IuqMpGFNXLz9+nDr3fVOq5C6bDAfJM39jRmvzySPV48Hb5+Pgqqko1NW7ZVnv3/GQ6NCBXRF9/o2InxOv0iWR9s+JTzRg/VANfnq7gosyWjvzpllsb/99KZalm7RvUuWM7Lfv8E3Xq2s1cYEA+ZiyJLFu2rDZs2KA+ffooNjZWlmVJkhwOh1q3bq3JkyerXLlyVzxOXFycRo4c6bbNN+ImFS51c67EXVDVrV5eEcVDtOG9ga5thQr56rYbK+nJTk0U2qCfLl60DEYI5I7IMuX04oR3dPbMGaWknFJY8ZJ6ZdQghUeWMR0akCuc/gEKL1VWKlVWFavW0vAnO2n9l5+q7f2Pmg4NyBEBAYGKqlxFvx/YbzoUeDNmZ/XIWBIpSVFRUfriiy907Ngx7dmzR5JUuXJlhYXZn5glNjZW/fu7zzIX3nhQjsYJafV3P6ve/WPctk0b+bB+TvxLr8avJIHENc8/IED+AQE6dfKEtiVs1KO9+poOCcgblqUL58+bjgLIMefOndP+fYmqXaee6VCAfMtoErl//36tWLFCFy5cUJMmTTzO1poVp9Mpp9Ppto1W1px3KiVVO3896Lbt9JlzOnr8tGt7sZBAlYssplLhoZKk6Ov+eXzLX0dO6K8jJ/M2YCCHbE3YIMuyVKbcdTr4xwHNmTpBZcpdpxZt7zIdGpBtZ8+k6O+D//d4piN//akDe3crKDhEQcGhWvbhbNW++TaFFCuh0yePa93ShTp25G/deGtzg1ED/86bE17WrY2bKTyylJKPHdWcGVN1+vQptW1/t+nQ4MUcVCI9MpZErlu3TnfccYdSUlL+CaRQIc2ePVudO3c2FRL+pTubXq/pox5xrc8d97gk6cW3l2rM1KWmwgL+lZTTp/Tu9Mk6cvgvFQkOVcPGLdSlex8VKlTYdGhAtv32y0+a8N9nXOsfz5wkSbqlRTt1fup5Jf2+X5tWfaHTJ44rKDhEFapUV/+4t1S6fJSpkIF/7e9Df2nkfwfqePIxFS0Wphq1auvtmfMUWaq06dCAfMthXboZMY81bdpUISEhmjp1qgICAhQbG6vPP/9cBw78+ymXA+o+nQMRAt5t82fjTIcA5Kqkk2dMhwDkqlqlQ02HAOS6iJD8+Y+ugffNNB2CS8rHj5sOIQNjlcjt27dr3bp1Kl36n38FevXVVzV9+nQdO3ZMxYoxAxwAAAAAM2hn9czY0yuTk5MVHh7uWg8KClJgYKCSk5NNhQQAAAAAuAKjE+vs3LlTSUlJrnXLsrRr1y6dPPl/k7DUrl3bRGgAAAAAgEwYTSJbtmypy2/JbN++vRwOhyzLksPhUFpamqHoAAAAABRIdLN6ZCyJTExMNHVqAAAAAMBVMpZEVqhQwdSpAQAAAABXydjEOikpKerTp4/KlCmj8PBwdenSRYcPHzYVDgAAAABI+md2Vm9ZvJGxJHL48OGKj4/XnXfeqYceekgrV67UU089ZSocAAAAAIANxtpZFy5cqBkzZuihhx6SJD388MO69dZblZaWJl9fX1NhAQAAACjgvLUC6C2MVSIPHDigxo0bu9ZvvvlmFSpUSH/++aepkAAAAAAAV2AsiUxLS5Ofn5/btkKFCunChQuGIgIAAAAAXImxdlbLshQTEyOn0+nadvbsWT355JMKCgpybVu4cKGJ8AAAAAAUUPm1nXXKlCmaMmWK9u3bJ0mqWbOmhg0bpnbt2kmSYmJiNHv2bLfXNGjQQJs2bcrWeYwlkd26dcuw7eGHHzYQCQAAAADkf2XLltVLL72kypUrS5Jmz56tu+++W1u3blXNmjUlSW3bttWsWbNcr7m8O9QOY0lk+sABAAAAAP9Ohw4d3NbHjBmjKVOmaNOmTa4k0ul0KjIy8l+dx1gSCQAAAADeyJvaWVNTU5Wamuq2zel0ut0WmJm0tDR9+OGHOn36tBo2bOjavmbNGoWHh6to0aJq2rSpxowZo/Dw8GzFZCyJbN68eaZ/OKGhoapatar69OmjcuXKGYgMAAAAALxDXFycRo4c6bZt+PDhGjFiRKbjt2/froYNG+rs2bMqUqSIFi1apBo1akiS2rVrpwceeEAVKlRQYmKihg4dqhYtWmjLli1XTErTc1iWZV31T/Qv/Oc//8l0e3JysrZs2aK9e/fqm2++UZ06dbJ97IC6T//L6ADvt/mzcaZDAHJV0skzpkMAclWt0qGmQwByXURIYdMhXJXQznNNh+ByKP7BbFUiz507p99++03Jycn6+OOP9c4772jt2rWuRDK9gwcPqkKFClqwYIHuvfde2zEZq0S+/vrrHvf36dNHgwcP1tKlS/MoIgAAAACQ5D3drLZaV9Pz8/NzTaxTv359JSQkaOLEiZo6dWqGsaVKlVKFChW0Z8+ebMVk7DmRV9KrVy9t3brVdBgAAAAAkG9ZlpWhknnJkSNHdODAAZUqVSpbx/TaiXUCAgJ09uxZ02EAAAAAKGC8aWKd7Bg8eLDatWuncuXK6eTJk1qwYIHWrFmjZcuW6dSpUxoxYoTuu+8+lSpVSvv27dPgwYNVokQJ3XPPPdk6j9cmkStWrFB0dLTpMAAAAAAgX/jrr7/0yCOP6ODBgwoNDVXt2rW1bNkytW7dWmfOnNH27ds1Z84cJScnq1SpUmrevLnef/99BQcHZ+s8xpLIJUuWZLr9+PHjSkhI0IwZMxQfH5+3QQEAAABAPjVjxows9wUEBGj58uU5ch5jSWTHjh0z3R4cHKxq1aopPj5eDzzwQN4GBQAAAKDAy6/trHnFWBJ58eJFU6cGAAAAAFwlY7Ozfvvtt/riiy/cts2ZM0cVK1ZUeHi4nnjiiSxnEQIAAAAAmGEsiRw+fLh++OEH1/r27dvVvXt3tWrVSi+88II+/fRTxcXFmQoPAAAAQAHlcDi8ZvFGxpLI77//Xi1btnStL1iwQA0aNND06dPVv39/vfHGG/rggw9MhQcAAAAAyISxJPLYsWOKiIhwra9du1Zt27Z1rd900006cOCAidAAAAAAAFkwlkRGREQoMTFRknTu3Dn973//U8OGDV37T548qcKFC5sKDwAAAEABZbqFlXbWLLRt21YvvPCCvv76a8XGxiowMFCNGzd27f/hhx9UqVIlU+EBAAAAADJh7BEfL774ou699141bdpURYoU0ezZs+Xn5+faP3PmTLVp08ZUeAAAAACATBhLIkuWLKmvv/5ax48fV5EiReTr6+u2/8MPP1SRIkUMRQcAAACgwPLOLlKvYSyJvCQ0NDTT7WFhYXkcCQAAAADgSownkQAAAADgTbx1QhtvYWxiHQAAAABA/kMSCQAAAACwjXZWAAAAAEiHdlbPqEQCAAAAAGwjiQQAAAAA2EY7KwAAAACkQzurZ1QiAQAAAAC2kUQCAAAAAGyjnRUAAAAA0qOb1SMqkQAAAAAA26hEAgAAAEA6TKzjGZVIAAAAAIBtJJEAAAAAANtoZwUAAACAdGhn9YxKJAAAAADANpJIAAAAAIBttLMCAAAAQDq0s3pGJRIAAAAAYBtJJAAAAADANtpZAQAAACAd2lk9oxIJAAAAALCNSiQAAAAApEch0iMqkQAAAAAA20giAQAAAAC20c4KAAAAAOkwsY5nVCIBAAAAALaRRAIAAAAAbKOdFQAAAADSoZ3VMyqRAAAAAADbSCIBAAAAALbRzgoAAAAA6dDO6hmVSAAAAACAbVQiAQAAACA9CpEeUYkEAAAAANhGEgkAAAAAsI12VgAAAABIh4l1PKMSCQAAAACwjSQSAAAAAGAb7awAAAAAkA7trJ5RiQQAAAAA2EYSCQAAAACwjXZWAAAAAEiHdlbPqEQCAAAAAGyjEgkAAAAA6VCJ9IxKJAAAAADANpJIAAAAAIBttLMCAAAAQHp0s3pEJRIAAAAAYBtJJAAAAADAtmuynfVYwmTTIQC5bv0vh02HAOSqyOAA0yEAuSrQ6Ws6BABZYHZWz6hEAgAAAABsI4kEAAAAANh2TbazAgAAAMDVop3VMyqRAAAAAADbqEQCAAAAQDoUIj2jEgkAAAAAsI0kEgAAAABgG+2sAAAAAJAOE+t4RiUSAAAAAGAbSSQAAAAAwDbaWQEAAAAgHbpZPaMSCQAAAACwjSQSAAAAAGAb7awAAAAAkA6zs3pGJRIAAAAAYBuVSAAAAABIh0KkZ1QiAQAAAAC2kUQCAAAAwDVgypQpql27tkJCQhQSEqKGDRvqiy++cO23LEsjRoxQ6dKlFRAQoGbNmmnHjh3ZPg9JJAAAAACk4+Pj8JolO8qWLauXXnpJmzdv1ubNm9WiRQvdfffdrkRx/Pjxeu211zR58mQlJCQoMjJSrVu31smTJ7P3/mRrNAAAAADAK3Xo0EF33HGHoqOjFR0drTFjxqhIkSLatGmTLMvShAkTNGTIEN17772qVauWZs+erZSUFM2bNy9b5yGJBAAAAIBrTFpamhYsWKDTp0+rYcOGSkxMVFJSktq0aeMa43Q61bRpU23YsCFbx2Z2VgAAAABIx5tmZ01NTVVqaqrbNqfTKafTmen47du3q2HDhjp79qyKFCmiRYsWqUaNGq5EMSIiwm18RESE9u/fn62YqEQCAAAAgJeKi4tTaGio2xIXF5fl+KpVq2rbtm3atGmTnnrqKXXr1k07d+507XdcliFblpVh25VQiQQAAAAALxUbG6v+/fu7bcuqCilJfn5+qly5siSpfv36SkhI0MSJEzVo0CBJUlJSkkqVKuUaf+jQoQzVySuhEgkAAAAA6TgcDq9ZnE6n65EdlxZPSeTlLMtSamqqKlasqMjISK1cudK179y5c1q7dq0aNWqUrfeHSiQAAAAAXAMGDx6sdu3aqVy5cjp58qQWLFigNWvWaNmyZXI4HOrXr5/Gjh2rKlWqqEqVKho7dqwCAwPVpUuXbJ2HJBIAAAAArgF//fWXHnnkER08eFChoaGqXbu2li1bptatW0uSBg4cqDNnzqh37946duyYGjRooBUrVig4ODhb53FYlmXlxg9g0tkLpiMAct/6Xw6bDgHIVZHBAaZDAHJV+RJc47j2BTvz591z1w9deeVBeWT76NamQ8ggf/6pAgAAAACMoJ0VAAAAANLJ7iMvChoqkQAAAAAA20giAQAAAAC20c4KAAAAAOnQzuoZlUgAAAAAgG0kkQAAAAAA22hnBQAAAIB06Gb1jEokAAAAAMA2kkgAAAAAgG20swIAAABAOszO6hmVSAAAAACAbVQiAQAAACAdCpGeUYkEAAAAANhGEgkAAAAAsI12VgAAAABIh4l1PKMSCQAAAACwjSQSAAAAAGAb7awAAAAAkA7drJ5RiQQAAAAA2EYSCQAAAACwjXZWAAAAAEiH2Vk9oxIJAAAAALCNSiQAAAAApEMh0jMqkQAAAAAA20giAQAAAAC20c4KAAAAAOkwsY5nVCIBAAAAALaRRAIAAAAAbKOdFQAAAADSoZvVMyqRAAAAAADbvDKJvHDhgk6dOmU6DAAAAADAZYwmkUuXLtXcuXPdto0ZM0ZFihRR0aJF1aZNGx07dsxQdAAAAAAKIofD4TWLNzKaRL7yyis6ceKEa33Dhg0aNmyYhg4dqg8++EAHDhzQ6NGjDUYIAAAAAEjP6MQ6P/74o1599VXX+kcffaTWrVtryJAhkiR/f3/17dtXr732mqkQAQAAABQwXloA9BpGK5EnT55U8eLFXevffPONWrRo4VqvWbOm/vzzTxOhAQAAAAAyYTSJLF26tHbt2iVJOnXqlL7//nvdeuutrv1HjhxRYGCgqfAAAAAAAJcx2s56//33q1+/fho8eLCWLl2qyMhI3XLLLa79mzdvVtWqVQ1GCAAAAKCg8dYJbbyF0SRy+PDh+vPPP/Xss88qMjJS7777rnx9fV3758+frw4dOhiMEAAAAACQntEkMjAwMMMjPtJbvXp1HkYDAAAAALgSo/dEHjp0yOP+tLQ0fffdd3kUDQAAAAD8MzurtyzeyGgSWapUKbdEsnr16vrtt99c64cPH1bDhg1NhAYAAAAAyITRJNKyLLf133//XRcuXPA4BgAAAABgjtF7Iu1gZiQAAAAAeYkcxDOjlUgAAAAAQP5itBLpcDh08uRJ+fv7y7IsORwOnTp1SidOnJAk1/8DAAAAQF6hEumZ0STSsixFR0e7rdetW9dtnT9AAAAAAPAeRpNIngMJAAAAAPmL0SRy//796tSpk5xOp8kwAAAAAMCFZkjPjE6s89hjj+n48eMmQwAAAAAAZINXPScSAAAAAODdjD8nkolzAAAAAHgTchTPjCeRMTExV7wncuHChXkUDQAAAADAE+NJZHBwsAICAkyHgaswY/pUfbVyhRIT98rp7686deqqX//ndF3FKNOhAVdlz45tWrlong788pOOHzuiJ2LjVOeWJq79n82foS1ff6ljhw/Jt1Bhla9UVXc9/IQqVq1pMGrg3zmTclrzZr6lb79ZrRPJx1SxclU9/vTzqlKN6xrXho/en6+PPligg3/+IUmKqlRZPXr11q2Nm1zhlQCyYjyJfOONNxQeHm46DFyFzQnfqVPnrqp5/fVKu5CmSW+8rid7dtfCJZ8rMDDQdHhAtp07e0Zlr6ushi3v0PSXhmTYH1G6nDo90V8lIkvr3LlUrfrkfU0a8R+NfPt9BYcWMxAx8O+9+cooHUj8VX1jRyusREmtXblUI59/ShNnfqTiJfn7GflfeESknu7XX+XKlZckfbbkEw3o+7Te++BjVapcxXB08FZ0s3pmNImk1zh/mzJthtv6qBfj1LxxQ+3auUP16t9kKCrg6tWs11A16zXMcv9NTdu4rd/X/Vlt+PIz/bHvV1W7oX5uhwfkuNTUs9q0bpVeePE11byhniTpoZgn9d36NVq+5EN16d7HcITAv9ekWXO39T7P9tPHHyzQ9h++J4kErpLRJJLZWa8tp06elCSFhIYajgTIfRfOn9c3yz9RQFARla1Y2XQ4wFW5mJamixfT5Ofn57bdz+nUrh+3mQkKyEVpaWn6csUynTmToto31DEdDrwYxS7PjCaRq1evVlhYmMkQkEMsy9Ir4+NU98Z6qlIl2nQ4QK7ZnrBeM18ZrnOpZxVSrLieGTlBRUKKmg4LuCoBgUGqWqO2Ppz7jsqWj1JosTB9s2qZ9uz6UaXKlDcdHpBjftm9W4890lnnzqUqIDBQL0+YpKhK/AMgcLWMJpFr167V2rVrM2wPDQ1V1apV1aZNG/n4eH6UZWpqqlJTU922Wb7OK874ipwV9+Io7dm9W/Fz55kOBchV0dffqNgJ8Tp9IlnfrPhUM8YP1cCXpyu4KPdEIn/qGztak18eqR4P3i4fH19FVammxi3bau+en0yHBuSYChWv07wPF+rkyZNa9eUKjfhvrKbNnEMiCVwlo0nkokWLMt2enJysP/74QzVr1tTy5cs9TrwTFxenkSNHum0bMnS4/jtsRE6GCg/ixozWmjWrNHP2u4qIjDQdDpCrnP4BCi9VVipVVhWr1tLwJztp/Zefqu39j5oODbgqkWXK6cUJ7+jsmTNKSTmlsOIl9cqoQQqPLGM6NCDHFC7sp3LlK0iSatSspZ0/btf89+ZqyLCRV3glCiq6WT0zmkRu3bo1y30HDx5Uly5dNHjwYL3zzjtZjouNjVX//v3dtlm+VCHzgmVZihszWqu+WqkZ8XNVtmw50yEBec+ydOH8edNRAP+af0CA/AMCdOrkCW1L2KhHe/U1HRKQayxLOn/unOkwgHzL+CM+slKqVCm9+OKLeuSRRzyOczoztq6evZCbkeGSsaNH6ouln2nCpLcUFBikw3//LUkqEhwsf39/w9EB2Xf2TIr+Pvi7a/3IX3/qwN7dCgoOUVBwqJZ9OFu1b75NIcVK6PTJ41q3dKGOHflbN97a3MNRAe+2NWGDLMtSmXLX6eAfBzRn6gSVKXedWrS9y3RoQI54c+LranRbY0VEllLK6dNavmyptmz+Tm9MmWY6NCDf8tokUpLKlCmjQ4cOmQ4DWfjg/fmSpO4x7on+qBfjdPc995oICfhXfvvlJ0347zOu9Y9nTpIk3dKinTo/9bySft+vTau+0OkTxxUUHKIKVaqrf9xbKl0+ylTIwL+WcvqU3p0+WUcO/6UiwaFq2LiFunTvo0KFCpsODcgRR44e1rAhg3T4779VpEiwqkRH640p03RLw1tNhwYv5kM/q0cOy4ufs/HJJ59oyJAh+vHHH7P1OiqRKAjW/3LYdAhArooMDjAdApCrypfgGse1L9jpeZJMb9V68ibTIbisfPoW0yFkYLQSeeLEiUy3Hz9+XAkJCRowYIB69OiRx1EBAAAAALJiNIksWrRolg/ydDgc6tWrlwYOHJjHUQEAAAAoyOhm9cxoErlq1apMk8iQkBBVqVJFRYoUMRAVAAAAACArRpPIZs2amTw9AAAAACCbjN7p+uijj+rkyZOu9e+//17ned4aAAAAAIMcDofXLN7IaBL53nvv6cyZM671xo0b68CBAwYjAgAAAAB4YrSd9fKni3jx00YAAAAAFBA+3lkA9Br588EtAAAAAAAjjFYiJWnnzp1KSkqS9E8l8qefftKpU6fcxtSuXdtEaAAAAACAyxhPIlu2bOnWxtq+fXtJ/9zMalmWHA6H0tLSTIUHAAAAoIDx1gltvIXRJDIxMdHk6QEAAAAA2WQ0iaxQoYLJ0wMAAAAAssnoxDrjx493e8THunXrlJqa6lo/efKkevfubSI0AAAAAAWUw+E9izcymkTGxsbq5MmTrvX27dvrjz/+cK2npKRo6tSpJkIDAAAAAGTCaBLJcyIBAAAAIH8xPjsrAAAAAHgTh7y0j9RLGK1EAgAAAAByRlxcnG666SYFBwcrPDxcHTt21M8//+w2JiYmRg6Hw2255ZZbsnUe45XId955R0WKFJEkXbhwQfHx8SpRooQkud0vCQAAAAB5wSefFiLXrl2rPn366KabbtKFCxc0ZMgQtWnTRjt37lRQUJBrXNu2bTVr1izXup+fX7bOYzSJLF++vKZPn+5aj4yM1Ny5czOMAQAAAAB4tmzZMrf1WbNmKTw8XFu2bFGTJk1c251OpyIjI6/6PEaTyH379l1xTPrZWgEAAACgIElNTXV7DKL0TxLodDqv+Nrjx49LksLCwty2r1mzRuHh4SpatKiaNm2qMWPGKDw83HZMXntPZFJSkp599llVrlzZdCgAAAAACpDL7xk0ucTFxSk0NNRtiYuLu+LPYFmW+vfvr9tuu021atVybW/Xrp3ee+89rVq1Sq+++qoSEhLUokWLDImqJ0aTyOTkZHXt2lUlS5ZU6dKl9cYbb+jixYsaNmyYoqKitHHjRs2cOdNkiAAAAABgTGxsrI4fP+62xMbGXvF1Tz/9tH744QfNnz/fbXunTp105513qlatWurQoYO++OIL7d69W59//rntmIy2sw4ePFjr1q1Tt27dtGzZMv3nP//RsmXLdPbsWX3xxRdq2rSpyfAAAAAAwCi7ravpPfPMM1qyZInWrVunsmXLehxbqlQpVahQQXv27LF9fKNJ5Oeff65Zs2apVatW6t27typXrqzo6GhNmDDBZFgAAAAACjBHPp2d1bIsPfPMM1q0aJHWrFmjihUrXvE1R44c0YEDB1SqVCnb5zHazvrnn3+qRo0akqSoqCj5+/urR48eJkMCAAAAgHypT58+evfddzVv3jwFBwcrKSlJSUlJOnPmjCTp1KlTeu6557Rx40bt27dPa9asUYcOHVSiRAndc889ts9jtBJ58eJFFS5c2LXu6+vr9vwSAAAAAIA9U6ZMkSQ1a9bMbfusWbMUExMjX19fbd++XXPmzFFycrJKlSql5s2b6/3331dwcLDt8xhNIi3LUkxMjKvH9+zZs3ryySczJJILFy40ER4AAACAAsgnn/azWpblcX9AQICWL1/+r89jNIns1q2b2/rDDz9sKBIAAAAAgB1Gk8hZs2aZPD0AAAAAZJBPC5F5xujEOgAAAACA/IUkEgAAAABgm9F2VgAAAADwNg76WT2iEgkAAAAAsI0kEgAAAABgG+2sAAAAAJAO3ayeUYkEAAAAANhGEgkAAAAAsI12VgAAAABIx4d+Vo+oRAIAAAAAbKMSCQAAAADpUIf0jEokAAAAAMA2kkgAAAAAgG20swIAAABAOg4m1vGISiQAAAAAwDaSSAAAAACAbbSzAgAAAEA6PnSzekQlEgAAAABgG0kkAAAAAMA22lkBAAAAIB1mZ/WMSiQAAAAAwDYqkQAAAACQDoVIz6hEAgAAAABsI4kEAAAAANhGOysAAAAApMPEOp5RiQQAAAAA2EYSCQAAAACwjXZWAAAAAEjHh25Wj6hEAgAAAABsI4kEAAAAANhGOysAAAAApMPsrJ5RiQQAAAAA2EYlEgAAAADSoQ7pGZVIAAAAAIBtJJEAAAAAANtoZwUAAACAdHyYWMcjKpEAAAAAANtIIgEAAAAAttHOCgAAAADp0M3qGZVIAAAAAIBtJJEAAAAAANuuKomcO3eubr31VpUuXVr79++XJE2YMEGffPJJjgYHAAAAAHnN4XB4zeKNsp1ETpkyRf3799cdd9yh5ORkpaWlSZKKFi2qCRMm5HR8AAAAAAAvku0kctKkSZo+fbqGDBkiX19f1/b69etr+/btORocAAAAAOQ1h8N7Fm+U7SQyMTFRdevWzbDd6XTq9OnTORIUAAAAAMA7ZTuJrFixorZt25Zh+xdffKEaNWrkREwAAAAAAC+V7edEPv/88+rTp4/Onj0ry7L03Xffaf78+YqLi9M777yTGzECAAAAQJ7x8dY+Ui+R7STyscce04ULFzRw4EClpKSoS5cuKlOmjCZOnKiHHnooN2IEAAAAAHiJbCeRktSzZ0/17NlThw8f1sWLFxUeHp7TcQEAAAAAvNBVJZGXlChRIqfiAAAAAACvQDerZ9lOIitWrOjxoZd79+79VwEBAAAAALxXtpPIfv36ua2fP39eW7du1bJly/T888/nVFwAAAAAAC+U7SSyb9++mW5/8803tXnz5n8dEAAAAACY5KnzElfxnMistGvXTh9//HFOHQ4AAAAA4IVyLIn86KOPFBYWllOHAwAAAAB4oWy3s9atW9etvGtZlpKSkvT333/rrbfeytHgrtZnOw6aDgHIddVLhJgOAchVuw6fMB0CkKsqRQSZDgFAFnKs0naNynYS2bFjR7d1Hx8flSxZUs2aNVO1atVyKi4AAAAAgBfKVhJ54cIFXXfddbr99tsVGRmZWzEBAAAAgDFMrONZtiq1hQoV0lNPPaXU1NTcigcAAAAA4MWy3e7boEEDbd26NTdiAQAAAAB4uWzfE9m7d28NGDBAv//+u+rVq6egIPebwmvXrp1jwQEAAABAXvOhm9Uj20nk448/rgkTJqhTp06SpGeffda1z+FwyLIsORwOpaWl5XyUAAAAAACvYDuJnD17tl566SUlJibmZjwAAAAAAC9mO4m0LEuSVKFChVwLBgAAAABMo53Vs2xNrMNUtwAAAABQsGVrYp3o6OgrJpJHjx79VwEBAAAAALxXtpLIkSNHKjQ0NLdiAQAAAADj6MD0LFtJ5EMPPaTw8PDcigUAAAAA4OVsJ5Fk4wAAAAAKAibW8cz2xDqXZmcFAAAAABRctiuRFy9ezM04AAAAAAD5QLbuiQQAAACAax138nmWredEAgAAAAAKNpJIAAAAAIBttLMCAAAAQDo+9LN6RCUSAAAAAGAbSSQAAAAAwDavSCITEhLUv39/tW/fXh06dFD//v21efNm02EBAAAAKIB8vGjJjri4ON10000KDg5WeHi4OnbsqJ9//tltjGVZGjFihEqXLq2AgAA1a9ZMO3bsyNZ5jCeRAwcOVIMGDfTOO+/o999/12+//abp06erQYMGGjRokOnwAAAAACBfWLt2rfr06aNNmzZp5cqVunDhgtq0aaPTp0+7xowfP16vvfaaJk+erISEBEVGRqp169Y6efKk7fMYnVhn9uzZmjRpkt544w316tVLhQsXliSdP39eU6ZM0aBBg1SzZk09+uijJsMEAAAAUIDk13l1li1b5rY+a9YshYeHa8uWLWrSpIksy9KECRM0ZMgQ3XvvvZL+yckiIiI0b9489erVy9Z5jFYi33zzTY0dO1ZPP/20K4GUpMKFC+vZZ5/VmDFjNHnyZIMRAgAAAED+dPz4cUlSWFiYJCkxMVFJSUlq06aNa4zT6VTTpk21YcMG28c1mkTu2LFDd999d5b7O3bsmO3+XAAAAAC4VqSmpurEiRNuS2pq6hVfZ1mW+vfvr9tuu021atWSJCUlJUmSIiIi3MZGRES49tlhNIn09fXVuXPnstx//vx5+fr65mFEAAAAAAo6H4fDa5a4uDiFhoa6LXFxcVf8GZ5++mn98MMPmj9/foZ9jsv6dS3LyrDN4/tje2QuqFevnt57770s98+dO1c33nhjHkYEAAAAAN4jNjZWx48fd1tiY2M9vuaZZ57RkiVLtHr1apUtW9a1PTIyUpIyVB0PHTqUoTrpidGJdQYMGKCOHTsqNTVVAwYMcAWelJSkV199VRMmTNCiRYtMhggAAAAAxjidTjmdTltjLcvSM888o0WLFmnNmjWqWLGi2/6KFSsqMjJSK1euVN26dSVJ586d09q1azVu3DjbMRlNItu3b6/XX39dzz33nF599VWFhoZK+ucGUF9fX7388stq3769yRABAAAAFDD5dXbWPn36aN68efrkk08UHBzsqjiGhoYqICBADodD/fr109ixY1WlShVVqVJFY8eOVWBgoLp06WL7PEaTSOmfUus999yjDz/8UHv27JEkRUdH67777lO5cuUMRwcAAAAA+cOUKVMkSc2aNXPbPmvWLMXExEiSBg4cqDNnzqh37946duyYGjRooBUrVig4ONj2eRyWZVk5FbS3+Oj7g6ZDAHJd9RIhpkMActWuwydMhwDkqvY1S5kOAch1/sZLVldn2PI9pkNwGXV7FdMhZGB0Yp0mTZooOTnZtb5kyRKdOXPGXEAAAAAACjwfh/cs3shoEvnNN9+4PeLj4Ycf1sGDVBEBAAAAwFt5VYH5GuysBQAAAJDP+OTXmXXyiNFKJAAAAAAgfzFeiVy+fLnr0R4XL17UV199pR9//NFtzF133WUiNAAAAADAZYwnkd26dXNb79Wrl9u6w+FQWlpaXoYEAAAAoACjm9Uzo0nkxYsXTZ4eAAAAAJBN3BMJAAAAALDNaCVy3bp1tsY1adIklyMBAAAAgH946/MZvYXRJLJZs2ZZ7nP8/0Zkh8OhCxcu5FFEAAAAAABPjCaRx44dy3R7SkqKJk6cqDfeeENRUVF5HBUAAAAAICtGk8hLj/a45OLFi5o5c6ZGjhwpHx8fvfnmmxlmbwUAAACA3OQQ/ayeGH/ExyULFy7U4MGD9ffffys2NlbPPPOMnE6n6bAAAAAAAOkYTyLXrl2rQYMGafv27erbt68GDRqUoUIJAAAAAHmFiXU8M5pE3nHHHfrqq6/02GOPafHixYqMjDQZDgAAAADgCowmkcuWLVOhQoX0/vvv64MPPshy3NGjR/MwKgAAAABAVowmkbNmzTJ5egAAAADIgHZWz4wmkcy8CgAAAAD5i4/pAAAAAAAA+YfRSmSxYsXkcFy5Vsw9kQAAAADyip0cpSAzmkROmDDB5OkBAAAAANmUr+6JnD9/vu666y4FBQXlUkQAAAAAAE/y1T2RvXr10l9//WU6DAAAAADXMB+H9yzeKF8lkZZlmQ4BAAAAAAo0o+2sAAAAAOBtmFfHs3xViQQAAAAAmEUSCQAAAACwjXZWAAAAAEjHh35Wj/JVJbJChQoqXLiw6TAAAAAAoMDKV5XIH3/80XQIAAAAAFCgGU0iixUrJoeNUvHRo0fzIBoAAAAA8N7nM3oLo0nkhAkTXP9tWZaeeuopjRo1SuHh4eaCAgAAAABkyWgS2a1bN7f1Z555Rvfdd5+ioqIMRQQAAAAA8CRf3RMJAAAAALmNyVk9y1ezswIAAAAAzCKJBAAAAADYZrSdtX///m7r586d05gxYxQaGuq2/bXXXsvLsAAAAAAUYD6in9UTo0nk1q1b3dYbNWqkvXv3um2z8wgQAAAAAEDeMJpErl692uTpAQAAACAD6liecU8kAAAAAMA2o5XI5ORkzZ8/X0899ZQkqWvXrjpz5oxrv6+vr6ZPn66iRYsaihAAAAAAkJ7RSuT06dO1fv161/qSJUvk4+Oj0NBQhYaGavv27ZowYYK5AAEAAAAUOD4O71m8kdEk8qOPPlKXLl3cto0fP16zZs3SrFmzFBcXp08++cRQdAAAAACAyxlNIn/99VdVrlzZtV61alX5+fm51m+44Qbt2bPHRGgAAAAAgEwYvScyJSVF586dc61v3rzZbf/p06d18eLFvA4LAAAAQAHmw/SsHhmtREZFRel///tflvs3b96sihUr5mFEAAAAAABPjCaR99xzj/773/8qKSkpw76DBw9q+PDhuueeewxEBgAAAADIjNF21oEDB+rjjz9WdHS0HnnkEUVHR8vhcOinn37Su+++qzJlymjQoEEmQwQAAABQwNDN6pnRJDI4OFjr169XbGys5s+fr+TkZElS0aJF1aVLF40dO1bBwcEmQwQAAAAApGM0iZSkYsWK6e2339aUKVP0999/S5JKliwpB+m/10nc+b2+XrJAfybu1sljR9T1udGqcXNj1/6P3ozT1rXL3V5Trkp1PTlmSl6HCuSYMymnNW/mW/r2m9U6kXxMFStX1eNPP68q1WqaDg3INj7HURDNmD5VX61cocTEvXL6+6tOnbrq1/85XVcxynRo8GJMrOOZ8STyEofDofDwcNNhwINzqWdV6rpKqte8nea9OizTMVXq3Kz7ev9fC7JvocJ5FR6QK958ZZQOJP6qvrGjFVaipNauXKqRzz+liTM/UvGSfGYhf+FzHAXR5oTv1KlzV9W8/nqlXUjTpDde15M9u2vhks8VGBhoOjwgXzKaRFasWDHTimNoaKiqVq2q5557TvXr1zcQGTJTtW4DVa3bwOOYQoUKK7ho8TyKCMhdqalntWndKr3w4muqeUM9SdJDMU/qu/VrtHzJh+rSvY/hCIHs4XMcBdGUaTPc1ke9GKfmjRtq184dqlf/JkNRAfmb0SSyX79+mW5PTk5WQkKCGjZsqBUrVqh58+Z5GxiuWuLObRrbo6P8g4qoYvUb1LpzDxUJLWY6LOCqXExL08WLafLz83Pb7ud0ateP28wEBeQyPsdxrTt18qQkKSQ01HAk8GZ0s3pmNIns27evx/2jR4/WiBEjSCLziei6DVSrYTMVKxGho4eS9OX7MzRj1H/U56VpKlTY78oHALxMQGCQqtaorQ/nvqOy5aMUWixM36xapj27flSpMuVNhwfkOD7Hca2zLEuvjI9T3RvrqUqVaNPhAPmW19wTmZn7779fEydO9DgmNTVVqampbtvOn0tVYT9nboaGTNRu1ML13xHlo1SmUlW90ruTfv7fJtVs0MRgZMDV6xs7WpNfHqkeD94uHx9fRVWppsYt22rvnp9MhwbkOD7Hca2Le3GU9uzerfi580yHAuRrPqYD+Lfi4uIUGhrqtiyaMcl0WJAUUqy4ipaM0JGDv5sOBbhqkWXK6cUJ72je5+s17f2lGj9lri5cuKDwyDKmQwNyHZ/juJbEjRmtNWtWafqs2YqIjDQdDrycjxct3shb45IkffTRR6pVq5bHMbGxsTp+/Ljbck/3Z/IoQniScvK4jh85pOBiTNCA/M8/IEBhxUvq1MkT2pawUTff2tR0SECu43Mc1wLLsjT2xVH66ssVmj5ztsqWLWc6JCDfM9rO+sYbb2S6/fjx40pISNAXX3yh5cuXZzrmEqfTKafTvXW1sN/pHIsR/yf1bIqOJP3hWj92KEl/7tujwCIhCigSrFUfxKvmLU0VXDRMx/5O0sr57ygwONTtGWRAfrM1YYMsy1KZctfp4B8HNGfqBJUpd51atL3LdGhAtvE5joJo7OiR+mLpZ5ow6S0FBQbp8P9/LnmR4GD5+/sbjg7In4wmka+//nqm20NCQlStWjV98803atDA81TkyDt//PqzZoz8j2t96Zw3JUl1m96uu3v2V9KBRG1dt0JnT59ScLHiqlizjjr1Gy5nAM9gQv6VcvqU3p0+WUcO/6UiwaFq2LiFunTvo0I8Ow/5EJ/jKIg+eH++JKl7zCNu20e9GKe777nXREjIBzJ7DCH+j8OyLMt0EDnto+8Pmg4ByHXVS4SYDgHIVbsOnzAdApCr2tcsZToEINf5e/U0nlmbvfmA6RBcutX3vhZsr/pjPXz4sBwOh4oX594LAAAAAGZQh/TM+MQ6ycnJ6tOnj0qUKKGIiAiFh4erRIkSevrpp5WcnGw6PAAAAABAOkYrkUePHlXDhg31xx9/qGvXrqpevbosy9KuXbsUHx+vr776Shs2bFCxYsVMhgkAAAAA+P+MJpGjRo2Sn5+ffv31V0VERGTY16ZNG40aNSrLCXgAAAAAIKf5MLGOR0bbWRcvXqxXXnklQwIpSZGRkRo/frwWLVpkIDIAAAAAQGaMJpEHDx5UzZo1s9xfq1YtJSUl5WFEAAAAAABPjCaRJUqU0L59+7Lcn5iYyEytAAAAAPKUw4sWb2Q0iWzbtq2GDBmic+fOZdiXmpqqoUOHqm3btgYiAwAAAABkxujEOiNHjlT9+vVVpUoV9enTR9WqVZMk7dy5U2+99ZZSU1M1d+5ckyECAAAAANIxmkSWLVtWGzduVO/evRUbGyvLsiRJDodDrVu31uTJk1WuXDmTIQIAAAAoYJic1TOjSaQkVaxYUV988YWOHTumPXv2SJIqV66ssLAww5EBAAAAAC5nPIm8pFixYrr55ptNhwEAAACggHNQivTIaBL5+OOPX3GMw+HQjBkz8iAaAAAAAMCVGE0ijx07luW+tLQ0ffnll0pNTSWJBAAAAAAvYTSJXLRoUabbP/nkEw0ePFhOp1PDhg3L46gAAAAAFGRGn4OYD3jV+7N+/Xrddttt6tKli9q3b6+9e/fqhRdeMB0WAAAAAOD/84okcseOHerQoYOaNWumqlWr6ueff9a4ceNUrFgx06EBAAAAANIxmkQeOHBAjz32mOrUqaNChQrphx9+0IwZM1S2bFmTYQEAAAAowBwOh9cs3sjoPZFVq1aVw+HQgAED1KhRI+3Zs8f1rMj07rrrLgPRAQAAAAAuZzSJPHv2rCRp/PjxWY5xOBxKS0vLq5AAAAAAAB4YbWe9ePHiFRcSSAAAAAB5yeFFS3atW7dOHTp0UOnSpeVwOLR48WK3/TExMRlaZm+55ZZsncMrJtYBAAAAAPx7p0+f1g033KDJkydnOaZt27Y6ePCga1m6dGm2zmE0iezdu7dOnTrlWp87d67benJysu644w4ToQEAAAAooExPpvNvJtZp166dXnzxRd17771ZjnE6nYqMjHQtYWFh2TqH0SRy6tSpSklJca336dNHhw4dcq2npqZq+fLlJkIDAAAAAONSU1N14sQJtyU1NfVfHXPNmjUKDw9XdHS0evbs6ZaD2WE0ibQsy+M6AAAAABRkcXFxCg0NdVvi4uKu+njt2rXTe++9p1WrVunVV19VQkKCWrRoka3E1OjsrAAAAADgbbxp4pjY2Fj179/fbZvT6bzq43Xq1Mn137Vq1VL9+vVVoUIFff755x5bYNMjiQQAAAAAL+V0Ov9V0nglpUqVUoUKFbRnzx7brzGeRA4bNkyBgYGSpHPnzmnMmDEKDQ2VJLf7JQEAAAAAOevIkSM6cOCASpUqZfs1RpPIJk2a6Oeff3atN2rUSHv37s0wBgAAAADyytXMiuotTp06pV9++cW1npiYqG3btiksLExhYWEaMWKE7rvvPpUqVUr79u3T4MGDVaJECd1zzz22z2E0iVyzZo3J0wMAAADANWXz5s1q3ry5a/3S/ZTdunXTlClTtH37ds2ZM0fJyckqVaqUmjdvrvfff1/BwcG2z2G8nTU7QkJCtG3bNkVFRZkOBQAAAAC8TrNmzTw+9SInHqGYr5JIHgECAAAAILfl32bWvOFNs9cCAAAAALxcvqpEAgAAAEBuy8fz6uQJKpEAAAAAANvyVRKZn6faBQAAAIBrQb5qZ2ViHQAAAAC5zYepdTwyWomMiorSkSNHbI//4osvVKZMmVyMCAAAAADgidFK5L59+5SWlmZ7/G233ZaL0QAAAAAAriRftbMCAAAAQG5jKhbPjCeRO3fuVFJSkscxtWvXzqNoAAAAAACeGE8iW7ZsmemEOQ6HQ5ZlyeFwZKvlFQAAAACQe4wnkd9++61KlixpOgwAAAAAkCQ5mJ3VI+NJZPny5RUeHm46DAAAAACADcaTSAAAAADwJkys45nR50Q2bdpUfn5+JkMAAAAAAGSD0Urk6tWrJUlnzpzRypUrtXv3bjkcDlWpUkWtW7dWQECAyfAAAAAAAJcx3s66ZMkS9ejRQ4cPH3bbXqJECc2YMUMdOnQwFBkAAACAgsiHiXU8MtrOumHDBt1///1q0qSJ1q9fr6NHj+ro0aP65ptv1LhxY91///3auHGjyRABAAAAAOk4rMwe0phH7rjjDpUrV05Tp07NdH+vXr104MABLV26NFvH/ej7gzkRHuDVqpcIMR0CkKt2HT5hOgQgV7WvWcp0CECu8zfe93h1lu3423QILm1ret/jEI3+sW7cuFHjxo3Lcn+fPn3UtGnTPIwIAAAAQEHH7KyeGW1nPXv2rEJCsq6mhIaGKjU1NQ8jAgAAAAB4YjSJjI6O1qpVq7Lc/9VXX6ly5cp5GBEAAAAAwBOjSWRMTIyee+65TO95/PzzzzVw4EA99thjBiIDAAAAUFA5HN6zeCOj90T27dtXGzZsUPv27VW1alVVr15dkrRz507t2bNHHTt2VN++fU2GCAAAAABIx2gl0sfHRx9++KHmz5+v6Oho/fTTT/rpp59UrVo1vffee/r444/l42M0RAAAAABAOl4x6W6nTp3UqVMn02EAAAAAgBzy0j5SL2E0ifTx8ZHjCo2+DodDFy5cyKOIAAAAAACeGE0iFy1alOW+DRs2aNKkSbIsKw8jAgAAAFDQ+VCI9MhoEnn33Xdn2PbTTz8pNjZWn376qbp27arRo0cbiAwAAAAAkBmvmbXmzz//VM+ePVW7dm1duHBB27Zt0+zZs1W+fHnToQEAAAAA/j/jE+scP35cY8eO1aRJk1SnTh199dVXaty4semwAAAAABRQTKzjmdEkcvz48Ro3bpwiIyM1f/78TNtbAQAAAADew2EZnLnGx8dHAQEBatWqlXx9fbMct3Dhwmwd96PvD/7b0ACvV71EiOkQgFy16/AJ0yEAuap9zVKmQwBynb/xvsers+qnI6ZDcGlRrbjpEDIw+sf66KOPXvERHwAAAACQl0hRPDOaRMbHx5s8PQAAAAAgm7xmdlYAAAAAgPfLp13KAAAAAJA7mJ3VMyqRAAAAAADbqEQCAAAAQDo+FCI9ohIJAAAAALCNJBIAAAAAYBvtrAAAAACQDhPreEYlEgAAAABgG0kkAAAAAMA22lkBAAAAIB0H3aweUYkEAAAAANhGEgkAAAAAsI12VgAAAABIh25Wz6hEAgAAAABsoxIJAAAAAOn4MLOOR1QiAQAAAAC2kUQCAAAAAGy7JttZbykfZjoEINcF+l2Tv76AS6mi/qZDAHJVSmqa6RCAXOdfyNd0CFeFZlbPqEQCAAAAAGwjiQQAAAAA2EY/HAAAAACkRz+rR1QiAQAAAAC2kUQCAAAAAGyjnRUAAAAA0nHQz+oRlUgAAAAAgG1UIgEAAAAgHQeFSI+oRAIAAAAAbCOJBAAAAADYRjsrAAAAAKRDN6tnVCIBAAAAALaRRAIAAAAAbKOdFQAAAADSo5/VIyqRAAAAAADbSCIBAAAAALbRzgoAAAAA6TjoZ/WISiQAAAAAwDYqkQAAAACQjoNCpEdUIgEAAAAAtpFEAgAAAABso50VAAAAANKhm9UzKpEAAAAAANtIIgEAAAAAttHOCgAAAADp0c/qEZVIAAAAAIBtJJEAAAAAcI1Yt26dOnTooNKlS8vhcGjx4sVu+y3L0ogRI1S6dGkFBASoWbNm2rFjR7bOQRIJAAAAAOk4vOh/2XX69GndcMMNmjx5cqb7x48fr9dee02TJ09WQkKCIiMj1bp1a508edL++2NZlpXtyLzc78dSTYcA5LpAP25pBgAA3i0syNd0CFdl6377CVVuq1sh+Kpf63A4tGjRInXs2FHSP1XI0qVLq1+/fho0aJAkKTU1VRERERo3bpx69epl67hUIgEAAAAgHYfDe5aclJiYqKSkJLVp08a1zel0qmnTptqwYYPt41DKAAAAAAAvlZqaqtRU905Lp9Mpp9OZ7WMlJSVJkiIiIty2R0REaP/+/baPQyUSAAAAALxUXFycQkND3Za4uLh/dUzHZSVOy7IybPOESiQAAAAApONNj4mMjY1V//793bZdTRVSkiIjIyX9U5EsVaqUa/uhQ4cyVCc9oRIJAAAAAF7K6XQqJCTEbbnaJLJixYqKjIzUypUrXdvOnTuntWvXqlGjRraPQyUSAAAAAK4Rp06d0i+//OJaT0xM1LZt2xQWFqby5curX79+Gjt2rKpUqaIqVapo7NixCgwMVJcuXWyfgyQSAAAAANLzpn7WbNq8ebOaN2/uWr/UCtutWzfFx8dr4MCBOnPmjHr37q1jx46pQYMGWrFihYKD7T9KhOdEAvkUz4kEAADeLr8+J/L7A97znMgbyl39cyJzC/dEAgAAAABso5QBAAAAAOk48nM/ax6gEgkAAAAAsI0kEgAAAABgG+2sAAAAAJCOg25Wj6hEAgAAAABsoxIJAAAAAOlQiPSMSiQAAAAAwDaSSAAAAACAbbSzAgAAAEB69LN6RCUSAAAAAGAbSSQAAAAAwDbaWQEAAAAgHQf9rB5RiQQAAAAA2EYSCQAAAACwjXZWAAAAAEjHQTerR1QiAQAAAAC2UYkEAAAAgHQoRHpGJRIAAAAAYJtXVCKPHz+ulStXat++fXI4HKpYsaJatWqlkJAQ06EBAAAAANIxnkS+++67evrpp3XixAm37aGhoXr77bfVqVMnQ5EBAAAAKJDoZ/XIaDvr//73Pz322GPq2LGjtm7dqjNnziglJUWbN29Whw4d9Mgjj+j77783GSIAAAAAIB2HZVmWqZM/9thjOnXqlD788MNM999///0KCQnRzJkzs3Xc34+l5kR4gFcL9DPeSAAAAOBRWJCv6RCuyq6Dp02H4FK9VJDpEDIwWolcv369evXqleX+J598Ut98800eRgQAAACgoHN40f+8kdEk8s8//1R0dHSW+6Ojo/XHH3/kYUQAAAAAAE+MJpEpKSny9/fPcr/T6dTZs2fzMCIAAAAAgCfGb6pavny5QkNDM92XnJyct8EAAAAAKPAc3tlF6jWMTqzj43PlQqjD4VBaWlq2jsvEOigImFgHAAB4u/w6sc7PSSmmQ3CpGhloOoQMjH4LvXjxosnTAwAAAEAGFCI9M3pPJAAAAAAgfzGaRPbu3VunTp1yrc+dO9dtPTk5WXfccYeJ0AAAAAAAmTB6T6Svr68OHjyo8PBwSVJISIi2bdumqKgoSdJff/2l0qVLc08kkAnuiQQAAN4uv94Tufsv77knMjrC++6JNFqJvDx/NZjPAgAAAABs4J5IAAAAAIBt9MMBAAAAQDoO5mf1yHgSOWzYMAUG/tPne+7cOY0ZM0ahoaGSpJQU7+lFBgAAAAAYnlinWbNmcjiunOWvXr06W8dlYh0UBEysAwAAvF1+nVhnz19nTIfgUiUiwHQIGRj9FrpmzRqTpwcAAACADGzUuQo0oxPrREVF6ciRIyZDAAAAAABkg9FK5L59+7L9DEgAAAAAyE0UIj3jER8AAAAAANuMz8yxc+dOJSUleRxTu3btPIoGAAAAAOCJ8SSyZcuWymyCWIfDIcuy5HA4aHkFAAAAkHfoZ/XIeBL57bffqmTJkqbDAAAAAADYYPyeyPLly6tChQoeF3ivvw/9pbHDY9WxTWPd0fRmPfHIA9r9007TYQG5YvbMaWp4Yw29/nKc6VCAXME1jmsd1ziQM4xXIpF/nTxxQn2f6KY69W7SS6+/paLFwvTnHwdUpEiw6dCAHLdzx3Z9svBDVa5S1XQoQK7gGse1jmsc2eGgn9Ujo5XIpk2bys/Pz2QI+BcWzJ2pkhERGjh0tKrVvF6RpcvoxptuUemy5UyHBuSolJTTGjFkoF4YOlLBISGmwwFyHNc4rnVc40DOMppErl69WkWLFjUZAv6FDV+vUdXqNTVy8ADd166pej36oD5f/JHpsIAc98pLL6rRbU11c4NGpkMBcgXXOK51XONAzjKaRPr4+MjX1zfDUqxYMd1yyy1auHChyfBwBQf//F1LFn6gMuXK66UJb6v9PQ9o8uvjtGLpEtOhATlm5fKl+vmnnXrqmf+YDgXIFVzjuNZxjeNqOBzes3gjo/dELly4UI5M3pnk5GR99913evjhhzV79mw98MADWR4jNTVVqampl22TnE5njscLd9bFi4quXlM9nuorSapStbr27/1VSxZ+oDZ33GU4OuDf+yvpoF5/OU4T35rOZwquSVzjuNZxjQO5w2Fl9pBGL/Hmm29qzpw5+vbbb7McM2LECI0cOdJt238GDlH/F4bmdngFXueOt6veTbfouSH/9/4v+fh9vRs/XR98+qXByAqGQD/mxcpta1d/qRcGPCtfX1/XtrS0NDkcDvn4+Gjtpm1u+4D8hmsc1zqucfPCgvLn+7vv8FnTIbhcV8LfdAgZeHUSuWfPHt188806duxYlmMyq0T+nUIlMi+MGTZIh/5K0sSps13b3powXrt2bNek6XMNRlYwkETmvtOnTyvp4J9u28aMGKIK11XUwzE9VKlyFUORATmDaxzXOq5x80gi/z1vTCK9+lvomTNn5O/v+U1zOp0ZEsYTaalZjEZOuu+hR/Rsz0f1Xvx0NWt5u37auV2fL/5I/3lhuOnQgBwRFBSU4QuGf0CAQkKL8sUD1wSucVzruMaB3OHVSeT06dNVt25d02EgC9Vq1NLIca9rxpSJmjtzqkqVKqPe/QaqVds7TYcGAAAAXD0vndDGWxhtZ+3fv3+m248fP67Nmzfr119/1ddff53tRPL3Y1Qice2jnRUAAHi7fNvOesSL2lmL087qZuvWrZluDwkJUdu2bdW7d29VqFAhj6MCAAAAAGTFaBK5evVqk6cHAAAAgAwc9LN6ZLwfbv/+/VqxYoUuXLigpk2bqkaNGqZDAgAAAABkwWgSuW7dOt1xxx1KSUn5J5hChTR79mx17tzZZFgAAAAAgCz4mDz50KFD1bx5c/3+++86cuSIHn/8cQ0cONBkSAAAAAAKOIfDexZvZHR21rCwMK1bt061atWS9M8DYUNCQnT48GEVK1bsqo/L7KwoCJidFQAAeLv8Ojvrb0e9J58oH+Y0HUIGRiuRycnJCg8Pd60HBQUpMDBQycnJ5oICAAAAUKA5vGjxRsZLGTt37lRSUpJr3bIs7dq1SydPnnRtq127tonQAAAAAACXMdrO6uPjI4fDocxCuLTd4XAoLS0tW8elnRUFAe2sAADA2+XXdtYDXtTOWs4L21mNfgtNTEw0eXoAAAAAyMBbJ7TxFkaTyAoVKpg8PQAAAAAgm4xOrHP06FH9/vvvbtt27Nihxx57TA8++KDmzZtnKDIAAAAAQGaMJpF9+vTRa6+95lo/dOiQGjdurISEBKWmpiomJkZz5841GCEAAACAgsf0nKzePT+r0SRy06ZNuuuuu1zrc+bMUVhYmLZt26ZPPvlEY8eO1ZtvvmkwQgAAAABAekaTyKSkJFWsWNG1vmrVKt1zzz0qVOifWzXvuusu7dmzx1R4AAAAAIDLGE0iQ0JClJyc7Fr/7rvvdMstt7jWHQ6HUlO9Z3pdAAAAANc+h8N7Fm9kNIm8+eab9cYbb+jixYv66KOPdPLkSbVo0cK1f/fu3SpXrpzBCAEAAAAA6Rl9xMfo0aPVqlUrvfvuu7pw4YIGDx6sYsWKufYvWLBATZs2NRghAAAAgILGSwuAXsNoElmnTh3t2rVLGzZsUGRkpBo0aOC2/6GHHlKNGjUMRQcAAAAAuJzDsizLdBA57fdj3EeJa1+gn9F/AwIAALiisCBf0yFclT+Tz5kOwaV0UT/TIWRg9FvoG2+8YWvcs88+m8uRAAAAAMA/vHVCG29htBKZ/vEeWXE4HNq7d2+2jkslEgUBlUgAAODt8msl8uBx76lElgqlEukmMTHR5OkBAAAAANlkvJRx8eJFxcfHa+HChdq3b58cDoeioqJ033336ZFHHpGDWjIAAACAPORgflaPjD4n0rIsdejQQT169NAff/yh66+/XjVr1tS+ffsUExOje+65x2R4AAAAAIDLGE0i4+Pj9fXXX+urr77S1q1bNX/+fC1YsEDff/+9vvzyS61atUpz5swxGSIAAAAA5AsjRoyQw+FwWyIjI3P8PEaTyPnz52vw4MFq3rx5hn0tWrTQCy+8oPfee89AZAAAAAAKLIcXLdlUs2ZNHTx40LVs3749+we5AqNJ5A8//KC2bdtmub9du3b6/vvv8zAiAAAAAMi/ChUqpMjISNdSsmTJHD+H0STy6NGjioiIyHJ/RESEjh07locRAQAAAID3SE1N1YkTJ9yW1NSsH2m4Z88elS5dWhUrVtRDDz2U7ccl2mE0iUxLS1OhQllPEOvr66sLFy7kYUQAAAAACjrTHazpl7i4OIWGhrotcXFxmcbdoEEDzZkzR8uXL9f06dOVlJSkRo0a6ciRIzn59shhWZaVo0fMBh8fH7Vr105OpzPT/ampqVq2bJnS0tKyddzfj2WdmQPXikA/40/oAQAA8CgsyNd0CFflrxPnTYfgUtR5MUPl0el0ZplDpXf69GlVqlRJAwcOVP/+/XMsJqPfQrt163bFMY8++mgeRAIAAAAA//CmR9XbTRgzExQUpOuvv1579uzJ0ZiMJpGzZs0yeXoAAAAAuGalpqZq165daty4cY4e1+g9kQAAAACAnPHcc89p7dq1SkxM1Lfffqv7779fJ06csNUBmh3cVAUAAAAA6Tiu5gGNXuD3339X586ddfjwYZUsWVK33HKLNm3apAoVKuToeYxOrJNbmFgHBQET6wAAAG+XXyfW+fuk9zwhomSw933no50VAAAAAGCb96W1AAAAAGBS/uxmzTNUIgEAAAAAtpFEAgAAAABso50VAAAAANKhm9UzKpEAAAAAANuoRAIAAABAOg5KkR5RiQQAAAAA2EYSCQAAAACwjXZWAAAAAEjHwdQ6HlGJBAAAAADYRhIJAAAAALCNdlYAAAAASIfZWT2jEgkAAAAAsI0kEgAAAABgG0kkAAAAAMA2kkgAAAAAgG1MrAMAAAAA6TCxjmdUIgEAAAAAtpFEAgAAAABso50VAAAAANJxiH5WT6hEAgAAAABsI4kEAAAAANhGOysAAAAApMPsrJ5RiQQAAAAA2EYSCQAAAACwjXZWAAAAAEiHblbPqEQCAAAAAGyjEgkAAAAA6VGK9IhKJAAAAADANpJIAAAAAIBttLMCAAAAQDoO+lk9ohIJAAAAALCNJBIAAAAAYBvtrAAAAACQjoNuVo+oRAIAAAAAbCOJBAAAAADYRjsrAAAAAKRDN6tnVCIBAAAAALZRiQQAAACA9ChFekQlEgAAAABgG0kkAAAAAMA22lkBAAAAIB0H/aweUYkEAAAAANhGEgkAAAAAsI12VgAAAABIx0E3q0dUIgEAAAAAtpFEAgAAAABsc1iWZZkOAvlbamqq4uLiFBsbK6fTaTocIMdxjaMg4DrHtY5rHMg5JJH4106cOKHQ0FAdP35cISEhpsMBchzXOAoCrnNc67jGgZxDOysAAAAAwDaSSAAAAACAbSSRAAAAAADbSCLxrzmdTg0fPpyb1HHN4hpHQcB1jmsd1ziQc5hYBwAAAABgG5VIAAAAAIBtJJEAAAAAANtIIgEAAAAAtpFE5mOHDh1Sr169VL58eTmdTkVGRur222/Xxo0bJUkOh0OLFy/O8Lp+/fqpWbNmrvWYmBg5HA45HA4VLlxYUVFReu6553T69GlJ0r59+1z7HQ6HihUrpiZNmmjt2rVuxz1w4IC6d++u0qVLy8/PTxUqVFDfvn115MgRt3HNmjVzHcvPz0+VKlVSbGysUlNTFR8f73auzJY1a9YoLS1NcXFxqlatmgICAhQWFqZbbrlFs2bNytk3GV4n/fVaqFAhlS9fXk899ZSOHTvmGnPddddleu289NJLkjJe05eWhx9+WJK0Zs0aORwOJScnZzh/nTp1NGLECNcYT0t8fLzHcUlJSZKk06dPa9CgQYqKipK/v79KliypZs2a6bPPPsv9N7SAu3Q9Xbo2Llm8eLEcDodrPS0tTa+//rpq164tf39/FS1aVO3atdP69etdY9J/tmW2XHfddVeMJ6tjPPnkk64xDodD/v7+2r9/v9trO3bsqJiYGLdtdj+Xr7vuOk2YMCFDPBMmTMgQ94kTJzR06FDVrFlTAQEBKl68uG666SaNHz/e7fewWbNm6tevX4ZjxsfHq2jRold8LzIbe+nviLZt27qNS05Odv39cElm7+Ntt93m9rrPPvtMzZo1U3BwsAIDA3XTTTcpPj7eVmzInsu/a0RERKh169aaOXOmLl686Bp3+bW4detWtW/fXuHh4fL399d1112nTp066fDhw5LsfUfJ6lpM/3tu9/fX7u/opSUoKEhVqlRRTEyMtmzZkoPvKGBWIdMB4Ordd999On/+vGbPnq2oqCj99ddf+uqrr3T06NFsH6tt27aaNWuWzp8/r6+//lo9evTQ6dOnNWXKFNeYL7/8UjVr1tShQ4c0ePBg3XHHHfrxxx9VsWJF7d27Vw0bNlR0dLTmz5+vihUraseOHXr++ef1xRdfaNOmTQoLC3Mdq2fPnho1apTOnTunhIQEPfbYY5KkYcOGuX05uPfee1WrVi2NGjXKtS0sLEwjRozQtGnTNHnyZNWvX18nTpzQ5s2b3b7A4Np16Xq9cOGCdu7cqccff1zJycmaP3++a8yoUaPUs2dPt9cFBwe7rV+6pi8JCAiwHUOjRo108OBB13rfvn114sQJt3/ICA0N1bfffitJ+vnnnxUSEuJ2jPDwcEnSk08+qe+++06TJ09WjRo1dOTIEW3YsCHDF33kDn9/f40bN069evVSsWLFMuy3LEsPPfSQvvzyS7388stq2bKlTpw4oTfffFPNmjXThx9+qI4dO2rhwoU6d+6cpH+St5tvvtntGvP19bUVz6XPx/QCAwPd1h0Oh4YNG6bZs2dneZzsfi7bcfToUd122206ceKERo8erXr16snPz0+//PKL5s2bp3nz5qlPnz7ZOmZ2FSpUSF999ZVWr16t5s2bexw7a9Yst79T/Pz8XP89adIk9evXT4MGDdJbb70lPz8/ffLJJ3ryySf1448/6pVXXsm1n6GguvTZnZaWpr/++kvLli1T37599dFHH2nJkiUqVMj9a+mhQ4fUqlUrdejQQcuXL1fRokWVmJioJUuWKCUlxW2sp+8odmTn99fO7+ila+/s2bPavXu3pk2bpgYNGmjmzJl69NFH7b1hgBcjicynkpOT9c0332jNmjVq2rSpJKlChQq6+eabr+p4lyqZktSlSxetXr1aixcvdksiixcvrsjISEVGRmrq1KkqW7asVqxYoV69eqlPnz7y8/PTihUrXF/Ey5cvr7p166pSpUoaMmSI27ECAwNd5ytfvrzmzZunFStWKC4uzu2LvJ+fn9vYSz799FP17t1bDzzwgGvbDTfccFU/O/Kf9Ndr2bJl1alTpwzVg+Dg4AzXzeUuXdNXw8/Pz+21AQEBSk1NzfJ44eHhWVZfPv30U02cOFF33HGHpH/+Jb5evXpXFReyr1WrVvrll18UFxen8ePHZ9j/wQcfuL7kdujQwbV92rRpOnLkiHr06KHWrVu7JWRnz56VdHXXWGafeZd75pln9Oqrr+q5557T9ddfn+mY7H4u2zF48GD99ttv+vnnn1WmTBnX9mrVqql9+/bKiwnfg4KC9OCDD+qFF15w/SNNVooWLZrpe3ngwAENGDBA/fr109ixY13bBwwYID8/Pz377LN64IEH1KBBgxyPvyBL/9ldpkwZ3XjjjbrlllvUsmVLxcfHq0ePHm7jN2zYoBMnTuidd95xJZgVK1ZUixYtMhzb03cUO7Lz+2vndzT9tXfdddepTZs26tatm55++ml16NAh03+wAvIT2lnzqSJFiqhIkSJavHixUlNTc/z4AQEBOn/+fJb7L/2L2/nz53X06FEtX75cvXv3zlDJiYyMVNeuXfX+++9n+eXi+++/1/r161W4cGHb8UVGRmrVqlX6+++/bb8G16a9e/dq2bJl2bp+vE1kZKSWLl2qkydPmg6lQPL19dXYsWM1adIk/f777xn2z5s3T9HR0W4J5CUDBgzQkSNHtHLlyrwI1aVRo0Zq3769YmNjM93/bz+XM3Px4kW9//77evjhh90SyPTStwDnphEjRmj79u366KOPrur1H330kc6fP6/nnnsuw75evXqpSJEibp0NyD0tWrTQDTfcoIULF2bYFxkZqQsXLmjRokXZulbTf0fxJv/5z3908uTJPP+8AHIDSWQ+VahQIcXHx2v27NkqWrSobr31Vg0ePFg//PDDvz72d999p3nz5qlly5aZ7j99+rRiY2Pl6+urpk2bas+ePbIsS9WrV890fPXq1XXs2DG3hO+tt95SkSJF5HQ6VadOHf399996/vnnbcf42muv6e+//1ZkZKRq166tJ598Ul988UX2flDkW5999pmKFCmigIAAVapUSTt37tSgQYPcxgwaNMj1jy2XlvT3S0n/fBFPv3/r1q25FnPZsmXdzlW1alXXvmnTpmnDhg2ue8v+85//uN1rh9x3zz33qE6dOho+fHiGfbt37/b4+XZpTE659PmYfsmsbTUuLk7Lli3T119/nWHf1XwuX8nff/+t5ORkt2tXkurVq+eKs3Pnzlf8WdLfO3a1Spcurb59+2rIkCG6cOFCluM6d+7sdu5L8wTs3r1boaGhKlWqVIbX+Pn5KSoqKkf/TOFZtWrVtG/fvgzbb7nlFg0ePFhdunRRiRIl1K5dO7388sv666+/sjzW5d9RcoPd39HLVatWTZIy/VmB/IZ21nzsvvvu05133qmvv/5aGzdu1LJlyzR+/Hi98847GSZXuJJLX8ovXLig8+fP6+6779akSZPcxjRq1Eg+Pj5KSUlRqVKlFB8fr+uvv/6K7USX/vUw/b9Qd+3aVUOGDNGJEyc0btw4hYSE6L777rMdb40aNfTjjz9qy5Yt+uabb7Ru3Tp16NBBMTExeuedd7LxkyM/at68uaZMmaKUlBS988472r17t5555hm3Mc8//3yG34PLqyfvv/++25fscuXK5VrMX3/9tds9menv/WnSpIn27t2rTZs2af369Vq1apUmTpyokSNHaujQobkWE9yNGzdOLVq00IABA7L92pyswF36fEzv0v2z6dWoUUOPPvqoBg0apA0bNmTrHJl9Ltt1+WsWLVqkc+fOadCgQTpz5ozbvsx+loULF7q1kF6tQYMGaerUqZo5c6YefPDBTMe8/vrratWqlWs9s6QxM5Zl5VlVFZ7f7zFjxqh///5atWqVNm3apLfffltjx47VunXr3Fq5s/qOkhvs/o5e7t/83gHehiQyn/P391fr1q3VunVrDRs2TD169NDw4cMVExOj4OBgHT9+PMNrkpOTFRoa6rbt0pfywoULq3Tp0pm2Br7//vuqUaOGihYtquLFi7u2V65cWQ6HQzt37lTHjh0zvO6nn35SsWLFVKJECde20NBQVa5cWZL07rvvqmbNmpoxY4a6d+9u+2f38fHRTTfd5KrcvPvuu3rkkUc0ZMgQ2zfSI38KCgpyXT9vvPGGmjdvrpEjR2r06NGuMSVKlHCNyUq5cuUyHXNpApzjx49nuI8xs98fOypWrOhxRsrChQurcePGaty4sV544QW9+OKLGjVqlAYNGuQ2GQhyT5MmTXT77bdr8ODBbv8AER0drZ07d2b6ml27dkmSqlSpkmNxpP98vJKRI0cqOjo6w0zc2f1cDgkJueLfFyVLllTRokX1008/uY0pX768pH/uQ758RuPMfhY7X7btKFq0qGJjYzVy5Ei1b98+0zGRkZGZvpfR0dE6fvy4/vzzT5UuXdpt37lz57R3795M77tD7ti1a5fHv7eLFy+uBx54QA888IDi4uJUt25dvfLKK27Vv6y+o0ier+/LJzyzIzu/o+ld+rzgOwquBbSzXmNq1KjhejRHtWrVlJCQ4Lbfsixt2bIlQzvSpS/lFSpUyPLesnLlyqlSpUoZPpyLFy+u1q1b66233srwr9BJSUl677331KlTpyz/5a1w4cIaPHiw/vvf/2aYbS07atSoIUmunx8Fx/Dhw/XKK6/ozz//zJHjValSRT4+Phl+fw4ePKg//vgjw+9PbqhRo4YuXLjgmuABeeOll17Sp59+6lbZe+ihh7Rnzx59+umnGca/+uqrrs9AE8qVK6enn35agwcPVlpammt7dj+XM/v7QpISEhJc17uPj48efPBBvfvuu/rjjz9y8aey75lnnpGPj48mTpyYrdfdd999KlSokF599dUM+95++22dPn06Q2sucseqVau0fft2291Ilx4Ndvnf9Vl9R5H+ub43b96cYXv66zsvTJgwQSEhIW7VcSC/ohKZTx05ckQPPPCAHn/8cdWuXVvBwcHavHmzxo8fr7vvvluS9Nxzz6lbt26qVq2a2rRpozNnzmjatGn69ddfc3wK9smTJ6tRo0a6/fbb9eKLL7pNJV+mTBmNGTPG4+u7dOmiwYMH66233sp0ooPL3X///br11lvVqFEjRUZGKjExUbGxsYqOjnbdc4CCo1mzZqpZs6bGjh2ryZMnS5JOnjzpeg7jJYGBgbb+1Tk4OFi9evXSgAEDVKhQId1www36888/NWTIEFWvXl1t2rTJdoyHDh3KkBAWL15chQsXVrNmzdS5c2fVr19fxYsX186dOzV48GA1b978qv6VHFfv+uuvV9euXd3a+R966CF9+OGH6tatW4ZHfCxZskQffvihgoKCciyGlJSUDNeu0+nMcjbH2NhYTZ8+XYmJierUqZNre3Y+l/v3769bb71Vo0aN0v333y9J+vjjj7Vs2TK3hHrs2LFas2aNGjRooFGjRql+/foKCgrSDz/8oI0bN6pWrVo59j7Y4e/vr5EjR2b777Ty5ctr/Pjxeu655+Tv769HHnlEhQsX1ieffKLBgwdrwIABzMyaC1JTU5WUlOT2iI+4uDi1b98+08defPbZZ1qwYIEeeughRUdHy7Isffrpp1q6dGm2ngvdu3dvTZ48WX369NETTzyhgIAArVy5UjNmzNDcuXOz/XPY+R1NTk5WUlKSUlNTtXv3bk2dOlWLFy/WnDlzbD8nFfBqFvKls2fPWi+88IJ14403WqGhoVZgYKBVtWpV67///a+VkpLiGrdgwQKrfv36VkhIiBUeHm7dfvvt1ubNm92O1a1bN+vuu+/O8lyJiYmWJGvr1q0eY9q3b58VExNjRUZGWoULF7bKlStnPfPMM9bhw4fdxjVt2tTq27dvhtePGTPGKlmypHXy5Mkrjp02bZrVvHlzq2TJkpafn59Vvnx5KyYmxtq3b5/HGJH/ZXW9vvfee5afn5/122+/WRUqVLAkZVh69eplWZa9a/rs2bPWqFGjrOrVq1sBAQFWhQoVrJiYGOvgwYPZimv16tWZxiLJ2rhxo2VZljV27FirYcOGVlhYmOXv729FRUVZzz77bIbfHeS8zP7c9u3bZzmdTiv9X5Hnz5+3XnnlFatmzZqW0+m0QkJCrNtvv936+uuvMz2u3c/NyzVt2jTTa+X22293jZFkLVq0yO11Y8eOtSRZ3bp1y/Cz2PlctizLWrlypdW4cWOrWLFiVrFixazbbrvNWrlyZYZxycnJVmxsrFWtWjXL6XRaAQEBVu3ata2hQ4daR44ccftZMvv8njVrlhUaGmrr/bh8bGavvXDhglWjRg1LkrV69WrX9szep8t98sknVuPGja2goCDL39/fqlevnjVz5kxbsSF7unXr5rqeCxUqZJUsWdJq1aqVNXPmTCstLc01rkKFCtbrr79uWZZl/frrr1bPnj2t6OhoKyAgwCpatKh10003WbNmzXKNt/u7tnnzZuv222+3wsPDrZCQEKt+/frW/PnzMx3r6Zh2f0cvLf7+/lalSpWsbt26WVu2bLH9fgHezmFZefBQJwAAAADANYF7IgEAAAAAtpFEAgCQB77++usMz5ZLvxRENWvWzPL9eO+990yHBwDIAu2sAADkgTNnznic1fRqHhmQ3+3fv1/nz5/PdF9ERITbs1UBAN6DJBIAAAAAYBvtrAAAAAAA20giAQAAAAC2kUQCAAAAAGwjiQQAAAAA2EYSCQAwbsSIEapTp45rPSYmRh07dszzOPbt2yeHw6Ft27bl+bkBAMgvSCIBAFmKiYmRw+GQw+FQ4cKFFRUVpeeee06nT5/O1fNOnDhR8fHxtsaS+AEAkLcKmQ4AAODd2rZtq1mzZun8+fP6+uuv1aNHD50+fVpTpkxxG3f+/HkVLlw4R84ZGhqaI8cBAAA5j0okAMAjp9OpyMhIlStXTl26dFHXrl21ePFiVwvqzJkzFRUVJafTKcuydPz4cT3xxBMKDw9XSEiIWrRooe+//97tmC+99JLrYfLdu3fX2bNn3fZf3s568eJFjRs3TpUrV5bT6VT58uU1ZswYSVLFihUlSXXr1pXD4VCzZs1cr5s1a5aqV68uf39/VatWTW+99Zbbeb777jvVrVtX/v7+ql+/vrZu3ZqD7xwAANcmKpEAgGwJCAjQ+fPnJUm//PKLPvjgA3388cfy9fWVJN15550KCwvT0qVLFRoaqqlTp6ply5bavXu3wsLC9MEHH2j48OF688031bhxY82dO1dvvPGGoqKisjxnbGyspk+frtdff1233XabDh48qJ9++knSP4ngzTffrC+//FI1a9aUn5+fJGn69OkaPny4Jk+erLp162rr1q3q2bOngoKC1K1bN50+fVrt27dXixYt9O677yoxMVF9+/bN5XcPAID8jyQSAGDbd999p3nz5qlly5aSpHPnzmnu3LkqWbKkJGnVqlXavn27Dh06JKfTKUl65ZVXtHjxYn300Ud64oknNGHCBD3++OPq0aOHJOnFF1/Ul19+maEaecnJkyc1ceJETZ48Wd26dZMkVapUSbfddpskuc5dvHhxRUZGul43evRovfrqq7r33nsl/VOx3Llzp6ZOnapu3brpvffeU1pammbOnKnAwEDVrFlTv//+u5566qmcftsAALim0M4KAPDos88+U5EiReTv76+GDRuqSZMmmjRpkiSpQoUKriROkrZs2aJTp06pePHiKlKkiGtJTEzUr7/+KknatWuXGjZs6HaOy9fT27Vrl1JTU12Jqx1///23Dhw4oO7du7vF8eKLL7rFccMNNygwMNBWHAAA4B9UIgEAHjVv3lxTpkxR4cKFVbp0abfJc4KCgtzGXrx4UaVKldKaNWsyHKdo0aJXdf6AgIBsv+bixYuS/mlpbdCggdu+S223lmVdVTwAABR0JJEAAI+CgoJUuXJlW2NvvPFGJSUlqVChQrruuusyHVO9enVt2rRJjz76qGvbpk2bsjxmlSpVFBAQoK+++srVApvepXsg09LSXNsiIiJUpkwZ7d27V127ds30uDVq1NDcuXN15swZV6LqKQ4AAPAP2lkBADmmVatWatiwoTp27Kjly5dr37592rBhg/773/9q8+bNkqS+fftq5syZmjlzpnbv3q3hw4drx44dWR7T399fgwYN0sCBAzVnzhz9+uuv2rRpk2bMmCFJCg8PV0BAgJYtW6a//vpLx48flySNGDFCcXFxmjhxonbv3q3t27dr1qxZeu211yRJXbp0kY+Pj7p3766dO3dq6dKleuWVV3L5HQIAIP8jiQQA5BiHw6GlS5eqSZMmevzxxxUdHa2HHnpI+/btU0REhCSpU6dOGjZsmAYNGqR69epp//79V5zMZujQoRowYICGDRum6tWrq1OnTjp06JAkqVChQnrjjTc0depUlS5dWnfffbckqUePHnrnnXcUHx+v66+/Xk2bNlV8fLzrkSBFihTRp59+qp07d6pu3boaMmSIxo0bl4vvDgAA1waHxU0hAAAAwP9rzw5kAAAAAIT5W6cQwG/RAiYnEgAAgE1EAgAAsIlIAAAANhEJAADAJiIBAADYRCQAAACbiAQAAGATkQAAAGwiEgAAgE1EAgAAsIlIAAAANhEJAADAFiJdMZvCiSk9AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Task 2 completed!\n"
          ]
        }
      ],
      "source": [
        "def run_claim_verification(train_claims, dev_claims, test_claims, evidence, retriever=None):\n",
        "    \"\"\"\n",
        "    Run the claim verification pipeline and generate predictions.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Claim Verification Hyperparameters ===\")\n",
        "    print(f\"MODEL_NAME: {VERIFICATION_MODEL_NAME}\")\n",
        "    print(f\"BATCH_SIZE: {VERIFICATION_BATCH_SIZE}\")\n",
        "    print(f\"LEARNING_RATE: {VERIFICATION_LEARNING_RATE}\")\n",
        "    print(f\"USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\")\n",
        "    print(f\"USE_MIXED_EVIDENCE: {USE_MIXED_EVIDENCE}\")\n",
        "    \n",
        "    # Load retrieved evidence from Task 1\n",
        "    dev_predictions = load_json(DEV_PREDICTIONS_PATH)\n",
        "    test_predictions = load_json(TEST_PREDICTIONS_PATH)\n",
        "    \n",
        "    # Extract retrieved evidence\n",
        "    dev_retrieved = {claim_id: data[\"evidences\"] for claim_id, data in dev_predictions.items()}\n",
        "    test_retrieved = {claim_id: data[\"evidences\"] for claim_id, data in test_predictions.items()}\n",
        "    \n",
        "    # Check if retriever is valid\n",
        "    if retriever is not None:\n",
        "        print(\"Checking retriever functionality...\")\n",
        "        try:\n",
        "            # Test retrieve method on a sample claim\n",
        "            sample_claim_id = next(iter(train_claims))\n",
        "            sample_claim_text = train_claims[sample_claim_id][\"claim_text\"]\n",
        "            sample_evidence, _ = retriever.retrieve(sample_claim_text)\n",
        "            print(f\"Retriever test successful. Found {len(sample_evidence)} evidence passages.\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Retriever test failed: {e}\")\n",
        "    \n",
        "    # Train model\n",
        "    model, stats = train_verification_model(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims,\n",
        "        evidence=evidence,\n",
        "        retriever=retriever,\n",
        "        use_mixed_evidence=USE_MIXED_EVIDENCE,\n",
        "        use_class_weights=USE_CLASS_WEIGHTS\n",
        "    )\n",
        "    \n",
        "    # Initialize tokenizer for predictions\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    \n",
        "    # Make predictions on dev set\n",
        "    print(\"\\nMaking predictions on development set...\")\n",
        "    dev_pred_labels = predict_claims(model, dev_claims, evidence, dev_retrieved, tokenizer)\n",
        "    \n",
        "    # Evaluate dev predictions\n",
        "    print(\"\\nEvaluating development predictions:\")\n",
        "    dev_metrics = evaluate_predictions(dev_pred_labels, dev_claims)\n",
        "    \n",
        "    # Update dev predictions file\n",
        "    update_predictions_file(DEV_PREDICTIONS_PATH, dev_pred_labels)\n",
        "    \n",
        "    # Make predictions on test set\n",
        "    print(\"\\nMaking predictions on test set...\")\n",
        "    test_pred_labels = predict_claims(model, test_claims, evidence, test_retrieved, tokenizer)\n",
        "    \n",
        "    # Update test predictions file\n",
        "    update_predictions_file(TEST_PREDICTIONS_PATH, test_pred_labels)\n",
        "    \n",
        "    print(\"\\nClaim verification completed!\")\n",
        "    return model, stats, dev_metrics\n",
        "\n",
        "# Main execution - running task 2\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if we already have retrieval results\n",
        "    if 'retrieval_results' not in locals() and os.path.exists(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')):\n",
        "        print(\"Loading saved retrieval results...\")\n",
        "        retrieval_results = torch.load(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt'),weights_only=True)\n",
        "    # Check if retriever is available\n",
        "    if 'retrieval_results' in locals() and 'retriever' in retrieval_results:\n",
        "        retriever = retrieval_results[\"retriever\"]\n",
        "        print(\"Found retriever in retrieval_results.\")\n",
        "    else:\n",
        "        print(\"Retriever not found in retrieval_results.\")    \n",
        "    print(\"\\n=== Running Claim Verification (Task 2) ===\")\n",
        "    model, stats, metrics = run_claim_verification(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims, \n",
        "        test_claims=test_claims,\n",
        "        evidence=evidence,\n",
        "        retriever=retriever\n",
        "    )\n",
        "    \n",
        "    # Visualize confusion matrix\n",
        "    visualize_confusion_matrix(\n",
        "        metrics[\"confusion_matrix\"], \n",
        "        list(LABEL_MAP.keys())\n",
        "    )\n",
        "    \n",
        "    print(\"\\nTask 2 completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Diagnose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_dataset(train_claims, dev_claims, evidence):\n",
        "    print(\"\\n=== Dataset Diagnosis ===\")\n",
        "    \n",
        "    # Check label distribution in claims\n",
        "    train_labels = {}\n",
        "    for claim_id, claim_data in train_claims.items():\n",
        "        if \"claim_label\" in claim_data:\n",
        "            label = claim_data[\"claim_label\"]\n",
        "            if label not in train_labels:\n",
        "                train_labels[label] = 0\n",
        "            train_labels[label] += 1\n",
        "    \n",
        "    print(\"\\nLabel Distribution in Training Claims:\")\n",
        "    total_train = sum(train_labels.values())\n",
        "    for label, count in train_labels.items():\n",
        "        print(f\"  {label}: {count} claims ({count/total_train*100:.2f}%)\")\n",
        "    \n",
        "    # Check average number of evidence passages per claim\n",
        "    train_evidence_counts = []\n",
        "    for claim_id, claim_data in train_claims.items():\n",
        "        if \"evidences\" in claim_data:\n",
        "            train_evidence_counts.append(len(claim_data[\"evidences\"]))\n",
        "    \n",
        "    print(\"\\nEvidence per Claim in Training Set:\")\n",
        "    if train_evidence_counts:\n",
        "        print(f\"  Average: {np.mean(train_evidence_counts):.2f}\")\n",
        "        print(f\"  Min: {min(train_evidence_counts)}\")\n",
        "        print(f\"  Max: {max(train_evidence_counts)}\")\n",
        "    \n",
        "    # Create a test dataset with the actual preprocessing\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    test_dataset = ClaimVerificationDataset(\n",
        "        claims=train_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "    \n",
        "    # Check processed sample distribution\n",
        "    sample_labels = {}\n",
        "    for sample in test_dataset.samples:\n",
        "        label = sample[\"label\"]\n",
        "        if label not in sample_labels:\n",
        "            sample_labels[label] = 0\n",
        "        sample_labels[label] += 1\n",
        "    \n",
        "    print(\"\\nLabel Distribution in Processed Samples:\")\n",
        "    total_samples = len(test_dataset.samples)\n",
        "    for label, count in sample_labels.items():\n",
        "        label_name = LABEL_MAP_REVERSE.get(label, f\"Unknown ({label})\")\n",
        "        print(f\"  {label_name}: {count} samples ({count/total_samples*100:.2f}%)\")\n",
        "    \n",
        "    # Check for data processing issues by examining a few samples\n",
        "    print(\"\\nSample Check (first 3 samples):\")\n",
        "    for i in range(min(3, len(test_dataset))):\n",
        "        sample = test_dataset[i]\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"  Label: {LABEL_MAP_REVERSE.get(sample['label'].item(), 'Unknown')}\")\n",
        "        \n",
        "        # Decode the input IDs to check tokenization\n",
        "        input_text = tokenizer.decode(sample[\"input_ids\"])\n",
        "        print(f\"  Tokenized text (truncated): {input_text[:100]}...\")\n",
        "        \n",
        "        # Check if special tokens are properly placed\n",
        "        cls_token_id = tokenizer.cls_token_id\n",
        "        sep_token_id = tokenizer.sep_token_id\n",
        "        has_cls = sample[\"input_ids\"][0] == cls_token_id\n",
        "        has_sep = sep_token_id in sample[\"input_ids\"]\n",
        "        \n",
        "        print(f\"  Has CLS token at start: {has_cls}\")\n",
        "        print(f\"  Has SEP token: {has_sep}\")\n",
        "        \n",
        "        # Check attention mask\n",
        "        valid_tokens = sum(sample[\"attention_mask\"].tolist())\n",
        "        print(f\"  Valid tokens: {valid_tokens}/{len(sample['input_ids'])}\")\n",
        "    \n",
        "    return test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_model_predictions(model, test_dataset):\n",
        "    print(\"\\n=== Model Prediction Diagnosis ===\")\n",
        "    \n",
        "    # Create a small subset of the dataset for quick testing\n",
        "    subset_size = min(100, len(test_dataset))\n",
        "    subset_indices = np.random.choice(len(test_dataset), subset_size, replace=False)\n",
        "    \n",
        "    # Create a dataloader for the subset\n",
        "    batch_size = 16\n",
        "    subset_sampler = torch.utils.data.SubsetRandomSampler(subset_indices)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=subset_sampler)\n",
        "    \n",
        "    # Move model to device and set to evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            \n",
        "            # Forward pass (handle models with and without token_type_ids)\n",
        "            if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "                _, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            else:\n",
        "                _, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            \n",
        "            # Get predictions\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "            \n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "            \n",
        "            # Print some example predictions\n",
        "            if len(all_preds) <= 5:  # Just show the first 5 examples\n",
        "                for i in range(len(preds)):\n",
        "                    print(f\"\\nPrediction example {len(all_preds) - len(preds) + i + 1}:\")\n",
        "                    print(f\"  True label: {LABEL_MAP_REVERSE.get(labels[i], 'Unknown')}\")\n",
        "                    print(f\"  Predicted: {LABEL_MAP_REVERSE.get(preds[i], 'Unknown')}\")\n",
        "                    print(f\"  Probabilities: {probs[i].cpu().numpy().round(3)}\")\n",
        "    \n",
        "    # Show prediction distribution\n",
        "    pred_counts = {}\n",
        "    for pred in all_preds:\n",
        "        if pred not in pred_counts:\n",
        "            pred_counts[pred] = 0\n",
        "        pred_counts[pred] += 1\n",
        "    \n",
        "    print(\"\\nPrediction Distribution:\")\n",
        "    for pred, count in pred_counts.items():\n",
        "        pred_name = LABEL_MAP_REVERSE.get(pred, f\"Unknown ({pred})\")\n",
        "        print(f\"  {pred_name}: {count} predictions ({count/len(all_preds)*100:.2f}%)\")\n",
        "    \n",
        "    # Show prediction vs actual distribution\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    labels = [LABEL_MAP_REVERSE.get(i, f\"Unknown\") for i in range(max(LABEL_MAP.values()) + 1)]\n",
        "    \n",
        "    # Print simple text version of confusion matrix\n",
        "    print(\"True \\\\ Pred\", end=\"\")\n",
        "    for label in labels:\n",
        "        print(f\" | {label[:3]}\", end=\"\")\n",
        "    print()\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"{label[:7]}\", end=\"\")\n",
        "        for j in range(len(labels)):\n",
        "            if i < cm.shape[0] and j < cm.shape[1]:\n",
        "                print(f\" | {cm[i, j]:3d}\", end=\"\")\n",
        "            else:\n",
        "                print(\" |   0\", end=\"\")\n",
        "        print()\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\nAccuracy on subset: {accuracy:.4f}\")\n",
        "    \n",
        "    return all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_gradient_flow(model, test_dataset):\n",
        "    print(\"\\n=== Gradient Flow Diagnosis ===\")\n",
        "    \n",
        "    # Create a small subset of the dataset for quick testing\n",
        "    subset_size = min(32, len(test_dataset))\n",
        "    subset_indices = np.random.choice(len(test_dataset), subset_size, replace=False)\n",
        "    \n",
        "    # Create a dataloader for the subset\n",
        "    batch_size = 16\n",
        "    subset_sampler = torch.utils.data.SubsetRandomSampler(subset_indices)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=subset_sampler)\n",
        "    \n",
        "    # Move model to device and set to training mode\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    # Create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=VERIFICATION_LEARNING_RATE)\n",
        "    \n",
        "    # Get a batch\n",
        "    batch = next(iter(test_loader))\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    \n",
        "    # Record initial parameter values\n",
        "    initial_params = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            initial_params[name] = param.clone().detach().cpu().numpy()\n",
        "            print(f\"  {name}: shape={param.shape}, mean={param.data.mean().item():.6f}, std={param.data.std().item():.6f}\")\n",
        "    \n",
        "    # Forward pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "        loss, logits = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            token_type_ids=batch[\"token_type_ids\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "    else:\n",
        "        loss, logits = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "    \n",
        "    print(f\"\\nLoss: {loss.item():.4f}\")\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Check gradients\n",
        "    print(\"\\nGradient Magnitudes:\")\n",
        "    zero_grad_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.grad is None:\n",
        "                zero_grad_params.append(name)\n",
        "                print(f\"  {name}: NO GRADIENT\")\n",
        "            else:\n",
        "                grad_mean = param.grad.abs().mean().item()\n",
        "                grad_max = param.grad.abs().max().item()\n",
        "                print(f\"  {name}: mean={grad_mean:.6f}, max={grad_max:.6f}\")\n",
        "    \n",
        "    if zero_grad_params:\n",
        "        print(f\"\\nWARNING: {len(zero_grad_params)} parameters have no gradient!\")\n",
        "    \n",
        "    # Apply gradients\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Check parameter changes\n",
        "    print(\"\\nParameter Changes After One Update:\")\n",
        "    unchanged_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            new_value = param.clone().detach().cpu().numpy()\n",
        "            old_value = initial_params[name]\n",
        "            \n",
        "            # Calculate relative change\n",
        "            abs_diff = np.abs(new_value - old_value)\n",
        "            rel_change = np.mean(abs_diff) / (np.mean(np.abs(old_value)) + 1e-9)\n",
        "            \n",
        "            change_description = f\"  {name}: relative change={rel_change:.6f}\"\n",
        "            \n",
        "            if rel_change == 0:\n",
        "                unchanged_params.append(name)\n",
        "                change_description += \" (NO CHANGE)\"\n",
        "            \n",
        "            print(change_description)\n",
        "    \n",
        "    if unchanged_params:\n",
        "        print(f\"\\nWARNING: {len(unchanged_params)} parameters did not change after update!\")\n",
        "        \n",
        "    return not (zero_grad_params or unchanged_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnose_mixed_evidence(train_claims, evidence, retriever):\n",
        "    print(\"\\n=== Mixed Evidence Diagnosis ===\")\n",
        "    \n",
        "    if retriever is None:\n",
        "        print(\"No retriever provided. Cannot diagnose mixed evidence functionality.\")\n",
        "        return False\n",
        "    \n",
        "    # Test retrieval for a few claims\n",
        "    sample_size = 5\n",
        "    sample_claims = dict(list(train_claims.items())[:sample_size])\n",
        "    \n",
        "    print(f\"Testing retrieval for {sample_size} sample claims...\")\n",
        "    \n",
        "    for claim_id, claim_data in sample_claims.items():\n",
        "        print(f\"\\nClaim: {claim_data['claim_text']}\")\n",
        "        \n",
        "        # Get ground truth evidence\n",
        "        if \"evidences\" in claim_data:\n",
        "            ground_truth = claim_data[\"evidences\"]\n",
        "            print(f\"Ground truth evidence count: {len(ground_truth)}\")\n",
        "            \n",
        "            # Show first ground truth evidence\n",
        "            if ground_truth:\n",
        "                first_evidence_id = ground_truth[0]\n",
        "                if first_evidence_id in evidence:\n",
        "                    print(f\"Ground truth evidence sample: {evidence[first_evidence_id][:100]}...\")\n",
        "        \n",
        "        # Retrieve evidence using retriever\n",
        "        try:\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "            retrieved_evidence, scores = retriever.retrieve(claim_text)\n",
        "            print(f\"Retrieved evidence count: {len(retrieved_evidence)}\")\n",
        "            \n",
        "            # Show first retrieved evidence\n",
        "            if retrieved_evidence:\n",
        "                first_retrieved_id = retrieved_evidence[0]\n",
        "                if first_retrieved_id in evidence:\n",
        "                    print(f\"Retrieved evidence sample: {evidence[first_retrieved_id][:100]}...\")\n",
        "                    print(f\"Retrieval score: {scores[0]:.4f}\")\n",
        "            \n",
        "            # Check overlap with ground truth\n",
        "            if \"evidences\" in claim_data:\n",
        "                overlap = set(retrieved_evidence).intersection(set(ground_truth))\n",
        "                print(f\"Overlap with ground truth: {len(overlap)}/{len(ground_truth)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {e}\")\n",
        "            return False\n",
        "    \n",
        "    # Create datasets with and without mixed evidence\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    \n",
        "    # Ground truth only dataset\n",
        "    gt_dataset = ClaimVerificationDataset(\n",
        "        claims=sample_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "    \n",
        "    # Get retrieved evidence for sample claims\n",
        "    retrieved_evidence_dict = {}\n",
        "    for claim_id, claim_data in sample_claims.items():\n",
        "        claim_text = claim_data[\"claim_text\"]\n",
        "        evidence_ids, _ = retriever.retrieve(claim_text)\n",
        "        retrieved_evidence_dict[claim_id] = evidence_ids\n",
        "    \n",
        "    # Mixed evidence dataset\n",
        "    mixed_dataset = ClaimVerificationDataset(\n",
        "        claims=sample_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=retrieved_evidence_dict,\n",
        "        mix_ratio=0.5\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nGround truth only dataset size: {len(gt_dataset)}\")\n",
        "    print(f\"Mixed evidence dataset size: {len(mixed_dataset)}\")\n",
        "    \n",
        "    # Check if mixed dataset is actually larger\n",
        "    is_working = len(mixed_dataset) > len(gt_dataset)\n",
        "    print(f\"Mixed evidence functionality is {'working' if is_working else 'NOT working properly'}\")\n",
        "    \n",
        "    return is_working"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Diagnose Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_full_diagnosis(train_claims, dev_claims, evidence, retriever=None):\n",
        "    \"\"\"\n",
        "    Run a full suite of diagnostic tests to identify model issues.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STARTING FULL DIAGNOSTIC TESTS ===\\n\")\n",
        "    \n",
        "    # Step 1: Diagnose the dataset\n",
        "    test_dataset = diagnose_dataset(train_claims, dev_claims, evidence)\n",
        "    \n",
        "    # Step 2: Create and diagnose a new model\n",
        "    print(\"\\nInitializing a fresh model for testing...\")\n",
        "    model = ClaimVerificationModel(num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Step 3: Check gradient flow\n",
        "    gradient_flows = diagnose_gradient_flow(model, test_dataset)\n",
        "    \n",
        "    # Step 4: Train for one epoch to capture initial predictions\n",
        "    print(\"\\nTraining for one epoch to check initial learning...\")\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=VERIFICATION_LEARNING_RATE)\n",
        "    train_loader = DataLoader(test_dataset, batch_size=VERIFICATION_BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "            loss, _ = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        else:\n",
        "            loss, _ = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "    \n",
        "    avg_loss /= len(train_loader)\n",
        "    print(f\"Average loss after one epoch: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Step 5: Check model predictions\n",
        "    all_preds, all_labels = diagnose_model_predictions(model, test_dataset)\n",
        "    \n",
        "    # Step 6: Check mixed evidence functionality if retriever is available\n",
        "    if retriever is not None:\n",
        "        mixed_evidence_works = diagnose_mixed_evidence(train_claims, evidence, retriever)\n",
        "    else:\n",
        "        print(\"\\nSkipping mixed evidence diagnosis (no retriever provided)\")\n",
        "        mixed_evidence_works = None\n",
        "    \n",
        "    # Summarize findings\n",
        "    print(\"\\n=== DIAGNOSTIC SUMMARY ===\")\n",
        "    \n",
        "    # Check for dataset imbalance\n",
        "    label_counts = {}\n",
        "    for label in all_labels:\n",
        "        if label not in label_counts:\n",
        "            label_counts[label] = 0\n",
        "        label_counts[label] += 1\n",
        "    \n",
        "    most_common_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
        "    most_common_pct = label_counts[most_common_label] / len(all_labels) * 100\n",
        "    \n",
        "    if most_common_pct > 50:\n",
        "        print(f\"✘ Dataset imbalance detected: {LABEL_MAP_REVERSE.get(most_common_label)} class dominates ({most_common_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"✓ Dataset balance looks reasonable (largest class: {most_common_pct:.1f}%)\")\n",
        "    \n",
        "    # Check for prediction uniformity\n",
        "    pred_counts = {}\n",
        "    for pred in all_preds:\n",
        "        if pred not in pred_counts:\n",
        "            pred_counts[pred] = 0\n",
        "        pred_counts[pred] += 1\n",
        "    \n",
        "    if len(pred_counts) < len(LABEL_MAP):\n",
        "        print(f\"✘ Model is not predicting all classes (only {len(pred_counts)} out of {len(LABEL_MAP)})\")\n",
        "    else:\n",
        "        print(f\"✓ Model is predicting all {len(LABEL_MAP)} classes\")\n",
        "    \n",
        "    most_common_pred = max(pred_counts.items(), key=lambda x: x[1])[0]\n",
        "    most_common_pred_pct = pred_counts[most_common_pred] / len(all_preds) * 100\n",
        "    \n",
        "    if most_common_pred_pct > 70:\n",
        "        print(f\"✘ Model is biased toward {LABEL_MAP_REVERSE.get(most_common_pred)} class ({most_common_pred_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"✓ Model prediction distribution looks reasonable\")\n",
        "    \n",
        "    # Check gradient flow\n",
        "    if gradient_flows:\n",
        "        print(\"✓ Gradient flow is working correctly\")\n",
        "    else:\n",
        "        print(\"✘ Gradient flow issues detected\")\n",
        "    \n",
        "    # Check mixed evidence\n",
        "    if mixed_evidence_works is not None:\n",
        "        if mixed_evidence_works:\n",
        "            print(\"✓ Mixed evidence functionality is working correctly\")\n",
        "        else:\n",
        "            print(\"✘ Mixed evidence functionality is NOT working properly\")\n",
        "    \n",
        "    print(\"\\n=== END OF DIAGNOSIS ===\")\n",
        "    \n",
        "    return {\n",
        "        \"dataset_imbalance\": most_common_pct > 50,\n",
        "        \"prediction_bias\": most_common_pred_pct > 70,\n",
        "        \"gradient_flow_issues\": not gradient_flows,\n",
        "        \"mixed_evidence_issues\": False if mixed_evidence_works else True\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Run diagnostic tests\n",
        "# if __name__ == \"__main__\":\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import seaborn as sns\n",
        "#     from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "    \n",
        "#     # Run full diagnosis\n",
        "#     if 'retrieval_results' not in locals() and os.path.exists(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')):\n",
        "#         print(\"Loading saved retrieval results...\")\n",
        "#         retrieval_results = torch.load(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt'),weights_only=True)\n",
        "#         retriever = retrieval_results.get(\"retriever\") if \"retriever\" in retrieval_results else None\n",
        "#     else:\n",
        "#         retriever = None\n",
        "    \n",
        "#     diagnosis_results = run_full_diagnosis(train_claims, dev_claims, evidence, retriever)\n",
        "    \n",
        "#     # Based on diagnosis, suggest fixes\n",
        "#     print(\"\\n=== RECOMMENDED FIXES ===\")\n",
        "    \n",
        "#     if diagnosis_results[\"dataset_imbalance\"]:\n",
        "#         print(\"1. Apply class weighting by setting USE_CLASS_WEIGHTS = True\")\n",
        "#         print(\"2. Consider oversampling minority classes or downsampling majority classes\")\n",
        "    \n",
        "#     if diagnosis_results[\"prediction_bias\"]:\n",
        "#         print(\"1. Reduce learning rate (try 5e-6 instead of 2e-5)\")\n",
        "#         print(\"2. Train for more epochs (5-10 instead of 3)\")\n",
        "#         print(\"3. Consider using focal loss instead of cross-entropy loss\")\n",
        "    \n",
        "#     if diagnosis_results[\"gradient_flow_issues\"]:\n",
        "#         print(\"1. Check for NaN values in inputs or gradients\")\n",
        "#         print(\"2. Reduce batch size if you're encountering memory issues\")\n",
        "#         print(\"3. Consider gradient clipping to prevent exploding gradients\")\n",
        "    \n",
        "#     if diagnosis_results[\"mixed_evidence_issues\"]:\n",
        "#         print(\"1. Fix the mixed evidence implementation\")\n",
        "#         print(\"2. Check the retriever functionality\")\n",
        "#         print(\"3. Ensure the mix_ratio is properly used in the dataset creation\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
