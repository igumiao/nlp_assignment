{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igumiao/nlp_G60/blob/xiaohong/Pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "fc1277b9-fff4-42c5-e626-cbf8eede5337"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla T4\n",
            "Loading /content/data/train-claims.json...\n",
            "Successfully loaded 1228 items.\n",
            "Loading /content/data/dev-claims.json...\n",
            "Successfully loaded 154 items.\n",
            "Loading /content/data/test-claims-unlabelled.json...\n",
            "Successfully loaded 153 items.\n",
            "Loading /content/data/evidence.json...\n",
            "Successfully loaded 1208827 items.\n",
            "\n",
            "=== Sample Data Structure ===\n",
            "Sample claim ID: claim-1937\n",
            "{\n",
            "  \"claim_text\": \"Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\",\n",
            "  \"claim_label\": \"DISPUTED\",\n",
            "  \"evidences\": [\n",
            "    \"evidence-442946\",\n",
            "    \"evidence-1194317\",\n",
            "    \"evidence-12171\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Sample evidence ID: evidence-0\n",
            "Evidence text: John Bennet Lawes, English entrepreneur and agricultural scientist\n",
            "Original claim: Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
            "Basic preprocessing: not only is there no scientific evidence that co2 is a pollutant higher co2 concentrations actually help ecosystems support more plant and animal life\n",
            "With stopword removal: scientific evidence co2 pollutant higher co2 concentrations actually help ecosystems support plant animal life\n",
            "With lemmatization: not only is there no scientific evidence that co2 is a pollutant higher co2 concentration actually help ecosystem support more plant and animal life\n",
            "With stemming: not onli is there no scientif evid that co2 is a pollut higher co2 concentr actual help ecosystem support more plant and anim life\n",
            "\n",
            "Original evidence: John Bennet Lawes, English entrepreneur and agricultural scientist\n",
            "Basic preprocessing: john bennet lawes english entrepreneur and agricultural scientist\n",
            "With stopword removal: john bennet lawes english entrepreneur agricultural scientist\n",
            "\n",
            "Data processing complete!\n"
          ]
        }
      ],
      "source": [
        "# 1. DataSet Processing\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# --- Configuration ---\n",
        "# if using Google Colab, set the data path accordingly\n",
        "DRIVE_DATA_PATH = '/content/data'  # Path to the data directory\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# File paths\n",
        "TRAIN_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'train-claims.json')\n",
        "DEV_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'dev-claims.json')\n",
        "TEST_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'test-claims-unlabelled.json')\n",
        "EVIDENCE_PATH = os.path.join(DRIVE_DATA_PATH, 'evidence.json')\n",
        "DEV_PREDICTIONS_PATH = os.path.join(DRIVE_DATA_PATH, 'dev-claims-predictions.json')\n",
        "TEST_PREDICTIONS_PATH = os.path.join(DRIVE_DATA_PATH, 'test-claims-predictions.json')\n",
        "EVAL_SCRIPT_PATH = os.path.join('eval.py')\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_json(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"Successfully loaded {len(data)} items.\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text, remove_stop_words=False, lemmatize=False, stem=False):\n",
        "    \"\"\"\n",
        "    Preprocess text with multiple options:\n",
        "    - Lowercase\n",
        "    - Remove special characters\n",
        "    - Optional: Remove stopwords\n",
        "    - Optional: Lemmatization\n",
        "    - Optional: Stemming\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and extra spaces\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords if requested\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply lemmatization if requested\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Apply stemming if requested\n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join the tokens back to a string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "def basic_dataset_analysis(data_dict, name):\n",
        "    \"\"\"Basic analysis of a dataset (claims or evidence).\"\"\"\n",
        "    print(f\"\\n=== Basic Analysis of {name} Dataset ===\")\n",
        "    print(f\"Number of items: {len(data_dict)}\")\n",
        "\n",
        "    # For claims, check label distribution\n",
        "    if name == 'Claims' and 'claim_label' in next(iter(data_dict.values())):\n",
        "        labels = [item.get('claim_label') for item in data_dict.values() if 'claim_label' in item]\n",
        "        label_counts = Counter(labels)\n",
        "        print(\"\\nLabel Distribution:\")\n",
        "        for label, count in label_counts.items():\n",
        "            print(f\"  {label}: {count} ({count/len(labels)*100:.2f}%)\")\n",
        "\n",
        "    # Check text lengths\n",
        "    if 'claim_text' in next(iter(data_dict.values()), {}):\n",
        "        # For claims\n",
        "        text_field = 'claim_text'\n",
        "        text_lengths = [len(item[text_field].split()) for item in data_dict.values()]\n",
        "    else:\n",
        "        # For evidence\n",
        "        text_lengths = [len(text.split()) for i, text in enumerate(data_dict.values()) if i < 1000]\n",
        "        text_field = 'evidence text'\n",
        "\n",
        "    print(f\"\\n{text_field.capitalize()} Length Statistics (in words):\")\n",
        "    print(f\"  Average length: {sum(text_lengths)/len(text_lengths):.2f}\")\n",
        "    print(f\"  Maximum length: {max(text_lengths)}\")\n",
        "    print(f\"  Minimum length: {min(text_lengths)}\")\n",
        "\n",
        "    return text_lengths\n",
        "\n",
        "# --- 1. Load and Explore Datasets ---\n",
        "train_claims = load_json(TRAIN_CLAIMS_PATH)\n",
        "dev_claims = load_json(DEV_CLAIMS_PATH)\n",
        "test_claims = load_json(TEST_CLAIMS_PATH)\n",
        "evidence = load_json(EVIDENCE_PATH)\n",
        "\n",
        "# Print a sample claim to understand structure\n",
        "print(\"\\n=== Sample Data Structure ===\")\n",
        "sample_claim_id = next(iter(train_claims))\n",
        "print(f\"Sample claim ID: {sample_claim_id}\")\n",
        "print(json.dumps(train_claims[sample_claim_id], indent=2))\n",
        "\n",
        "# Print a sample evidence\n",
        "sample_evidence_id = next(iter(evidence))\n",
        "print(f\"\\nSample evidence ID: {sample_evidence_id}\")\n",
        "print(f\"Evidence text: {evidence[sample_evidence_id]}\")\n",
        "\n",
        "# # --- 2. Basic Dataset Analysis ---\n",
        "# train_lengths = basic_dataset_analysis(train_claims, \"Training Claims\")\n",
        "# dev_lengths = basic_dataset_analysis(dev_claims, \"Development Claims\")\n",
        "# test_lengths = basic_dataset_analysis(test_claims, \"Test Claims\")\n",
        "\n",
        "# # Sample evidence for analysis (avoid analyzing all 1.2M items)\n",
        "# sample_evidence = {k: evidence[k] for k in list(evidence.keys())[:1000]}\n",
        "# evidence_lengths = basic_dataset_analysis(sample_evidence, \"Evidence (Sample)\")\n",
        "\n",
        "# # --- 3. Create a simple visualization ---\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(train_lengths, bins=30, alpha=0.5, label='Claims')\n",
        "# plt.hist(evidence_lengths, bins=30, alpha=0.5, label='Evidence')\n",
        "# plt.xlabel('Word Count')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.title('Distribution of Text Lengths: Claims vs Evidence')\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.savefig('text_length_distribution.png')\n",
        "# plt.show()\n",
        "\n",
        "# # --- 4. Preprocessing Examples ---\n",
        "# print(\"\\n=== Preprocessing Examples ===\")\n",
        "\n",
        "# Sample a claim\n",
        "sample_claim = train_claims[sample_claim_id]['claim_text']\n",
        "print(f\"Original claim: {sample_claim}\")\n",
        "print(f\"Basic preprocessing: {preprocess_text(sample_claim)}\")\n",
        "print(f\"With stopword removal: {preprocess_text(sample_claim, remove_stop_words=True)}\")\n",
        "print(f\"With lemmatization: {preprocess_text(sample_claim, lemmatize=True)}\")\n",
        "print(f\"With stemming: {preprocess_text(sample_claim, stem=True)}\")\n",
        "\n",
        "# Sample an evidence\n",
        "sample_evidence_text = evidence[sample_evidence_id]\n",
        "print(f\"\\nOriginal evidence: {sample_evidence_text}\")\n",
        "print(f\"Basic preprocessing: {preprocess_text(sample_evidence_text)}\")\n",
        "print(f\"With stopword removal: {preprocess_text(sample_evidence_text, remove_stop_words=True)}\")\n",
        "\n",
        "print(\"\\nData processing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "# --- Global Configuration Variables ---\n",
        "# TF-IDF settings\n",
        "TFIDF_MAX_FEATURES = 20000\n",
        "TFIDF_TOP_K = 500\n",
        "\n",
        "# Model settings\n",
        "TRANSFORMER_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "TRANSFORMER_BATCH_SIZE = 32\n",
        "TRANSFORMER_MAX_LENGTH = 512\n",
        "\n",
        "# Retrieval settings\n",
        "EVIDENCE_FINAL_TOP_K = 4  # The value to use for evidence retrieval\n",
        "EXPERIMENT_WITH_MULTIPLE_K = False  # Set to True to experiment with different k values\n",
        "EVIDENCE_TOP_K_VALUES = [3, 4, 5 ,6] if EXPERIMENT_WITH_MULTIPLE_K else [EVIDENCE_FINAL_TOP_K]\n",
        "\n",
        "# Memory management\n",
        "CUDA_CACHE_CLEAR_FREQUENCY = 20  # Clear CUDA cache every N claims\n",
        "PROGRESS_REPORT_FREQUENCY = 50  # Report progress every N claims\n",
        "\n",
        "# Text Processing settings\n",
        "REMOVE_STOP_WORDS = False\n",
        "LEMMATIZE = True\n",
        "\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqVJ2sVFY6S6"
      },
      "source": [
        "#### Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AFa_9iYWY6S6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Hugging Face Embedder Class ---\n",
        "class HuggingFaceEmbedder:\n",
        "    def __init__(self, model_name=TRANSFORMER_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Initialize the Hugging Face embedder.\n",
        "        \"\"\"\n",
        "        print(f\"Loading model {model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        \"\"\"\n",
        "        Perform mean pooling on model outputs using attention mask.\n",
        "        \"\"\"\n",
        "        # Mean pooling - take average of all token embeddings\n",
        "        token_embeddings = model_output[0]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    def encode(self, texts, batch_size=TRANSFORMER_BATCH_SIZE, show_progress=False):\n",
        "        \"\"\"\n",
        "        Encode texts to embeddings using the model.\n",
        "        \"\"\"\n",
        "        all_embeddings = []\n",
        "\n",
        "        # Process texts in batches\n",
        "        iterator = range(0, len(texts), batch_size)\n",
        "        if show_progress:\n",
        "            iterator = tqdm(iterator, desc=\"Encoding texts\")\n",
        "\n",
        "        for i in iterator:\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "            # Tokenize\n",
        "            encoded_input = self.tokenizer(batch_texts, padding=True, truncation=True,\n",
        "                                          max_length=TRANSFORMER_MAX_LENGTH, return_tensors='pt')\n",
        "\n",
        "            # Move to device\n",
        "            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
        "\n",
        "            # Get model output\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoded_input)\n",
        "\n",
        "            # Perform pooling and get embeddings\n",
        "            batch_embeddings = self.mean_pooling(outputs, encoded_input['attention_mask'])\n",
        "\n",
        "            # Normalize embeddings\n",
        "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
        "\n",
        "            # Move to CPU to free up GPU memory\n",
        "            all_embeddings.append(batch_embeddings.cpu())\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        if len(all_embeddings) > 0:\n",
        "            all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "        return all_embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1Xzxl0Y6S7"
      },
      "source": [
        "#### Two-Stage Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Z67XoOT8Y6S7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Two-Stage Retriever Class ---\n",
        "class TwoStageRetriever:\n",
        "    def __init__(self, evidence_corpus, tfidf_preprocessing=None, model_name=TRANSFORMER_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Initialize the two-stage retriever with TF-IDF and Hugging Face Transformers.\n",
        "\n",
        "        Args:\n",
        "            evidence_corpus (dict): Dictionary of evidence texts\n",
        "            tfidf_preprocessing (function, optional): Function to preprocess text for TF-IDF\n",
        "            model_name (str): Name of the Hugging Face model to use\n",
        "        \"\"\"\n",
        "        self.evidence_corpus = evidence_corpus\n",
        "        self.evidence_ids = list(evidence_corpus.keys())\n",
        "        self.tfidf_preprocessing = tfidf_preprocessing if tfidf_preprocessing else preprocess_text\n",
        "\n",
        "        # Prepare evidence texts for TF-IDF\n",
        "        print(\"Preparing evidence texts for TF-IDF...\")\n",
        "        start_time = time.time()\n",
        "        self.evidence_texts = [self.tfidf_preprocessing(evidence_corpus[eid]) for eid in tqdm(self.evidence_ids)]\n",
        "        print(f\"Evidence preparation took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "        # Initialize TF-IDF vectorizer\n",
        "        print(\"Fitting TF-IDF vectorizer...\")\n",
        "        start_time = time.time()\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES)\n",
        "        self.tfidf_evidence_matrix = self.tfidf_vectorizer.fit_transform(self.evidence_texts)\n",
        "        print(f\"TF-IDF fitting took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "        # Initialize Hugging Face embedder\n",
        "        self.embedder = HuggingFaceEmbedder(model_name)\n",
        "\n",
        "        print(\"Two-stage retriever initialized successfully.\")\n",
        "\n",
        "    def retrieve(self, claim, tfidf_top_k=TFIDF_TOP_K, final_top_k=EVIDENCE_FINAL_TOP_K):\n",
        "        \"\"\"\n",
        "        Retrieve evidence for a claim using the two-stage approach.\n",
        "\n",
        "        Args:\n",
        "            claim (str): The claim text\n",
        "            tfidf_top_k (int): Number of candidates to retrieve with TF-IDF\n",
        "            final_top_k (int): Number of final candidates to return after re-ranking\n",
        "\n",
        "        Returns:\n",
        "            list: Top k evidence IDs\n",
        "            list: Corresponding similarity scores\n",
        "        \"\"\"\n",
        "        # Stage 1: TF-IDF retrieval\n",
        "        processed_claim = self.tfidf_preprocessing(claim)\n",
        "        claim_vector = self.tfidf_vectorizer.transform([processed_claim])\n",
        "\n",
        "        # Calculate cosine similarity with all evidence\n",
        "        tfidf_similarities = cosine_similarity(claim_vector, self.tfidf_evidence_matrix)[0]\n",
        "\n",
        "        # Get top-k candidate indices\n",
        "        tfidf_top_indices = np.argsort(-tfidf_similarities)[:tfidf_top_k]\n",
        "        tfidf_top_evidence_ids = [self.evidence_ids[idx] for idx in tfidf_top_indices]\n",
        "        tfidf_top_evidence_texts = [self.evidence_corpus[eid] for eid in tfidf_top_evidence_ids]\n",
        "\n",
        "        # Stage 2: Transformer embedding re-ranking\n",
        "        # Encode claim and candidates\n",
        "        with torch.no_grad():\n",
        "            claim_embedding = self.embedder.encode([claim])\n",
        "            candidate_embeddings = self.embedder.encode(tfidf_top_evidence_texts)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = torch.matmul(candidate_embeddings, claim_embedding.t()).squeeze()\n",
        "\n",
        "        # Get top-k final indices\n",
        "        top_k_indices = torch.argsort(similarities, descending=True)[:final_top_k].tolist()\n",
        "\n",
        "        # Get final evidence IDs and scores\n",
        "        final_evidence_ids = [tfidf_top_evidence_ids[idx] for idx in top_k_indices]\n",
        "        final_scores = [similarities[idx].item() for idx in top_k_indices]\n",
        "\n",
        "        return final_evidence_ids, final_scores\n",
        "\n",
        "    def batch_retrieve(self, claims_dict, tfidf_top_k=TFIDF_TOP_K, final_top_k=EVIDENCE_FINAL_TOP_K):\n",
        "        \"\"\"\n",
        "        Retrieve evidence for multiple claims in batch mode.\n",
        "\n",
        "        Args:\n",
        "            claims_dict (dict): Dictionary of claims\n",
        "            tfidf_top_k (int): Number of candidates to retrieve with TF-IDF\n",
        "            final_top_k (int): Number of final candidates to return after re-ranking\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping claim IDs to lists of evidence IDs\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        total_claims = len(claims_dict)\n",
        "\n",
        "        print(f\"Processing {total_claims} claims...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx, (claim_id, claim_data) in enumerate(tqdm(claims_dict.items())):\n",
        "            claim_text = claim_data['claim_text']\n",
        "            evidence_ids, _ = self.retrieve(claim_text, tfidf_top_k, final_top_k)\n",
        "            results[claim_id] = evidence_ids\n",
        "\n",
        "            # # Periodically clear CUDA cache to prevent memory leaks\n",
        "            # if torch.cuda.is_available() and (idx % CUDA_CACHE_CLEAR_FREQUENCY == 0):\n",
        "            #     torch.cuda.empty_cache()\n",
        "            #     gc.collect()\n",
        "\n",
        "            # Periodically report progress\n",
        "            if (idx + 1) % PROGRESS_REPORT_FREQUENCY == 0 or (idx + 1) == total_claims:\n",
        "                elapsed = time.time() - start_time\n",
        "                claims_per_second = (idx + 1) / elapsed\n",
        "                print(f\"Processed {idx + 1}/{total_claims} claims \"\n",
        "                      f\"({(idx + 1)/total_claims*100:.1f}%) \"\n",
        "                      f\"at {claims_per_second:.2f} claims/second\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Total retrieval time: {total_time:.2f} seconds \"\n",
        "              f\"({total_time/60:.2f} minutes)\")\n",
        "\n",
        "        return results\n",
        "\n",
        "def evaluate_retrieval_fscore(retrieved_evidences, groundtruth_claims):\n",
        "    \"\"\"\n",
        "    Calculate F-score for evidence retrieval.\n",
        "    \"\"\"\n",
        "    f_scores = []\n",
        "\n",
        "    for claim_id, claim_data in groundtruth_claims.items():\n",
        "        if claim_id in retrieved_evidences:\n",
        "            # Get retrieved and ground truth evidence sets\n",
        "            retrieved_set = set(retrieved_evidences[claim_id])\n",
        "            groundtruth_set = set(claim_data[\"evidences\"])\n",
        "\n",
        "            # Calculate number of correct retrievals\n",
        "            correct = len(retrieved_set.intersection(groundtruth_set))\n",
        "\n",
        "            # Calculate precision, recall, and F-score\n",
        "            precision = correct / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "            recall = correct / len(groundtruth_set) if len(groundtruth_set) > 0 else 0\n",
        "\n",
        "            # Calculate F-score\n",
        "            fscore = 0\n",
        "            if precision > 0 and recall > 0:\n",
        "                fscore = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "            f_scores.append(fscore)\n",
        "\n",
        "    # Return average F-score\n",
        "    return np.mean(f_scores) if f_scores else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "E7hu2EowY6S7"
      },
      "outputs": [],
      "source": [
        "# --- Main Retrieval Function ---\n",
        "def run_evidence_retrieval(train_claims, dev_claims, test_claims, evidence):\n",
        "    \"\"\"\n",
        "    Run the evidence retrieval pipeline and evaluate results.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Retrieval Hyperparameters ===\")\n",
        "    print(f\"TFIDF_MAX_FEATURES: {TFIDF_MAX_FEATURES}\")\n",
        "    print(f\"TFIDF_TOP_K: {TFIDF_TOP_K}\")\n",
        "    print(f\"TRANSFORMER_MODEL: {TRANSFORMER_MODEL_NAME}\")\n",
        "    print(f\"EVIDENCE_TOP_K_VALUES: {EVIDENCE_TOP_K_VALUES}\")\n",
        "\n",
        "    # Use your existing preprocessing function\n",
        "    def preprocess_for_retrieval(text):\n",
        "        return preprocess_text(text, remove_stop_words=REMOVE_STOP_WORDS, lemmatize=LEMMATIZE)\n",
        "\n",
        "    # Results container\n",
        "    all_results = {}\n",
        "\n",
        "    # Initialize the retriever (only once)\n",
        "    print(\"Initializing two-stage retriever...\")\n",
        "    start_time = time.time()\n",
        "    retriever = TwoStageRetriever(\n",
        "        evidence_corpus=evidence,\n",
        "        tfidf_preprocessing=preprocess_for_retrieval,\n",
        "        model_name=TRANSFORMER_MODEL_NAME\n",
        "    )\n",
        "    print(f\"Retriever initialization took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Run only with selected top_k value(s)\n",
        "    for top_k in EVIDENCE_TOP_K_VALUES:\n",
        "        print(f\"\\n--- Evidence Retrieval with Top-K = {top_k} ---\")\n",
        "\n",
        "        # Retrieve evidence for dev set\n",
        "        print(f\"Retrieving evidence for development set...\")\n",
        "        dev_evidence_results = retriever.batch_retrieve(\n",
        "            dev_claims,\n",
        "            tfidf_top_k=TFIDF_TOP_K,\n",
        "            final_top_k=top_k\n",
        "        )\n",
        "\n",
        "        # Create dev predictions format\n",
        "        dev_predictions = {}\n",
        "        for claim_id, claim_data in dev_claims.items():\n",
        "            dev_predictions[claim_id] = {\n",
        "                \"claim_text\": claim_data[\"claim_text\"],\n",
        "                \"claim_label\": claim_data[\"claim_label\"],\n",
        "                \"evidences\": dev_evidence_results[claim_id]\n",
        "            }\n",
        "\n",
        "        # Save dev predictions\n",
        "        dev_predictions_path = DEV_PREDICTIONS_PATH\n",
        "        if EXPERIMENT_WITH_MULTIPLE_K:\n",
        "            # If experimenting, save with specific filename\n",
        "            dev_predictions_path = os.path.join(DRIVE_DATA_PATH, f'dev-claims-predictions-top{top_k}.json')\n",
        "\n",
        "        print(f\"Saving development predictions to {dev_predictions_path}\")\n",
        "        with open(dev_predictions_path, 'w') as f:\n",
        "            json.dump(dev_predictions, f, indent=2)\n",
        "\n",
        "        # # Evaluate dev results\n",
        "        print(f\"Evaluating development results...\")\n",
        "        fscore = evaluate_retrieval_fscore(dev_evidence_results, dev_claims)\n",
        "        print(f\"Evidence Retrieval F-score = {fscore:.4f}\")\n",
        "\n",
        "        # Store results\n",
        "        all_results[f\"dev_top{top_k}\"] = dev_evidence_results\n",
        "\n",
        "        # Also retrieve for test set if this is the final top_k\n",
        "        if top_k == EVIDENCE_FINAL_TOP_K or not EXPERIMENT_WITH_MULTIPLE_K:\n",
        "            print(f\"\\nRetrieving evidence for test set...\")\n",
        "            test_evidence_results = retriever.batch_retrieve(\n",
        "                test_claims,\n",
        "                tfidf_top_k=TFIDF_TOP_K,\n",
        "                final_top_k=top_k\n",
        "            )\n",
        "\n",
        "            # Create test predictions\n",
        "            test_predictions = {}\n",
        "            for claim_id, claim_data in test_claims.items():\n",
        "                test_predictions[claim_id] = {\n",
        "                    \"claim_text\": claim_data[\"claim_text\"],\n",
        "                    \"claim_label\": None,  # Will be filled by classification\n",
        "                    \"evidences\": test_evidence_results[claim_id]\n",
        "                }\n",
        "\n",
        "            # Save test predictions\n",
        "            print(f\"Saving test predictions to {TEST_PREDICTIONS_PATH}\")\n",
        "            with open(TEST_PREDICTIONS_PATH, 'w') as f:\n",
        "                json.dump(test_predictions, f, indent=2)\n",
        "\n",
        "            all_results[\"test\"] = test_evidence_results\n",
        "\n",
        "    all_results[\"retriever\"] = retriever\n",
        "    print(\"\\nEvidence retrieval completed!\")\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HksuCfjLY6S8",
        "outputId": "571f78d3-02e5-4a05-8863-47019e30152e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running Evidence Retrieval Pipeline ===\n",
            "Running evidence retrieval pipeline...\n",
            "\n",
            "=== Retrieval Hyperparameters ===\n",
            "TFIDF_MAX_FEATURES: 20000\n",
            "TFIDF_TOP_K: 500\n",
            "TRANSFORMER_MODEL: sentence-transformers/all-MiniLM-L6-v2\n",
            "EVIDENCE_TOP_K_VALUES: [4]\n",
            "Initializing two-stage retriever...\n",
            "Preparing evidence texts for TF-IDF...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1208827/1208827 [04:22<00:00, 4602.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence preparation took 262.63 seconds\n",
            "Fitting TF-IDF vectorizer...\n",
            "TF-IDF fitting took 24.20 seconds\n",
            "Loading model sentence-transformers/all-MiniLM-L6-v2...\n",
            "Two-stage retriever initialized successfully.\n",
            "Retriever initialization took 287.30 seconds\n",
            "\n",
            "--- Evidence Retrieval with Top-K = 4 ---\n",
            "Retrieving evidence for development set...\n",
            "Processing 154 claims...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 50/154 [00:52<01:53,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 50/154 claims (32.5%) at 0.95 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▍   | 100/154 [01:41<00:53,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 100/154 claims (64.9%) at 0.98 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 150/154 [02:30<00:03,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 150/154 claims (97.4%) at 1.00 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [02:33<00:00,  1.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 154/154 claims (100.0%) at 1.00 claims/second\n",
            "Total retrieval time: 153.89 seconds (2.56 minutes)\n",
            "Saving development predictions to /content/data/dev-claims-predictions.json\n",
            "Evaluating development results...\n",
            "Evidence Retrieval F-score = 0.1734\n",
            "\n",
            "Retrieving evidence for test set...\n",
            "Processing 153 claims...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 50/153 [00:48<01:38,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 50/153 claims (32.7%) at 1.02 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 100/153 [01:36<00:55,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 100/153 claims (65.4%) at 1.03 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 150/153 [02:25<00:03,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 150/153 claims (98.0%) at 1.03 claims/second\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 153/153 [02:28<00:00,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 153/153 claims (100.0%) at 1.03 claims/second\n",
            "Total retrieval time: 148.12 seconds (2.47 minutes)\n",
            "Saving test predictions to /content/data/test-claims-predictions.json\n",
            "\n",
            "Evidence retrieval completed!\n",
            "Saving retrieval results to /content/data/retrieval_results.pt\n",
            "\n",
            "Evidence retrieval step completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Run Evidence Retrieval Pipeline ---\n",
        "print(\"\\n=== Running Evidence Retrieval Pipeline ===\")\n",
        "\n",
        "# Check if retrieval results already exist to avoid recomputation\n",
        "retrieval_results_path = os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')\n",
        "\n",
        "if os.path.exists(retrieval_results_path) and False:  # Set to True to use cached results\n",
        "    print(f\"Loading cached retrieval results from {retrieval_results_path}\")\n",
        "    retrieval_results = torch.load(retrieval_results_path,weights_only=True)\n",
        "    print(\"Loaded retrieval results successfully.\")\n",
        "else:\n",
        "    print(\"Running evidence retrieval pipeline...\")\n",
        "    retrieval_results = run_evidence_retrieval(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims,\n",
        "        test_claims=test_claims,\n",
        "        evidence=evidence\n",
        "    )\n",
        "\n",
        "    # Save retrieval results (excluding the retriever object which might be large)\n",
        "    results_to_save = {k: v for k, v in retrieval_results.items() if k != 'retriever'}\n",
        "    print(f\"Saving retrieval results to {retrieval_results_path}\")\n",
        "    torch.save(results_to_save, retrieval_results_path)\n",
        "\n",
        "print(\"\\nEvidence retrieval step completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiDbhUkVY6S8"
      },
      "source": [
        "#### Classification with \"distilbert-base-uncased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhUlkzOMY6S8"
      },
      "source": [
        "Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BJHl1K1pY6S8"
      },
      "outputs": [],
      "source": [
        "# --- Task 2: Claim Verification Configuration ---\n",
        "# Model settings\n",
        "VERIFICATION_MODEL_NAME = \"distilbert-base-uncased\"\n",
        "VERIFICATION_BATCH_SIZE = 16\n",
        "VERIFICATION_MAX_LENGTH = 512\n",
        "VERIFICATION_LEARNING_RATE = 1e-5\n",
        "VERIFICATION_NUM_EPOCHS = 6\n",
        "VERIFICATION_NUM_LABELS = 4\n",
        "\n",
        "# Class management\n",
        "USE_CLASS_WEIGHTS = False\n",
        "USE_MIXED_EVIDENCE = False  # Whether to use both gold and retrieved evidence\n",
        "MIX_RATIO=0.5  # Ratio of gold evidence to retrieved evidence\n",
        "\n",
        "# Training phases\n",
        "PHASE1_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_phase1.pt')\n",
        "PHASE2_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_phase2.pt')\n",
        "FINAL_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_final.pt')\n",
        "\n",
        "# Label mapping\n",
        "LABEL_MAP = {\n",
        "    \"SUPPORTS\": 0,\n",
        "    \"REFUTES\": 1,\n",
        "    \"NOT_ENOUGH_INFO\": 2,\n",
        "    \"DISPUTED\": 3\n",
        "}\n",
        "LABEL_MAP_REVERSE = {v: k for k, v in LABEL_MAP.items()}\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "G5gwD0hAY6S8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def prepare_input_sequence(\n",
        "    claim, evidence, tokenizer, max_length=VERIFICATION_MAX_LENGTH\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepare input sequence for the model by tokenizing and formatting claim + evidence.\n",
        "    Implements smart truncation to handle long evidence passages.\n",
        "    \"\"\"\n",
        "    # Tokenize the claim and evidence pair\n",
        "    if \"distilbert\" in VERIFICATION_MODEL_NAME.lower():\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text=claim,\n",
        "            text_pair=evidence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=None,  # Return Python lists\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,  # No token_type_ids for DistilBERT\n",
        "        )\n",
        "    else:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text=claim,\n",
        "            text_pair=evidence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=None,  # Return Python lists\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "\n",
        "class ClaimVerificationDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        claims,\n",
        "        evidence,\n",
        "        tokenizer,\n",
        "        max_length=VERIFICATION_MAX_LENGTH,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Dataset for claim verification.\n",
        "\n",
        "        Args:\n",
        "            claims: Dictionary of claims\n",
        "            evidence: Dictionary of evidence passages\n",
        "            tokenizer: Tokenizer for preprocessing\n",
        "            max_length: Maximum sequence length\n",
        "            use_ground_truth: Whether to use ground truth evidence\n",
        "            retrieved_evidence: Dictionary of retrieved evidence IDs (from Task 1)\n",
        "            mix_ratio: Ratio of retrieved evidence to mix in (0.0-1.0)\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "\n",
        "        print(\n",
        "            f\"Creating dataset (use_ground_truth={use_ground_truth}, mix_ratio={mix_ratio})...\"\n",
        "        )\n",
        "\n",
        "        # Set random seed for reproducibility when mixing evidence\n",
        "        random.seed(42)\n",
        "\n",
        "        # Process each claim\n",
        "        for claim_id, claim_data in tqdm(claims.items(), desc=\"Processing claims\"):\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "\n",
        "            # Skip claims without labels (e.g., test set)\n",
        "            if \"claim_label\" not in claim_data:\n",
        "                continue\n",
        "\n",
        "            label = LABEL_MAP.get(claim_data[\"claim_label\"])\n",
        "\n",
        "            # Use ground truth evidence if available and requested\n",
        "            if use_ground_truth and \"evidences\" in claim_data:\n",
        "                for ev_id in claim_data[\"evidences\"]:\n",
        "                    if ev_id in evidence:\n",
        "                        self.samples.append(\n",
        "                            {\n",
        "                                \"claim_id\": claim_id,\n",
        "                                \"claim_text\": claim_text,\n",
        "                                \"evidence_id\": ev_id,\n",
        "                                \"evidence_text\": evidence[ev_id],\n",
        "                                \"label\": label,\n",
        "                                \"is_ground_truth\": True,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "            # Mix in retrieved evidence if requested\n",
        "            if retrieved_evidence and claim_id in retrieved_evidence and mix_ratio > 0:\n",
        "                for ev_id in retrieved_evidence[claim_id]:\n",
        "                    # Skip if already included as ground truth\n",
        "                    if (\n",
        "                        use_ground_truth\n",
        "                        and \"evidences\" in claim_data\n",
        "                        and ev_id in claim_data[\"evidences\"]\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    # Only include with probability equal to mix_ratio\n",
        "                    if random.random() < mix_ratio and ev_id in evidence:\n",
        "                        self.samples.append(\n",
        "                            {\n",
        "                                \"claim_id\": claim_id,\n",
        "                                \"claim_text\": claim_text,\n",
        "                                \"evidence_id\": ev_id,\n",
        "                                \"evidence_text\": evidence[ev_id],\n",
        "                                \"label\": label,\n",
        "                                \"is_ground_truth\": False,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "        print(f\"Created dataset with {len(self.samples)} samples\")\n",
        "\n",
        "        # Print ground truth vs retrieved evidence statistics\n",
        "        ground_truth_count = sum(\n",
        "            1 for sample in self.samples if sample[\"is_ground_truth\"]\n",
        "        )\n",
        "        retrieved_count = len(self.samples) - ground_truth_count\n",
        "        print(f\"Ground truth evidence: {ground_truth_count}\")\n",
        "        print(f\"Retrieved evidence: {retrieved_count}\")\n",
        "        print(f\"Ratio of retrieved to total: {retrieved_count/len(self.samples):.2f}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Tokenize and prepare input sequence\n",
        "        encoded = prepare_input_sequence(\n",
        "            sample[\"claim_text\"],\n",
        "            sample[\"evidence_text\"],\n",
        "            self.tokenizer,\n",
        "            self.max_length,\n",
        "        )\n",
        "\n",
        "        # Convert to tensors\n",
        "        item = {\n",
        "            \"input_ids\": torch.tensor(encoded[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(encoded[\"attention_mask\"]),\n",
        "            \"label\": torch.tensor(sample[\"label\"]),\n",
        "        }\n",
        "\n",
        "        # Add token_type_ids only if not using DistilBERT\n",
        "        if \"token_type_ids\" in encoded:\n",
        "            item[\"token_type_ids\"] = torch.tensor(encoded[\"token_type_ids\"])\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9-jZ1GMBY6S8"
      },
      "outputs": [],
      "source": [
        "class ClaimVerificationModel(nn.Module):\n",
        "    def __init__(self, num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Transformer-based model for claim verification.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
        "        self.evidence_attention = nn.Linear(self.transformer.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs\n",
        "            attention_mask: Attention mask\n",
        "            token_type_ids: Optional token type IDs\n",
        "            labels: Optional labels for loss calculation\n",
        "\n",
        "        Returns:\n",
        "            If labels provided: (loss, logits)\n",
        "            If labels not provided: logits\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if 'distilbert' in VERIFICATION_MODEL_NAME.lower():\n",
        "            outputs = self.transformer(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "        else:\n",
        "            outputs = self.transformer(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "        # Get CLS token representation\n",
        "        cls_output = outputs[0][:, 0, :]  # CLS token output\n",
        "        # Calculate attention scores - apply consistently regardless of training mode\n",
        "        attention_scores = self.evidence_attention(cls_output)\n",
        "        # Apply the attention as a weighting factor\n",
        "        cls_output = cls_output * torch.sigmoid(attention_scores)\n",
        "\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        # Calculate loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits\n",
        "\n",
        "    def get_attention_score(self, cls_output):\n",
        "        \"\"\"Calculate attention score for evidence weighting\"\"\"\n",
        "        return torch.sigmoid(self.evidence_attention(cls_output))\n",
        "\n",
        "    def predict_with_evidence_set(self, claim_text, evidence_texts, tokenizer):\n",
        "        \"\"\"\n",
        "        Process multiple evidence passages for one claim and make a prediction.\n",
        "\n",
        "        Args:\n",
        "            claim_text: The claim text\n",
        "            evidence_texts: List of evidence texts\n",
        "            tokenizer: Tokenizer for encoding\n",
        "\n",
        "        Returns:\n",
        "            Probability distribution over the 4 classes\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        all_logits = []\n",
        "        all_attention = []\n",
        "\n",
        "        # Default if no evidence\n",
        "        if not evidence_texts:\n",
        "            print(f\"No evidence passages provided.\")\n",
        "            return torch.tensor([0, 0, 1, 0], device=device)  # Default to NOT_ENOUGH_INFO\n",
        "\n",
        "        # Process each evidence passage\n",
        "        for evidence_text in evidence_texts:\n",
        "            # Prepare input\n",
        "            encoded = prepare_input_sequence(claim_text, evidence_text, tokenizer, VERIFICATION_MAX_LENGTH)\n",
        "            input_ids = torch.tensor([encoded[\"input_ids\"]]).to(device)\n",
        "            attention_mask = torch.tensor([encoded[\"attention_mask\"]]).to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            with torch.no_grad():\n",
        "                if 'distilbert' in VERIFICATION_MODEL_NAME.lower():\n",
        "                    outputs = self.transformer(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    token_type_ids = torch.tensor([encoded[\"token_type_ids\"]]).to(device) if \"token_type_ids\" in encoded else None\n",
        "                    outputs = self.transformer(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        token_type_ids=token_type_ids\n",
        "                    )\n",
        "\n",
        "                cls_output = outputs[0][:, 0, :]\n",
        "\n",
        "                # Get attention score\n",
        "                attention_score = self.get_attention_score(cls_output)\n",
        "\n",
        "                # Get logits\n",
        "                logits = self.classifier(self.dropout(cls_output))\n",
        "\n",
        "                all_logits.append(logits)\n",
        "                all_attention.append(attention_score)\n",
        "\n",
        "        # Stack results\n",
        "        stacked_logits = torch.cat(all_logits, dim=0)\n",
        "        stacked_attention = torch.cat(all_attention, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "        # Normalize attention\n",
        "        norm_attention = F.softmax(stacked_attention.squeeze(-1), dim=0)\n",
        "\n",
        "        # Weighted average of logits\n",
        "        weighted_logits = torch.sum(stacked_logits * norm_attention.unsqueeze(-1), dim=0)\n",
        "\n",
        "        # Return probabilities\n",
        "        return F.softmax(weighted_logits, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-UZYQQPtY6S9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def get_loss_function(labels=None, use_class_weights=False):\n",
        "    \"\"\"Get loss function with optional class weighting.\"\"\"\n",
        "    if use_class_weights and labels is not None:\n",
        "        # Calculate class weights\n",
        "        class_counts = torch.bincount(labels)\n",
        "        class_weights = len(labels) / (len(class_counts) * class_counts)\n",
        "        class_weights = class_weights.to(device)\n",
        "\n",
        "        print(f\"Using class weights: {class_weights}\")\n",
        "        return nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler=None):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        # Move batch to device\n",
        "        optimizer.zero_grad()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "            loss, logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        else:\n",
        "            loss, logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "                loss, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            else:\n",
        "                loss, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Option 2: Calculate F1 score (macro-averaged)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    # Option 3: Calculate balanced accuracy\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, target_names=list(LABEL_MAP.keys()), output_dict=True)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"loss\": total_loss / len(dataloader),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_score\": f1,  # Added F1 score\n",
        "        \"balanced_accuracy\": balanced_acc,  # Added balanced accuracy\n",
        "        \"report\": report,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, num_epochs=VERIFICATION_NUM_EPOCHS,\n",
        "                batch_size=VERIFICATION_BATCH_SIZE, learning_rate=VERIFICATION_LEARNING_RATE,\n",
        "                use_class_weights=USE_CLASS_WEIGHTS, save_path=FINAL_MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Train the claim verification model.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_dataset: Training dataset\n",
        "        val_dataset: Validation dataset\n",
        "        num_epochs: Number of training epochs\n",
        "        batch_size: Batch size\n",
        "        learning_rate: Learning rate\n",
        "        use_class_weights: Whether to use class weighting\n",
        "        save_path: Path to save the best model\n",
        "\n",
        "    Returns:\n",
        "        The trained model and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Get loss function\n",
        "    if use_class_weights:\n",
        "        all_labels = torch.tensor([sample[\"label\"] for i, sample in enumerate(train_dataset)])\n",
        "        loss_fn = get_loss_function(all_labels, use_class_weights=True)\n",
        "    else:\n",
        "        loss_fn = get_loss_function()\n",
        "\n",
        "     # Initialize tracking variables\n",
        "    best_val_accuracy = 0.0\n",
        "    best_val_f1 = 0.0  # For Option 2\n",
        "    best_val_balanced_acc = 0.0  # For Option 3\n",
        "    best_epoch = 0\n",
        "    training_stats = []\n",
        "\n",
        "      # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer)\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        val_metrics = evaluate(model, val_loader)\n",
        "        print(f\"Validation loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"F1 Score (macro): {val_metrics['f1_score']:.4f}, Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        # Print detailed metrics\n",
        "        print(\"\\nValidation Report:\")\n",
        "        for label, metrics in val_metrics['report'].items():\n",
        "            if label in LABEL_MAP_REVERSE:\n",
        "                print(f\"  {LABEL_MAP_REVERSE[int(label)]}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1-score']:.4f}\")\n",
        "\n",
        "        # Save stats\n",
        "        training_stats.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_metrics[\"loss\"],\n",
        "            \"val_accuracy\": val_metrics[\"accuracy\"],\n",
        "            \"val_f1_score\": val_metrics[\"f1_score\"],  # Added F1 score\n",
        "            \"val_balanced_accuracy\": val_metrics[\"balanced_accuracy\"],  # Added balanced accuracy\n",
        "            \"val_report\": val_metrics[\"report\"]\n",
        "        })\n",
        "\n",
        "        # Option 1 : Save best model based on accuracy\n",
        "        # if val_metrics[\"accuracy\"] > best_val_accuracy:\n",
        "        #    best_val_accuracy = val_metrics[\"accuracy\"]\n",
        "        #    best_epoch = epoch + 1\n",
        "        #    torch.save(model.state_dict(), save_path)\n",
        "        #    print(f\"New best model saved with accuracy {best_val_accuracy:.4f}\")\n",
        "\n",
        "        # Option 2: Save best model based on F1 score\n",
        "        if val_metrics[\"f1_score\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"f1_score\"]\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), save_path)  # Added weights_only=True\n",
        "            print(f\"New best model saved with F1 score {best_val_f1:.4f}\")\n",
        "\n",
        "        # Option 3: Save best model based on balanced accuracy\n",
        "        # if val_metrics[\"balanced_accuracy\"] > best_val_balanced_acc:\n",
        "        #    best_val_balanced_acc = val_metrics[\"balanced_accuracy\"]\n",
        "        #    best_epoch = epoch + 1\n",
        "        #    torch.save(model.state_dict(), save_path, weights_only=True)\n",
        "        #    print(f\"New best model saved with balanced accuracy {best_val_balanced_acc:.4f}\")\n",
        "\n",
        "    # Print training summary\n",
        "    if best_val_f1 > 0:  # If using Option 2\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with F1 score {best_val_f1:.4f}\")\n",
        "    elif best_val_balanced_acc > 0:  # If using Option 3\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with balanced accuracy {best_val_balanced_acc:.4f}\")\n",
        "    else:  # If using original Option 1\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with accuracy {best_val_accuracy:.4f}\")\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(save_path, weights_only=True))  # Added weights_only=True\n",
        "\n",
        "    return model, training_stats\n",
        "\n",
        "def visualize_confusion_matrix(confusion_matrix, classes):\n",
        "    \"\"\"\n",
        "    Visualize the confusion matrix.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVL1u9lmY6S-"
      },
      "outputs": [],
      "source": [
        "def train_verification_model(train_claims, dev_claims, evidence, retriever=None,\n",
        "                            use_mixed_evidence=USE_MIXED_EVIDENCE,\n",
        "                            use_class_weights=USE_CLASS_WEIGHTS):\n",
        "    \"\"\"\n",
        "    Train the verification model with configurable training phases.\n",
        "\n",
        "    Args:\n",
        "        train_claims: Training claims\n",
        "        dev_claims: Development claims\n",
        "        evidence: Evidence corpus\n",
        "        retriever: Optional retriever for getting training evidence\n",
        "        use_mixed_evidence: Whether to use Phase 2 (mixed evidence training)\n",
        "        use_class_weights: Whether to use class weighting\n",
        "\n",
        "    Returns:\n",
        "        Trained model and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    model = ClaimVerificationModel(num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME)\n",
        "    model.to(device)\n",
        "\n",
        "    # Phase 1: Train on ground truth evidence\n",
        "    print(\"\\n=== Phase 1: Training on ground truth evidence ===\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ClaimVerificationDataset(\n",
        "        claims=train_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    val_dataset = ClaimVerificationDataset(\n",
        "        claims=dev_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    # Train for Phase 1\n",
        "    model, phase1_stats = train_model(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        use_class_weights=use_class_weights,\n",
        "        save_path=PHASE1_MODEL_PATH\n",
        "    )\n",
        "\n",
        "    phase1_accuracy = phase1_stats[-1][\"val_accuracy\"]\n",
        "    print(f\"Phase 1 Model Accuracy: {phase1_accuracy:.4f}\")\n",
        "\n",
        "    # Phase 2: Mix ground truth and retrieved evidence (optional)\n",
        "    if use_mixed_evidence and retriever is not None:\n",
        "        print(\"\\n=== Phase 2: Training with mixed evidence ===\")\n",
        "\n",
        "        # Get retrieved evidence for training set\n",
        "        print(\"Retrieving evidence for training claims...\")\n",
        "        train_retrieved = {}\n",
        "        for claim_id, claim_data in tqdm(train_claims.items()):\n",
        "            # Skip if no ground truth label\n",
        "            if \"claim_label\" not in claim_data:\n",
        "                continue\n",
        "\n",
        "            # Get claim text\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "\n",
        "            # Retrieve evidence\n",
        "            evidence_ids, _ = retriever.retrieve(claim_text)\n",
        "            train_retrieved[claim_id] = evidence_ids\n",
        "\n",
        "        # Create mixed dataset\n",
        "        mixed_train_dataset = ClaimVerificationDataset(\n",
        "            claims=train_claims,\n",
        "            evidence=evidence,\n",
        "            tokenizer=tokenizer,\n",
        "            use_ground_truth=True,\n",
        "            retrieved_evidence=train_retrieved,\n",
        "            mix_ratio=0.5\n",
        "        )\n",
        "\n",
        "        # Continue training from Phase 1 model\n",
        "        model.load_state_dict(torch.load(PHASE1_MODEL_PATH,weights_only=True))\n",
        "\n",
        "        # Train for Phase 2\n",
        "        model, phase2_stats = train_model(\n",
        "            model=model,\n",
        "            train_dataset=mixed_train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            num_epochs=2,  # Fewer epochs for Phase 2\n",
        "            use_class_weights=use_class_weights,\n",
        "            save_path=PHASE2_MODEL_PATH\n",
        "        )\n",
        "\n",
        "        phase2_accuracy = phase2_stats[-1][\"val_accuracy\"]\n",
        "        print(f\"Phase 2 Model Accuracy: {phase2_accuracy:.4f}\")\n",
        "        print(f\"Improvement: {phase2_accuracy - phase1_accuracy:.4f}\")\n",
        "\n",
        "        return model, {\"phase1\": phase1_stats, \"phase2\": phase2_stats}\n",
        "\n",
        "    return model, {\"phase1\": phase1_stats}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9sEVeQI_Y6S_"
      },
      "outputs": [],
      "source": [
        "def predict_claims(model, claims, evidence, retrieved_evidence, tokenizer):\n",
        "    \"\"\"\n",
        "    Make predictions on claims using retrieved evidence.\n",
        "\n",
        "    Args:\n",
        "        model: Trained verification model\n",
        "        claims: Dictionary of claims\n",
        "        evidence: Evidence corpus\n",
        "        retrieved_evidence: Dictionary mapping claim IDs to lists of evidence IDs\n",
        "        tokenizer: Tokenizer for preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping claim IDs to predicted labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = {}\n",
        "\n",
        "    print(f\"Making predictions for {len(claims)} claims...\")\n",
        "    for claim_id, claim_data in tqdm(claims.items()):\n",
        "        claim_text = claim_data[\"claim_text\"]\n",
        "\n",
        "        # Get evidence texts\n",
        "        evidence_ids = retrieved_evidence.get(claim_id, [])\n",
        "        evidence_texts = [evidence[eid] for eid in evidence_ids if eid in evidence]\n",
        "\n",
        "        # Skip if no evidence (default to NOT_ENOUGH_INFO)\n",
        "        if not evidence_texts:\n",
        "            print(f\"No evidence passages provided HOWWWWW.\")\n",
        "            predictions[claim_id] = \"NOT_ENOUGH_INFO\"\n",
        "            continue\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            probs = model.predict_with_evidence_set(claim_text, evidence_texts, tokenizer)\n",
        "\n",
        "        # Get class with highest probability\n",
        "        label_id = torch.argmax(probs).item()\n",
        "        predictions[claim_id] = LABEL_MAP_REVERSE[label_id]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def update_predictions_file(predictions_path, predictions):\n",
        "    \"\"\"Update predictions file with claim labels.\"\"\"\n",
        "    # Load current predictions\n",
        "    with open(predictions_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Update with new predictions\n",
        "    for claim_id, label in predictions.items():\n",
        "        if claim_id in data:\n",
        "            data[claim_id][\"claim_label\"] = label\n",
        "\n",
        "    # Save updated predictions\n",
        "    with open(predictions_path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    print(f\"Updated predictions saved to {predictions_path}\")\n",
        "\n",
        "def evaluate_predictions(predictions, groundtruth):\n",
        "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Collect all predictions and labels\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for claim_id, claim_data in groundtruth.items():\n",
        "        if claim_id in predictions:\n",
        "            true_label = claim_data[\"claim_label\"]\n",
        "            pred_label = predictions[claim_id]\n",
        "\n",
        "            y_true.append(LABEL_MAP[true_label])\n",
        "            y_pred.append(LABEL_MAP[pred_label])\n",
        "\n",
        "            if pred_label == true_label:\n",
        "                correct += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    # Get detailed report\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    report = classification_report(y_true, y_pred, target_names=list(LABEL_MAP.keys()), output_dict=True)\n",
        "\n",
        "    # Get confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    for label, metrics in report.items():\n",
        "        if label in LABEL_MAP_REVERSE:\n",
        "            print(f\"  {LABEL_MAP_REVERSE[int(label)]}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1-score']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"report\": report,\n",
        "        \"confusion_matrix\": cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JzWB7vP1Y6S_",
        "outputId": "51e90426-7057-43e4-9c70-54dfa5edbe73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found retriever in retrieval_results.\n",
            "\n",
            "=== Running Claim Verification (Task 2) ===\n",
            "\n",
            "=== Claim Verification Hyperparameters ===\n",
            "MODEL_NAME: distilbert-base-uncased\n",
            "BATCH_SIZE: 16\n",
            "LEARNING_RATE: 1e-05\n",
            "USE_CLASS_WEIGHTS: True\n",
            "USE_MIXED_EVIDENCE: True\n",
            "Loading /content/data/dev-claims-predictions.json...\n",
            "Successfully loaded 154 items.\n",
            "Loading /content/data/test-claims-predictions.json...\n",
            "Successfully loaded 153 items.\n",
            "Checking retriever functionality...\n",
            "Retriever test successful. Found 4 evidence passages.\n",
            "\n",
            "=== Phase 1: Training on ground truth evidence ===\n",
            "Creating dataset (use_ground_truth=True, mix_ratio=0.0)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing claims: 100%|██████████| 1228/1228 [00:00<00:00, 164893.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created dataset with 4122 samples\n",
            "Ground truth evidence: 4122\n",
            "Retrieved evidence: 0\n",
            "Ratio of retrieved to total: 0.00\n",
            "Creating dataset (use_ground_truth=True, mix_ratio=0.0)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing claims: 100%|██████████| 154/154 [00:00<00:00, 105422.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created dataset with 491 samples\n",
            "Ground truth evidence: 491\n",
            "Retrieved evidence: 0\n",
            "Ratio of retrieved to total: 0.00\n",
            "\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using class weights: tensor([0.7673, 2.2549, 0.5339, 2.6288], device='cuda:0')\n",
            "\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [03:10<00:00,  1.36it/s, loss=1.09]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 1.0965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.14it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.1814, Accuracy: 0.5153\n",
            "F1 Score (macro): 0.3342, Balanced Accuracy: 0.3554\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3342\n",
            "\n",
            "Epoch 2/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [03:09<00:00,  1.36it/s, loss=0.378]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.6830\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.3107, Accuracy: 0.5010\n",
            "F1 Score (macro): 0.3822, Balanced Accuracy: 0.3986\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3822\n",
            "\n",
            "Epoch 3/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [03:10<00:00,  1.36it/s, loss=0.0568]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.3027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.6194, Accuracy: 0.5051\n",
            "F1 Score (macro): 0.4211, Balanced Accuracy: 0.4227\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.4211\n",
            "\n",
            "Epoch 4/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [03:09<00:00,  1.36it/s, loss=0.019]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.1201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 1.9792, Accuracy: 0.5173\n",
            "F1 Score (macro): 0.4435, Balanced Accuracy: 0.4476\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.4435\n",
            "\n",
            "Epoch 5/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [03:09<00:00,  1.36it/s, loss=0.00878]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.0579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 2.1376, Accuracy: 0.5214\n",
            "F1 Score (macro): 0.4218, Balanced Accuracy: 0.4221\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Epoch 6/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 258/258 [03:09<00:00,  1.36it/s, loss=0.0202]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.0314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 2.4996, Accuracy: 0.4949\n",
            "F1 Score (macro): 0.3930, Balanced Accuracy: 0.4030\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Training complete. Best model from epoch 4 with F1 score 0.4435\n",
            "Phase 1 Model Accuracy: 0.4949\n",
            "\n",
            "=== Phase 2: Training with mixed evidence ===\n",
            "Retrieving evidence for training claims...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1228/1228 [19:46<00:00,  1.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dataset (use_ground_truth=True, mix_ratio=0.5)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing claims: 100%|██████████| 1228/1228 [00:00<00:00, 99116.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created dataset with 6232 samples\n",
            "Ground truth evidence: 4122\n",
            "Retrieved evidence: 2110\n",
            "Ratio of retrieved to total: 0.34\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Module.load_state_dict() got an unexpected keyword argument 'weights_only'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-b9812e2bbed7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retriever not found in retrieval_results.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Running Claim Verification (Task 2) ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     model, stats, metrics = run_claim_verification(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mtrain_claims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_claims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdev_claims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_claims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-b9812e2bbed7>\u001b[0m in \u001b[0;36mrun_claim_verification\u001b[0;34m(train_claims, dev_claims, test_claims, evidence, retriever)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     model, stats = train_verification_model(\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtrain_claims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_claims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mdev_claims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_claims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-f874da11feb1>\u001b[0m in \u001b[0;36mtrain_verification_model\u001b[0;34m(train_claims, dev_claims, evidence, retriever, use_mixed_evidence, use_class_weights)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Continue training from Phase 1 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPHASE1_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Train for Phase 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Module.load_state_dict() got an unexpected keyword argument 'weights_only'"
          ]
        }
      ],
      "source": [
        "def run_claim_verification(train_claims, dev_claims, test_claims, evidence, retriever=None):\n",
        "    \"\"\"\n",
        "    Run the claim verification pipeline and generate predictions.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Claim Verification Hyperparameters ===\")\n",
        "    print(f\"MODEL_NAME: {VERIFICATION_MODEL_NAME}\")\n",
        "    print(f\"BATCH_SIZE: {VERIFICATION_BATCH_SIZE}\")\n",
        "    print(f\"LEARNING_RATE: {VERIFICATION_LEARNING_RATE}\")\n",
        "    print(f\"USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\")\n",
        "    print(f\"USE_MIXED_EVIDENCE: {USE_MIXED_EVIDENCE}\")\n",
        "\n",
        "    # Load retrieved evidence from Task 1\n",
        "    dev_predictions = load_json(DEV_PREDICTIONS_PATH)\n",
        "    test_predictions = load_json(TEST_PREDICTIONS_PATH)\n",
        "\n",
        "    # Extract retrieved evidence\n",
        "    dev_retrieved = {claim_id: data[\"evidences\"] for claim_id, data in dev_predictions.items()}\n",
        "    test_retrieved = {claim_id: data[\"evidences\"] for claim_id, data in test_predictions.items()}\n",
        "\n",
        "    # Check if retriever is valid\n",
        "    if retriever is not None:\n",
        "        print(\"Checking retriever functionality...\")\n",
        "        try:\n",
        "            # Test retrieve method on a sample claim\n",
        "            sample_claim_id = next(iter(train_claims))\n",
        "            sample_claim_text = train_claims[sample_claim_id][\"claim_text\"]\n",
        "            sample_evidence, _ = retriever.retrieve(sample_claim_text)\n",
        "            print(f\"Retriever test successful. Found {len(sample_evidence)} evidence passages.\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Retriever test failed: {e}\")\n",
        "\n",
        "    # Train model\n",
        "    model, stats = train_verification_model(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims,\n",
        "        evidence=evidence,\n",
        "        retriever=retriever,\n",
        "        use_mixed_evidence=USE_MIXED_EVIDENCE,\n",
        "        use_class_weights=USE_CLASS_WEIGHTS\n",
        "    )\n",
        "\n",
        "    # Initialize tokenizer for predictions\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "\n",
        "    # Make predictions on dev set\n",
        "    print(\"\\nMaking predictions on development set...\")\n",
        "    dev_pred_labels = predict_claims(model, dev_claims, evidence, dev_retrieved, tokenizer)\n",
        "\n",
        "    # Evaluate dev predictions\n",
        "    print(\"\\nEvaluating development predictions:\")\n",
        "    dev_metrics = evaluate_predictions(dev_pred_labels, dev_claims)\n",
        "\n",
        "    # Update dev predictions file\n",
        "    update_predictions_file(DEV_PREDICTIONS_PATH, dev_pred_labels)\n",
        "\n",
        "    # Make predictions on test set\n",
        "    print(\"\\nMaking predictions on test set...\")\n",
        "    test_pred_labels = predict_claims(model, test_claims, evidence, test_retrieved, tokenizer)\n",
        "\n",
        "    # Update test predictions file\n",
        "    update_predictions_file(TEST_PREDICTIONS_PATH, test_pred_labels)\n",
        "\n",
        "    print(\"\\nClaim verification completed!\")\n",
        "    return model, stats, dev_metrics\n",
        "\n",
        "# Main execution - running task 2\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if we already have retrieval results\n",
        "    if 'retrieval_results' not in locals() and os.path.exists(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')):\n",
        "        print(\"Loading saved retrieval results...\")\n",
        "        retrieval_results = torch.load(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt'),weights_only=True)\n",
        "    # Check if retriever is available\n",
        "    if 'retrieval_results' in locals() and 'retriever' in retrieval_results:\n",
        "        retriever = retrieval_results[\"retriever\"]\n",
        "        print(\"Found retriever in retrieval_results.\")\n",
        "    else:\n",
        "        print(\"Retriever not found in retrieval_results.\")\n",
        "    print(\"\\n=== Running Claim Verification (Task 2) ===\")\n",
        "    model, stats, metrics = run_claim_verification(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims,\n",
        "        test_claims=test_claims,\n",
        "        evidence=evidence,\n",
        "        retriever=retriever\n",
        "    )\n",
        "\n",
        "    # Visualize confusion matrix\n",
        "    visualize_confusion_matrix(\n",
        "        metrics[\"confusion_matrix\"],\n",
        "        list(LABEL_MAP.keys())\n",
        "    )\n",
        "\n",
        "    print(\"\\nTask 2 completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hblEzunwY6S_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGWzlldNY6S_"
      },
      "source": [
        "#### Diagnose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A57J0UDSY6S_"
      },
      "outputs": [],
      "source": [
        "def diagnose_dataset(train_claims, dev_claims, evidence):\n",
        "    print(\"\\n=== Dataset Diagnosis ===\")\n",
        "\n",
        "    # Check label distribution in claims\n",
        "    train_labels = {}\n",
        "    for claim_id, claim_data in train_claims.items():\n",
        "        if \"claim_label\" in claim_data:\n",
        "            label = claim_data[\"claim_label\"]\n",
        "            if label not in train_labels:\n",
        "                train_labels[label] = 0\n",
        "            train_labels[label] += 1\n",
        "\n",
        "    print(\"\\nLabel Distribution in Training Claims:\")\n",
        "    total_train = sum(train_labels.values())\n",
        "    for label, count in train_labels.items():\n",
        "        print(f\"  {label}: {count} claims ({count/total_train*100:.2f}%)\")\n",
        "\n",
        "    # Check average number of evidence passages per claim\n",
        "    train_evidence_counts = []\n",
        "    for claim_id, claim_data in train_claims.items():\n",
        "        if \"evidences\" in claim_data:\n",
        "            train_evidence_counts.append(len(claim_data[\"evidences\"]))\n",
        "\n",
        "    print(\"\\nEvidence per Claim in Training Set:\")\n",
        "    if train_evidence_counts:\n",
        "        print(f\"  Average: {np.mean(train_evidence_counts):.2f}\")\n",
        "        print(f\"  Min: {min(train_evidence_counts)}\")\n",
        "        print(f\"  Max: {max(train_evidence_counts)}\")\n",
        "\n",
        "    # Create a test dataset with the actual preprocessing\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    test_dataset = ClaimVerificationDataset(\n",
        "        claims=train_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    # Check processed sample distribution\n",
        "    sample_labels = {}\n",
        "    for sample in test_dataset.samples:\n",
        "        label = sample[\"label\"]\n",
        "        if label not in sample_labels:\n",
        "            sample_labels[label] = 0\n",
        "        sample_labels[label] += 1\n",
        "\n",
        "    print(\"\\nLabel Distribution in Processed Samples:\")\n",
        "    total_samples = len(test_dataset.samples)\n",
        "    for label, count in sample_labels.items():\n",
        "        label_name = LABEL_MAP_REVERSE.get(label, f\"Unknown ({label})\")\n",
        "        print(f\"  {label_name}: {count} samples ({count/total_samples*100:.2f}%)\")\n",
        "\n",
        "    # Check for data processing issues by examining a few samples\n",
        "    print(\"\\nSample Check (first 3 samples):\")\n",
        "    for i in range(min(3, len(test_dataset))):\n",
        "        sample = test_dataset[i]\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"  Label: {LABEL_MAP_REVERSE.get(sample['label'].item(), 'Unknown')}\")\n",
        "\n",
        "        # Decode the input IDs to check tokenization\n",
        "        input_text = tokenizer.decode(sample[\"input_ids\"])\n",
        "        print(f\"  Tokenized text (truncated): {input_text[:100]}...\")\n",
        "\n",
        "        # Check if special tokens are properly placed\n",
        "        cls_token_id = tokenizer.cls_token_id\n",
        "        sep_token_id = tokenizer.sep_token_id\n",
        "        has_cls = sample[\"input_ids\"][0] == cls_token_id\n",
        "        has_sep = sep_token_id in sample[\"input_ids\"]\n",
        "\n",
        "        print(f\"  Has CLS token at start: {has_cls}\")\n",
        "        print(f\"  Has SEP token: {has_sep}\")\n",
        "\n",
        "        # Check attention mask\n",
        "        valid_tokens = sum(sample[\"attention_mask\"].tolist())\n",
        "        print(f\"  Valid tokens: {valid_tokens}/{len(sample['input_ids'])}\")\n",
        "\n",
        "    return test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHTkK4vlY6S_"
      },
      "outputs": [],
      "source": [
        "def diagnose_model_predictions(model, test_dataset):\n",
        "    print(\"\\n=== Model Prediction Diagnosis ===\")\n",
        "\n",
        "    # Create a small subset of the dataset for quick testing\n",
        "    subset_size = min(100, len(test_dataset))\n",
        "    subset_indices = np.random.choice(len(test_dataset), subset_size, replace=False)\n",
        "\n",
        "    # Create a dataloader for the subset\n",
        "    batch_size = 16\n",
        "    subset_sampler = torch.utils.data.SubsetRandomSampler(subset_indices)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=subset_sampler)\n",
        "\n",
        "    # Move model to device and set to evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass (handle models with and without token_type_ids)\n",
        "            if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "                _, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            else:\n",
        "                _, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "\n",
        "            # Get predictions\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "            # Print some example predictions\n",
        "            if len(all_preds) <= 5:  # Just show the first 5 examples\n",
        "                for i in range(len(preds)):\n",
        "                    print(f\"\\nPrediction example {len(all_preds) - len(preds) + i + 1}:\")\n",
        "                    print(f\"  True label: {LABEL_MAP_REVERSE.get(labels[i], 'Unknown')}\")\n",
        "                    print(f\"  Predicted: {LABEL_MAP_REVERSE.get(preds[i], 'Unknown')}\")\n",
        "                    print(f\"  Probabilities: {probs[i].cpu().numpy().round(3)}\")\n",
        "\n",
        "    # Show prediction distribution\n",
        "    pred_counts = {}\n",
        "    for pred in all_preds:\n",
        "        if pred not in pred_counts:\n",
        "            pred_counts[pred] = 0\n",
        "        pred_counts[pred] += 1\n",
        "\n",
        "    print(\"\\nPrediction Distribution:\")\n",
        "    for pred, count in pred_counts.items():\n",
        "        pred_name = LABEL_MAP_REVERSE.get(pred, f\"Unknown ({pred})\")\n",
        "        print(f\"  {pred_name}: {count} predictions ({count/len(all_preds)*100:.2f}%)\")\n",
        "\n",
        "    # Show prediction vs actual distribution\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    labels = [LABEL_MAP_REVERSE.get(i, f\"Unknown\") for i in range(max(LABEL_MAP.values()) + 1)]\n",
        "\n",
        "    # Print simple text version of confusion matrix\n",
        "    print(\"True \\\\ Pred\", end=\"\")\n",
        "    for label in labels:\n",
        "        print(f\" | {label[:3]}\", end=\"\")\n",
        "    print()\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"{label[:7]}\", end=\"\")\n",
        "        for j in range(len(labels)):\n",
        "            if i < cm.shape[0] and j < cm.shape[1]:\n",
        "                print(f\" | {cm[i, j]:3d}\", end=\"\")\n",
        "            else:\n",
        "                print(\" |   0\", end=\"\")\n",
        "        print()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\nAccuracy on subset: {accuracy:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiNF2khnY6TA"
      },
      "outputs": [],
      "source": [
        "def diagnose_gradient_flow(model, test_dataset):\n",
        "    print(\"\\n=== Gradient Flow Diagnosis ===\")\n",
        "\n",
        "    # Create a small subset of the dataset for quick testing\n",
        "    subset_size = min(32, len(test_dataset))\n",
        "    subset_indices = np.random.choice(len(test_dataset), subset_size, replace=False)\n",
        "\n",
        "    # Create a dataloader for the subset\n",
        "    batch_size = 16\n",
        "    subset_sampler = torch.utils.data.SubsetRandomSampler(subset_indices)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=subset_sampler)\n",
        "\n",
        "    # Move model to device and set to training mode\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=VERIFICATION_LEARNING_RATE)\n",
        "\n",
        "    # Get a batch\n",
        "    batch = next(iter(test_loader))\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    # Record initial parameter values\n",
        "    initial_params = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            initial_params[name] = param.clone().detach().cpu().numpy()\n",
        "            print(f\"  {name}: shape={param.shape}, mean={param.data.mean().item():.6f}, std={param.data.std().item():.6f}\")\n",
        "\n",
        "    # Forward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "        loss, logits = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            token_type_ids=batch[\"token_type_ids\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "    else:\n",
        "        loss, logits = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "\n",
        "    print(f\"\\nLoss: {loss.item():.4f}\")\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradients\n",
        "    print(\"\\nGradient Magnitudes:\")\n",
        "    zero_grad_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.grad is None:\n",
        "                zero_grad_params.append(name)\n",
        "                print(f\"  {name}: NO GRADIENT\")\n",
        "            else:\n",
        "                grad_mean = param.grad.abs().mean().item()\n",
        "                grad_max = param.grad.abs().max().item()\n",
        "                print(f\"  {name}: mean={grad_mean:.6f}, max={grad_max:.6f}\")\n",
        "\n",
        "    if zero_grad_params:\n",
        "        print(f\"\\nWARNING: {len(zero_grad_params)} parameters have no gradient!\")\n",
        "\n",
        "    # Apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # Check parameter changes\n",
        "    print(\"\\nParameter Changes After One Update:\")\n",
        "    unchanged_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            new_value = param.clone().detach().cpu().numpy()\n",
        "            old_value = initial_params[name]\n",
        "\n",
        "            # Calculate relative change\n",
        "            abs_diff = np.abs(new_value - old_value)\n",
        "            rel_change = np.mean(abs_diff) / (np.mean(np.abs(old_value)) + 1e-9)\n",
        "\n",
        "            change_description = f\"  {name}: relative change={rel_change:.6f}\"\n",
        "\n",
        "            if rel_change == 0:\n",
        "                unchanged_params.append(name)\n",
        "                change_description += \" (NO CHANGE)\"\n",
        "\n",
        "            print(change_description)\n",
        "\n",
        "    if unchanged_params:\n",
        "        print(f\"\\nWARNING: {len(unchanged_params)} parameters did not change after update!\")\n",
        "\n",
        "    return not (zero_grad_params or unchanged_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85UC0VVyY6TA"
      },
      "outputs": [],
      "source": [
        "def diagnose_mixed_evidence(train_claims, evidence, retriever):\n",
        "    print(\"\\n=== Mixed Evidence Diagnosis ===\")\n",
        "\n",
        "    if retriever is None:\n",
        "        print(\"No retriever provided. Cannot diagnose mixed evidence functionality.\")\n",
        "        return False\n",
        "\n",
        "    # Test retrieval for a few claims\n",
        "    sample_size = 5\n",
        "    sample_claims = dict(list(train_claims.items())[:sample_size])\n",
        "\n",
        "    print(f\"Testing retrieval for {sample_size} sample claims...\")\n",
        "\n",
        "    for claim_id, claim_data in sample_claims.items():\n",
        "        print(f\"\\nClaim: {claim_data['claim_text']}\")\n",
        "\n",
        "        # Get ground truth evidence\n",
        "        if \"evidences\" in claim_data:\n",
        "            ground_truth = claim_data[\"evidences\"]\n",
        "            print(f\"Ground truth evidence count: {len(ground_truth)}\")\n",
        "\n",
        "            # Show first ground truth evidence\n",
        "            if ground_truth:\n",
        "                first_evidence_id = ground_truth[0]\n",
        "                if first_evidence_id in evidence:\n",
        "                    print(f\"Ground truth evidence sample: {evidence[first_evidence_id][:100]}...\")\n",
        "\n",
        "        # Retrieve evidence using retriever\n",
        "        try:\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "            retrieved_evidence, scores = retriever.retrieve(claim_text)\n",
        "            print(f\"Retrieved evidence count: {len(retrieved_evidence)}\")\n",
        "\n",
        "            # Show first retrieved evidence\n",
        "            if retrieved_evidence:\n",
        "                first_retrieved_id = retrieved_evidence[0]\n",
        "                if first_retrieved_id in evidence:\n",
        "                    print(f\"Retrieved evidence sample: {evidence[first_retrieved_id][:100]}...\")\n",
        "                    print(f\"Retrieval score: {scores[0]:.4f}\")\n",
        "\n",
        "            # Check overlap with ground truth\n",
        "            if \"evidences\" in claim_data:\n",
        "                overlap = set(retrieved_evidence).intersection(set(ground_truth))\n",
        "                print(f\"Overlap with ground truth: {len(overlap)}/{len(ground_truth)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Create datasets with and without mixed evidence\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "\n",
        "    # Ground truth only dataset\n",
        "    gt_dataset = ClaimVerificationDataset(\n",
        "        claims=sample_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    # Get retrieved evidence for sample claims\n",
        "    retrieved_evidence_dict = {}\n",
        "    for claim_id, claim_data in sample_claims.items():\n",
        "        claim_text = claim_data[\"claim_text\"]\n",
        "        evidence_ids, _ = retriever.retrieve(claim_text)\n",
        "        retrieved_evidence_dict[claim_id] = evidence_ids\n",
        "\n",
        "    # Mixed evidence dataset\n",
        "    mixed_dataset = ClaimVerificationDataset(\n",
        "        claims=sample_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=retrieved_evidence_dict,\n",
        "        mix_ratio=0.5\n",
        "    )\n",
        "\n",
        "    print(f\"\\nGround truth only dataset size: {len(gt_dataset)}\")\n",
        "    print(f\"Mixed evidence dataset size: {len(mixed_dataset)}\")\n",
        "\n",
        "    # Check if mixed dataset is actually larger\n",
        "    is_working = len(mixed_dataset) > len(gt_dataset)\n",
        "    print(f\"Mixed evidence functionality is {'working' if is_working else 'NOT working properly'}\")\n",
        "\n",
        "    return is_working"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdO00OOkY6TA"
      },
      "source": [
        "#### Diagnose Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPj1mdHBY6TA"
      },
      "outputs": [],
      "source": [
        "def run_full_diagnosis(train_claims, dev_claims, evidence, retriever=None):\n",
        "    \"\"\"\n",
        "    Run a full suite of diagnostic tests to identify model issues.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STARTING FULL DIAGNOSTIC TESTS ===\\n\")\n",
        "\n",
        "    # Step 1: Diagnose the dataset\n",
        "    test_dataset = diagnose_dataset(train_claims, dev_claims, evidence)\n",
        "\n",
        "    # Step 2: Create and diagnose a new model\n",
        "    print(\"\\nInitializing a fresh model for testing...\")\n",
        "    model = ClaimVerificationModel(num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME)\n",
        "    model.to(device)\n",
        "\n",
        "    # Step 3: Check gradient flow\n",
        "    gradient_flows = diagnose_gradient_flow(model, test_dataset)\n",
        "\n",
        "    # Step 4: Train for one epoch to capture initial predictions\n",
        "    print(\"\\nTraining for one epoch to check initial learning...\")\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=VERIFICATION_LEARNING_RATE)\n",
        "    train_loader = DataLoader(test_dataset, batch_size=VERIFICATION_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "            loss, _ = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        else:\n",
        "            loss, _ = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "    avg_loss /= len(train_loader)\n",
        "    print(f\"Average loss after one epoch: {avg_loss:.4f}\")\n",
        "\n",
        "    # Step 5: Check model predictions\n",
        "    all_preds, all_labels = diagnose_model_predictions(model, test_dataset)\n",
        "\n",
        "    # Step 6: Check mixed evidence functionality if retriever is available\n",
        "    if retriever is not None:\n",
        "        mixed_evidence_works = diagnose_mixed_evidence(train_claims, evidence, retriever)\n",
        "    else:\n",
        "        print(\"\\nSkipping mixed evidence diagnosis (no retriever provided)\")\n",
        "        mixed_evidence_works = None\n",
        "\n",
        "    # Summarize findings\n",
        "    print(\"\\n=== DIAGNOSTIC SUMMARY ===\")\n",
        "\n",
        "    # Check for dataset imbalance\n",
        "    label_counts = {}\n",
        "    for label in all_labels:\n",
        "        if label not in label_counts:\n",
        "            label_counts[label] = 0\n",
        "        label_counts[label] += 1\n",
        "\n",
        "    most_common_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
        "    most_common_pct = label_counts[most_common_label] / len(all_labels) * 100\n",
        "\n",
        "    if most_common_pct > 50:\n",
        "        print(f\"✘ Dataset imbalance detected: {LABEL_MAP_REVERSE.get(most_common_label)} class dominates ({most_common_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"✓ Dataset balance looks reasonable (largest class: {most_common_pct:.1f}%)\")\n",
        "\n",
        "    # Check for prediction uniformity\n",
        "    pred_counts = {}\n",
        "    for pred in all_preds:\n",
        "        if pred not in pred_counts:\n",
        "            pred_counts[pred] = 0\n",
        "        pred_counts[pred] += 1\n",
        "\n",
        "    if len(pred_counts) < len(LABEL_MAP):\n",
        "        print(f\"✘ Model is not predicting all classes (only {len(pred_counts)} out of {len(LABEL_MAP)})\")\n",
        "    else:\n",
        "        print(f\"✓ Model is predicting all {len(LABEL_MAP)} classes\")\n",
        "\n",
        "    most_common_pred = max(pred_counts.items(), key=lambda x: x[1])[0]\n",
        "    most_common_pred_pct = pred_counts[most_common_pred] / len(all_preds) * 100\n",
        "\n",
        "    if most_common_pred_pct > 70:\n",
        "        print(f\"✘ Model is biased toward {LABEL_MAP_REVERSE.get(most_common_pred)} class ({most_common_pred_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"✓ Model prediction distribution looks reasonable\")\n",
        "\n",
        "    # Check gradient flow\n",
        "    if gradient_flows:\n",
        "        print(\"✓ Gradient flow is working correctly\")\n",
        "    else:\n",
        "        print(\"✘ Gradient flow issues detected\")\n",
        "\n",
        "    # Check mixed evidence\n",
        "    if mixed_evidence_works is not None:\n",
        "        if mixed_evidence_works:\n",
        "            print(\"✓ Mixed evidence functionality is working correctly\")\n",
        "        else:\n",
        "            print(\"✘ Mixed evidence functionality is NOT working properly\")\n",
        "\n",
        "    print(\"\\n=== END OF DIAGNOSIS ===\")\n",
        "\n",
        "    return {\n",
        "        \"dataset_imbalance\": most_common_pct > 50,\n",
        "        \"prediction_bias\": most_common_pred_pct > 70,\n",
        "        \"gradient_flow_issues\": not gradient_flows,\n",
        "        \"mixed_evidence_issues\": False if mixed_evidence_works else True\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umE-x2RNY6TA"
      },
      "outputs": [],
      "source": [
        "# Run diagnostic tests\n",
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "    # Run full diagnosis\n",
        "    if 'retrieval_results' not in locals() and os.path.exists(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')):\n",
        "        print(\"Loading saved retrieval results...\")\n",
        "        retrieval_results = torch.load(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt'),weights_only=True)\n",
        "        retriever = retrieval_results.get(\"retriever\") if \"retriever\" in retrieval_results else None\n",
        "    else:\n",
        "        retriever = None\n",
        "\n",
        "    diagnosis_results = run_full_diagnosis(train_claims, dev_claims, evidence, retriever)\n",
        "\n",
        "    # Based on diagnosis, suggest fixes\n",
        "    print(\"\\n=== RECOMMENDED FIXES ===\")\n",
        "\n",
        "    if diagnosis_results[\"dataset_imbalance\"]:\n",
        "        print(\"1. Apply class weighting by setting USE_CLASS_WEIGHTS = True\")\n",
        "        print(\"2. Consider oversampling minority classes or downsampling majority classes\")\n",
        "\n",
        "    if diagnosis_results[\"prediction_bias\"]:\n",
        "        print(\"1. Reduce learning rate (try 5e-6 instead of 2e-5)\")\n",
        "        print(\"2. Train for more epochs (5-10 instead of 3)\")\n",
        "        print(\"3. Consider using focal loss instead of cross-entropy loss\")\n",
        "\n",
        "    if diagnosis_results[\"gradient_flow_issues\"]:\n",
        "        print(\"1. Check for NaN values in inputs or gradients\")\n",
        "        print(\"2. Reduce batch size if you're encountering memory issues\")\n",
        "        print(\"3. Consider gradient clipping to prevent exploding gradients\")\n",
        "\n",
        "    if diagnosis_results[\"mixed_evidence_issues\"]:\n",
        "        print(\"1. Fix the mixed evidence implementation\")\n",
        "        print(\"2. Check the retriever functionality\")\n",
        "        print(\"3. Ensure the mix_ratio is properly used in the dataset creation\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
