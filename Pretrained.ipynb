{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igumiao/nlp_G60/blob/xiaohong/Pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp_s6r6DK4zX",
        "outputId": "478e7a00-5384-4c42-ed24-2c329dba42a3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e744a6d-7922-43bf-a28e-968f5bc06af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "Loading /content/drive/MyDrive/data/train-claims.json...\n",
            "Successfully loaded 1228 items.\n",
            "Loading /content/drive/MyDrive/data/dev-claims.json...\n",
            "Successfully loaded 154 items.\n",
            "Loading /content/drive/MyDrive/data/test-claims-unlabelled.json...\n",
            "Successfully loaded 153 items.\n",
            "Loading /content/drive/MyDrive/data/evidence.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 1208827 items.\n",
            "\n",
            "=== Sample Data Structure ===\n",
            "Sample claim ID: claim-1937\n",
            "{\n",
            "  \"claim_text\": \"Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\",\n",
            "  \"claim_label\": \"DISPUTED\",\n",
            "  \"evidences\": [\n",
            "    \"evidence-442946\",\n",
            "    \"evidence-1194317\",\n",
            "    \"evidence-12171\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Sample evidence ID: evidence-0\n",
            "Evidence text: John Bennet Lawes, English entrepreneur and agricultural scientist\n",
            "Original claim: Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
            "Basic preprocessing: not only is there no scientific evidence that co2 is a pollutant higher co2 concentrations actually help ecosystems support more plant and animal life\n",
            "With stopword removal: scientific evidence co2 pollutant higher co2 concentrations actually help ecosystems support plant animal life\n",
            "With lemmatization: not only is there no scientific evidence that co2 is a pollutant higher co2 concentration actually help ecosystem support more plant and animal life\n",
            "With stemming: not onli is there no scientif evid that co2 is a pollut higher co2 concentr actual help ecosystem support more plant and anim life\n",
            "\n",
            "Original evidence: John Bennet Lawes, English entrepreneur and agricultural scientist\n",
            "Basic preprocessing: john bennet lawes english entrepreneur and agricultural scientist\n",
            "With stopword removal: john bennet lawes english entrepreneur agricultural scientist\n",
            "\n",
            "Data processing complete!\n"
          ]
        }
      ],
      "source": [
        "# 1. DataSet Processing\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# --- Configuration ---\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/data'  # Path to the data directory\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# File paths\n",
        "TRAIN_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'train-claims.json')\n",
        "DEV_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'dev-claims.json')\n",
        "TEST_CLAIMS_PATH = os.path.join(DRIVE_DATA_PATH, 'test-claims-unlabelled.json')\n",
        "EVIDENCE_PATH = os.path.join(DRIVE_DATA_PATH, 'evidence.json')\n",
        "# DEV_PREDICTIONS_PATH = os.path.join(DRIVE_DATA_PATH, 'dev-claims-predictions.json')\n",
        "DEV_PREDICTIONS_PATH = os.path.join(DRIVE_DATA_PATH, 'WithAttentionONON.json')\n",
        "TEST_PREDICTIONS_PATH = os.path.join(DRIVE_DATA_PATH, 'test-claims-predictions.json')\n",
        "EVAL_SCRIPT_PATH = os.path.join('eval.py')\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_json(filepath):\n",
        "    \"\"\"Loads JSON data from a file.\"\"\"\n",
        "    print(f\"Loading {filepath}...\")\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"Successfully loaded {len(data)} items.\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error: Could not decode JSON from {filepath}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text, remove_stop_words=False, lemmatize=False, stem=False):\n",
        "    \"\"\"\n",
        "    Preprocess text with multiple options:\n",
        "    - Lowercase\n",
        "    - Remove special characters\n",
        "    - Optional: Remove stopwords\n",
        "    - Optional: Lemmatization\n",
        "    - Optional: Stemming\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and extra spaces\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords if requested\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply lemmatization if requested\n",
        "    if lemmatize:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Apply stemming if requested\n",
        "    if stem:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join the tokens back to a string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "def basic_dataset_analysis(data_dict, name):\n",
        "    \"\"\"Basic analysis of a dataset (claims or evidence).\"\"\"\n",
        "    print(f\"\\n=== Basic Analysis of {name} Dataset ===\")\n",
        "    print(f\"Number of items: {len(data_dict)}\")\n",
        "\n",
        "    # For claims, check label distribution\n",
        "    if name == 'Claims' and 'claim_label' in next(iter(data_dict.values())):\n",
        "        labels = [item.get('claim_label') for item in data_dict.values() if 'claim_label' in item]\n",
        "        label_counts = Counter(labels)\n",
        "        print(\"\\nLabel Distribution:\")\n",
        "        for label, count in label_counts.items():\n",
        "            print(f\"  {label}: {count} ({count/len(labels)*100:.2f}%)\")\n",
        "\n",
        "    # Check text lengths\n",
        "    if 'claim_text' in next(iter(data_dict.values()), {}):\n",
        "        # For claims\n",
        "        text_field = 'claim_text'\n",
        "        text_lengths = [len(item[text_field].split()) for item in data_dict.values()]\n",
        "    else:\n",
        "        # For evidence\n",
        "        text_lengths = [len(text.split()) for i, text in enumerate(data_dict.values()) if i < 1000]\n",
        "        text_field = 'evidence text'\n",
        "\n",
        "    print(f\"\\n{text_field.capitalize()} Length Statistics (in words):\")\n",
        "    print(f\"  Average length: {sum(text_lengths)/len(text_lengths):.2f}\")\n",
        "    print(f\"  Maximum length: {max(text_lengths)}\")\n",
        "    print(f\"  Minimum length: {min(text_lengths)}\")\n",
        "\n",
        "    return text_lengths\n",
        "\n",
        "# --- 1. Load and Explore Datasets ---\n",
        "train_claims = load_json(TRAIN_CLAIMS_PATH)\n",
        "dev_claims = load_json(DEV_CLAIMS_PATH)\n",
        "test_claims = load_json(TEST_CLAIMS_PATH)\n",
        "evidence = load_json(EVIDENCE_PATH)\n",
        "\n",
        "# Print a sample claim to understand structure\n",
        "print(\"\\n=== Sample Data Structure ===\")\n",
        "sample_claim_id = next(iter(train_claims))\n",
        "print(f\"Sample claim ID: {sample_claim_id}\")\n",
        "print(json.dumps(train_claims[sample_claim_id], indent=2))\n",
        "\n",
        "# Print a sample evidence\n",
        "sample_evidence_id = next(iter(evidence))\n",
        "print(f\"\\nSample evidence ID: {sample_evidence_id}\")\n",
        "print(f\"Evidence text: {evidence[sample_evidence_id]}\")\n",
        "\n",
        "# # --- 2. Basic Dataset Analysis ---\n",
        "# train_lengths = basic_dataset_analysis(train_claims, \"Training Claims\")\n",
        "# dev_lengths = basic_dataset_analysis(dev_claims, \"Development Claims\")\n",
        "# test_lengths = basic_dataset_analysis(test_claims, \"Test Claims\")\n",
        "\n",
        "# # Sample evidence for analysis (avoid analyzing all 1.2M items)\n",
        "# sample_evidence = {k: evidence[k] for k in list(evidence.keys())[:1000]}\n",
        "# evidence_lengths = basic_dataset_analysis(sample_evidence, \"Evidence (Sample)\")\n",
        "\n",
        "# # --- 3. Create a simple visualization ---\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.hist(train_lengths, bins=30, alpha=0.5, label='Claims')\n",
        "# plt.hist(evidence_lengths, bins=30, alpha=0.5, label='Evidence')\n",
        "# plt.xlabel('Word Count')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.title('Distribution of Text Lengths: Claims vs Evidence')\n",
        "# plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.savefig('text_length_distribution.png')\n",
        "# plt.show()\n",
        "\n",
        "# # --- 4. Preprocessing Examples ---\n",
        "# print(\"\\n=== Preprocessing Examples ===\")\n",
        "\n",
        "# Sample a claim\n",
        "sample_claim = train_claims[sample_claim_id]['claim_text']\n",
        "print(f\"Original claim: {sample_claim}\")\n",
        "print(f\"Basic preprocessing: {preprocess_text(sample_claim)}\")\n",
        "print(f\"With stopword removal: {preprocess_text(sample_claim, remove_stop_words=True)}\")\n",
        "print(f\"With lemmatization: {preprocess_text(sample_claim, lemmatize=True)}\")\n",
        "print(f\"With stemming: {preprocess_text(sample_claim, stem=True)}\")\n",
        "\n",
        "# Sample an evidence\n",
        "sample_evidence_text = evidence[sample_evidence_id]\n",
        "print(f\"\\nOriginal evidence: {sample_evidence_text}\")\n",
        "print(f\"Basic preprocessing: {preprocess_text(sample_evidence_text)}\")\n",
        "print(f\"With stopword removal: {preprocess_text(sample_evidence_text, remove_stop_words=True)}\")\n",
        "\n",
        "print(\"\\nData processing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "# --- Global Configuration Variables ---\n",
        "# TF-IDF settings\n",
        "TFIDF_MAX_FEATURES = 20000\n",
        "TFIDF_TOP_K = 500\n",
        "\n",
        "# Model settings\n",
        "TRANSFORMER_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "TRANSFORMER_BATCH_SIZE = 32\n",
        "TRANSFORMER_MAX_LENGTH = 512\n",
        "\n",
        "# Retrieval settings\n",
        "EVIDENCE_FINAL_TOP_K = 4  # The value to use for evidence retrieval\n",
        "EXPERIMENT_WITH_MULTIPLE_K = False  # Set to True to experiment with different k values\n",
        "EVIDENCE_TOP_K_VALUES = [3, 4, 5 ,6] if EXPERIMENT_WITH_MULTIPLE_K else [EVIDENCE_FINAL_TOP_K]\n",
        "\n",
        "# Memory management\n",
        "CUDA_CACHE_CLEAR_FREQUENCY = 20  # Clear CUDA cache every N claims\n",
        "PROGRESS_REPORT_FREQUENCY = 50  # Report progress every N claims\n",
        "\n",
        "# Text Processing settings\n",
        "REMOVE_STOP_WORDS = False\n",
        "LEMMATIZE = True\n",
        "\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqVJ2sVFY6S6"
      },
      "source": [
        "#### Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AFa_9iYWY6S6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Hugging Face Embedder Class ---\n",
        "class HuggingFaceEmbedder:\n",
        "    def __init__(self, model_name=TRANSFORMER_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Initialize the Hugging Face embedder.\n",
        "        \"\"\"\n",
        "        print(f\"Loading model {model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        \"\"\"\n",
        "        Perform mean pooling on model outputs using attention mask.\n",
        "        \"\"\"\n",
        "        # Mean pooling - take average of all token embeddings\n",
        "        token_embeddings = model_output[0]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    def encode(self, texts, batch_size=TRANSFORMER_BATCH_SIZE, show_progress=False):\n",
        "        \"\"\"\n",
        "        Encode texts to embeddings using the model.\n",
        "        \"\"\"\n",
        "        all_embeddings = []\n",
        "\n",
        "        # Process texts in batches\n",
        "        iterator = range(0, len(texts), batch_size)\n",
        "        if show_progress:\n",
        "            iterator = tqdm(iterator, desc=\"Encoding texts\")\n",
        "\n",
        "        for i in iterator:\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "            # Tokenize\n",
        "            encoded_input = self.tokenizer(batch_texts, padding=True, truncation=True,\n",
        "                                          max_length=TRANSFORMER_MAX_LENGTH, return_tensors='pt')\n",
        "\n",
        "            # Move to device\n",
        "            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
        "\n",
        "            # Get model output\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoded_input)\n",
        "\n",
        "            # Perform pooling and get embeddings\n",
        "            batch_embeddings = self.mean_pooling(outputs, encoded_input['attention_mask'])\n",
        "\n",
        "            # Normalize embeddings\n",
        "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
        "\n",
        "            # Move to CPU to free up GPU memory\n",
        "            all_embeddings.append(batch_embeddings.cpu())\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        if len(all_embeddings) > 0:\n",
        "            all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "        return all_embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1Xzxl0Y6S7"
      },
      "source": [
        "#### Two-Stage Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Z67XoOT8Y6S7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Two-Stage Retriever Class ---\n",
        "class TwoStageRetriever:\n",
        "    def __init__(self, evidence_corpus, tfidf_preprocessing=None, model_name=TRANSFORMER_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Initialize the two-stage retriever with TF-IDF and Hugging Face Transformers.\n",
        "\n",
        "        Args:\n",
        "            evidence_corpus (dict): Dictionary of evidence texts\n",
        "            tfidf_preprocessing (function, optional): Function to preprocess text for TF-IDF\n",
        "            model_name (str): Name of the Hugging Face model to use\n",
        "        \"\"\"\n",
        "        self.evidence_corpus = evidence_corpus\n",
        "        self.evidence_ids = list(evidence_corpus.keys())\n",
        "        self.tfidf_preprocessing = tfidf_preprocessing if tfidf_preprocessing else preprocess_text\n",
        "\n",
        "        # Prepare evidence texts for TF-IDF\n",
        "        print(\"Preparing evidence texts for TF-IDF...\")\n",
        "        start_time = time.time()\n",
        "        self.evidence_texts = [self.tfidf_preprocessing(evidence_corpus[eid]) for eid in tqdm(self.evidence_ids)]\n",
        "        print(f\"Evidence preparation took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "        # Initialize TF-IDF vectorizer\n",
        "        print(\"Fitting TF-IDF vectorizer...\")\n",
        "        start_time = time.time()\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES)\n",
        "        self.tfidf_evidence_matrix = self.tfidf_vectorizer.fit_transform(self.evidence_texts)\n",
        "        print(f\"TF-IDF fitting took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "        # Initialize Hugging Face embedder\n",
        "        self.embedder = HuggingFaceEmbedder(model_name)\n",
        "\n",
        "        print(\"Two-stage retriever initialized successfully.\")\n",
        "\n",
        "    def retrieve(self, claim, tfidf_top_k=TFIDF_TOP_K, final_top_k=EVIDENCE_FINAL_TOP_K):\n",
        "        \"\"\"\n",
        "        Retrieve evidence for a claim using the two-stage approach.\n",
        "\n",
        "        Args:\n",
        "            claim (str): The claim text\n",
        "            tfidf_top_k (int): Number of candidates to retrieve with TF-IDF\n",
        "            final_top_k (int): Number of final candidates to return after re-ranking\n",
        "\n",
        "        Returns:\n",
        "            list: Top k evidence IDs\n",
        "            list: Corresponding similarity scores\n",
        "        \"\"\"\n",
        "        # Stage 1: TF-IDF retrieval\n",
        "        processed_claim = self.tfidf_preprocessing(claim)\n",
        "        claim_vector = self.tfidf_vectorizer.transform([processed_claim])\n",
        "\n",
        "        # Calculate cosine similarity with all evidence\n",
        "        tfidf_similarities = cosine_similarity(claim_vector, self.tfidf_evidence_matrix)[0]\n",
        "\n",
        "        # Get top-k candidate indices\n",
        "        tfidf_top_indices = np.argsort(-tfidf_similarities)[:tfidf_top_k]\n",
        "        tfidf_top_evidence_ids = [self.evidence_ids[idx] for idx in tfidf_top_indices]\n",
        "        tfidf_top_evidence_texts = [self.evidence_corpus[eid] for eid in tfidf_top_evidence_ids]\n",
        "\n",
        "        # Stage 2: Transformer embedding re-ranking\n",
        "        # Encode claim and candidates\n",
        "        with torch.no_grad():\n",
        "            claim_embedding = self.embedder.encode([claim])\n",
        "            candidate_embeddings = self.embedder.encode(tfidf_top_evidence_texts)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = torch.matmul(candidate_embeddings, claim_embedding.t()).squeeze()\n",
        "\n",
        "        # Get top-k final indices\n",
        "        top_k_indices = torch.argsort(similarities, descending=True)[:final_top_k].tolist()\n",
        "\n",
        "        # Get final evidence IDs and scores\n",
        "        final_evidence_ids = [tfidf_top_evidence_ids[idx] for idx in top_k_indices]\n",
        "        final_scores = [similarities[idx].item() for idx in top_k_indices]\n",
        "\n",
        "        return final_evidence_ids, final_scores\n",
        "\n",
        "    def batch_retrieve(self, claims_dict, tfidf_top_k=TFIDF_TOP_K, final_top_k=EVIDENCE_FINAL_TOP_K):\n",
        "        \"\"\"\n",
        "        Retrieve evidence for multiple claims in batch mode.\n",
        "\n",
        "        Args:\n",
        "            claims_dict (dict): Dictionary of claims\n",
        "            tfidf_top_k (int): Number of candidates to retrieve with TF-IDF\n",
        "            final_top_k (int): Number of final candidates to return after re-ranking\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping claim IDs to lists of evidence IDs\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        total_claims = len(claims_dict)\n",
        "\n",
        "        print(f\"Processing {total_claims} claims...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx, (claim_id, claim_data) in enumerate(tqdm(claims_dict.items())):\n",
        "            claim_text = claim_data['claim_text']\n",
        "            evidence_ids, _ = self.retrieve(claim_text, tfidf_top_k, final_top_k)\n",
        "            results[claim_id] = evidence_ids\n",
        "\n",
        "            # # Periodically clear CUDA cache to prevent memory leaks\n",
        "            # if torch.cuda.is_available() and (idx % CUDA_CACHE_CLEAR_FREQUENCY == 0):\n",
        "            #     torch.cuda.empty_cache()\n",
        "            #     gc.collect()\n",
        "\n",
        "            # Periodically report progress\n",
        "            if (idx + 1) % PROGRESS_REPORT_FREQUENCY == 0 or (idx + 1) == total_claims:\n",
        "                elapsed = time.time() - start_time\n",
        "                claims_per_second = (idx + 1) / elapsed\n",
        "                print(f\"Processed {idx + 1}/{total_claims} claims \"\n",
        "                      f\"({(idx + 1)/total_claims*100:.1f}%) \"\n",
        "                      f\"at {claims_per_second:.2f} claims/second\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Total retrieval time: {total_time:.2f} seconds \"\n",
        "              f\"({total_time/60:.2f} minutes)\")\n",
        "\n",
        "        return results\n",
        "\n",
        "def evaluate_retrieval_fscore(retrieved_evidences, groundtruth_claims):\n",
        "    \"\"\"\n",
        "    Calculate F-score for evidence retrieval.\n",
        "    \"\"\"\n",
        "    f_scores = []\n",
        "\n",
        "    for claim_id, claim_data in groundtruth_claims.items():\n",
        "        if claim_id in retrieved_evidences:\n",
        "            # Get retrieved and ground truth evidence sets\n",
        "            retrieved_set = set(retrieved_evidences[claim_id])\n",
        "            groundtruth_set = set(claim_data[\"evidences\"])\n",
        "\n",
        "            # Calculate number of correct retrievals\n",
        "            correct = len(retrieved_set.intersection(groundtruth_set))\n",
        "\n",
        "            # Calculate precision, recall, and F-score\n",
        "            precision = correct / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "            recall = correct / len(groundtruth_set) if len(groundtruth_set) > 0 else 0\n",
        "\n",
        "            # Calculate F-score\n",
        "            fscore = 0\n",
        "            if precision > 0 and recall > 0:\n",
        "                fscore = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "            f_scores.append(fscore)\n",
        "\n",
        "    # Return average F-score\n",
        "    return np.mean(f_scores) if f_scores else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "E7hu2EowY6S7"
      },
      "outputs": [],
      "source": [
        "# --- Main Retrieval Function ---\n",
        "def run_evidence_retrieval(train_claims, dev_claims, test_claims, evidence):\n",
        "    \"\"\"\n",
        "    Run the evidence retrieval pipeline and evaluate results.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Retrieval Hyperparameters ===\")\n",
        "    print(f\"TFIDF_MAX_FEATURES: {TFIDF_MAX_FEATURES}\")\n",
        "    print(f\"TFIDF_TOP_K: {TFIDF_TOP_K}\")\n",
        "    print(f\"TRANSFORMER_MODEL: {TRANSFORMER_MODEL_NAME}\")\n",
        "    print(f\"EVIDENCE_TOP_K_VALUES: {EVIDENCE_TOP_K_VALUES}\")\n",
        "\n",
        "    # Use your existing preprocessing function\n",
        "    def preprocess_for_retrieval(text):\n",
        "        return preprocess_text(text, remove_stop_words=REMOVE_STOP_WORDS, lemmatize=LEMMATIZE)\n",
        "\n",
        "    # Results container\n",
        "    all_results = {}\n",
        "\n",
        "    # Initialize the retriever (only once)\n",
        "    print(\"Initializing two-stage retriever...\")\n",
        "    start_time = time.time()\n",
        "    retriever = TwoStageRetriever(\n",
        "        evidence_corpus=evidence,\n",
        "        tfidf_preprocessing=preprocess_for_retrieval,\n",
        "        model_name=TRANSFORMER_MODEL_NAME\n",
        "    )\n",
        "    print(f\"Retriever initialization took {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Run only with selected top_k value(s)\n",
        "    for top_k in EVIDENCE_TOP_K_VALUES:\n",
        "        print(f\"\\n--- Evidence Retrieval with Top-K = {top_k} ---\")\n",
        "\n",
        "        # Retrieve evidence for dev set\n",
        "        print(f\"Retrieving evidence for development set...\")\n",
        "        dev_evidence_results = retriever.batch_retrieve(\n",
        "            dev_claims,\n",
        "            tfidf_top_k=TFIDF_TOP_K,\n",
        "            final_top_k=top_k\n",
        "        )\n",
        "\n",
        "        # Create dev predictions format\n",
        "        dev_predictions = {}\n",
        "        for claim_id, claim_data in dev_claims.items():\n",
        "            dev_predictions[claim_id] = {\n",
        "                \"claim_text\": claim_data[\"claim_text\"],\n",
        "                \"claim_label\": claim_data[\"claim_label\"],\n",
        "                \"evidences\": dev_evidence_results[claim_id]\n",
        "            }\n",
        "\n",
        "        # Save dev predictions\n",
        "        dev_predictions_path = DEV_PREDICTIONS_PATH\n",
        "        if EXPERIMENT_WITH_MULTIPLE_K:\n",
        "            # If experimenting, save with specific filename\n",
        "            dev_predictions_path = os.path.join(DRIVE_DATA_PATH, f'dev-claims-predictions-top{top_k}.json')\n",
        "\n",
        "        print(f\"Saving development predictions to {dev_predictions_path}\")\n",
        "        with open(dev_predictions_path, 'w') as f:\n",
        "            json.dump(dev_predictions, f, indent=2)\n",
        "\n",
        "        # # Evaluate dev results\n",
        "        print(f\"Evaluating development results...\")\n",
        "        fscore = evaluate_retrieval_fscore(dev_evidence_results, dev_claims)\n",
        "        print(f\"Evidence Retrieval F-score = {fscore:.4f}\")\n",
        "\n",
        "        # Store results\n",
        "        all_results[f\"dev_top{top_k}\"] = dev_evidence_results\n",
        "\n",
        "        # Also retrieve for test set if this is the final top_k\n",
        "        if top_k == EVIDENCE_FINAL_TOP_K or not EXPERIMENT_WITH_MULTIPLE_K:\n",
        "            print(f\"\\nRetrieving evidence for test set...\")\n",
        "            test_evidence_results = retriever.batch_retrieve(\n",
        "                test_claims,\n",
        "                tfidf_top_k=TFIDF_TOP_K,\n",
        "                final_top_k=top_k\n",
        "            )\n",
        "\n",
        "            # Create test predictions\n",
        "            test_predictions = {}\n",
        "            for claim_id, claim_data in test_claims.items():\n",
        "                test_predictions[claim_id] = {\n",
        "                    \"claim_text\": claim_data[\"claim_text\"],\n",
        "                    \"claim_label\": None,  # Will be filled by classification\n",
        "                    \"evidences\": test_evidence_results[claim_id]\n",
        "                }\n",
        "\n",
        "            # Save test predictions\n",
        "            print(f\"Saving test predictions to {TEST_PREDICTIONS_PATH}\")\n",
        "            with open(TEST_PREDICTIONS_PATH, 'w') as f:\n",
        "                json.dump(test_predictions, f, indent=2)\n",
        "\n",
        "            all_results[\"test\"] = test_evidence_results\n",
        "\n",
        "    all_results[\"retriever\"] = retriever\n",
        "    print(\"\\nEvidence retrieval completed!\")\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HksuCfjLY6S8",
        "outputId": "c8b2b821-8d0b-4e06-f2d0-03af26766346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Evidence Retrieval Pipeline ===\n",
            "Running evidence retrieval pipeline...\n",
            "\n",
            "=== Retrieval Hyperparameters ===\n",
            "TFIDF_MAX_FEATURES: 20000\n",
            "TFIDF_TOP_K: 500\n",
            "TRANSFORMER_MODEL: sentence-transformers/all-MiniLM-L6-v2\n",
            "EVIDENCE_TOP_K_VALUES: [4]\n",
            "Initializing two-stage retriever...\n",
            "Preparing evidence texts for TF-IDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1208827/1208827 [04:00<00:00, 5021.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evidence preparation took 240.74 seconds\n",
            "Fitting TF-IDF vectorizer...\n",
            "TF-IDF fitting took 21.98 seconds\n",
            "Loading model sentence-transformers/all-MiniLM-L6-v2...\n",
            "Two-stage retriever initialized successfully.\n",
            "Retriever initialization took 263.06 seconds\n",
            "\n",
            "--- Evidence Retrieval with Top-K = 4 ---\n",
            "Retrieving evidence for development set...\n",
            "Processing 154 claims...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 50/154 [00:46<01:38,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 50/154 claims (32.5%) at 1.06 claims/second\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 100/154 [01:32<00:48,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100/154 claims (64.9%) at 1.08 claims/second\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 150/154 [02:17<00:03,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 150/154 claims (97.4%) at 1.09 claims/second\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 154/154 [02:21<00:00,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 154/154 claims (100.0%) at 1.09 claims/second\n",
            "Total retrieval time: 141.31 seconds (2.36 minutes)\n",
            "Saving development predictions to /content/drive/MyDrive/data/WithAttentionONON.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating development results...\n",
            "Evidence Retrieval F-score = 0.1734\n",
            "\n",
            "Retrieving evidence for test set...\n",
            "Processing 153 claims...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 50/153 [00:45<01:32,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 50/153 claims (32.7%) at 1.11 claims/second\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 100/153 [01:30<00:48,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100/153 claims (65.4%) at 1.11 claims/second\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 150/153 [02:15<00:02,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 150/153 claims (98.0%) at 1.11 claims/second\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 153/153 [02:18<00:00,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 153/153 claims (100.0%) at 1.11 claims/second\n",
            "Total retrieval time: 138.24 seconds (2.30 minutes)\n",
            "Saving test predictions to /content/drive/MyDrive/data/test-claims-predictions.json\n",
            "\n",
            "Evidence retrieval completed!\n",
            "Saving retrieval results to /content/drive/MyDrive/data/retrieval_results.pt\n",
            "\n",
            "Evidence retrieval step completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Run Evidence Retrieval Pipeline ---\n",
        "print(\"\\n=== Running Evidence Retrieval Pipeline ===\")\n",
        "\n",
        "# Check if retrieval results already exist to avoid recomputation\n",
        "retrieval_results_path = os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')\n",
        "\n",
        "if os.path.exists(retrieval_results_path) and False:  # Set to True to use cached results\n",
        "    print(f\"Loading cached retrieval results from {retrieval_results_path}\")\n",
        "    retrieval_results = torch.load(retrieval_results_path,weights_only=True)\n",
        "    print(\"Loaded retrieval results successfully.\")\n",
        "else:\n",
        "    print(\"Running evidence retrieval pipeline...\")\n",
        "    retrieval_results = run_evidence_retrieval(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims,\n",
        "        test_claims=test_claims,\n",
        "        evidence=evidence\n",
        "    )\n",
        "\n",
        "    # Save retrieval results (excluding the retriever object which might be large)\n",
        "    results_to_save = {k: v for k, v in retrieval_results.items() if k != 'retriever'}\n",
        "    print(f\"Saving retrieval results to {retrieval_results_path}\")\n",
        "    torch.save(results_to_save, retrieval_results_path)\n",
        "\n",
        "print(\"\\nEvidence retrieval step completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiDbhUkVY6S8"
      },
      "source": [
        "#### Classification with \"distilbert-base-uncased\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhUlkzOMY6S8"
      },
      "source": [
        "Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "BJHl1K1pY6S8"
      },
      "outputs": [],
      "source": [
        "# --- Task 2: Claim Verification Configuration ---\n",
        "# Model settings\n",
        "VERIFICATION_MODEL_NAME = \"distilbert-base-uncased\"\n",
        "VERIFICATION_BATCH_SIZE = 16\n",
        "VERIFICATION_MAX_LENGTH = 512\n",
        "VERIFICATION_LEARNING_RATE = 1e-5\n",
        "VERIFICATION_NUM_EPOCHS = 6\n",
        "VERIFICATION_NUM_LABELS = 4\n",
        "\n",
        "# Class management\n",
        "USE_CLASS_WEIGHTS = True\n",
        "USE_MIXED_EVIDENCE = True  # Whether to use both gold and retrieved evidence\n",
        "MIX_RATIO=0.5  # Ratio of gold evidence to retrieved evidence\n",
        "\n",
        "# Training phases\n",
        "PHASE1_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_phase1.pt')\n",
        "PHASE2_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_phase2.pt')\n",
        "FINAL_MODEL_PATH = os.path.join(DRIVE_DATA_PATH, 'verification_model_final.pt')\n",
        "\n",
        "# Label mapping\n",
        "LABEL_MAP = {\n",
        "    \"SUPPORTS\": 0,\n",
        "    \"REFUTES\": 1,\n",
        "    \"NOT_ENOUGH_INFO\": 2,\n",
        "    \"DISPUTED\": 3\n",
        "}\n",
        "LABEL_MAP_REVERSE = {v: k for k, v in LABEL_MAP.items()}\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "G5gwD0hAY6S8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def prepare_input_sequence(\n",
        "    claim, evidence, tokenizer, max_length=VERIFICATION_MAX_LENGTH\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepare input sequence for the model by tokenizing and formatting claim + evidence.\n",
        "    Implements smart truncation to handle long evidence passages.\n",
        "    \"\"\"\n",
        "    # Tokenize the claim and evidence pair\n",
        "    if \"distilbert\" in VERIFICATION_MODEL_NAME.lower():\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text=claim,\n",
        "            text_pair=evidence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=None,  # Return Python lists\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,  # No token_type_ids for DistilBERT\n",
        "        )\n",
        "    else:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text=claim,\n",
        "            text_pair=evidence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=None,  # Return Python lists\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "    return encoded\n",
        "\n",
        "\n",
        "class ClaimVerificationDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        claims,\n",
        "        evidence,\n",
        "        tokenizer,\n",
        "        max_length=VERIFICATION_MAX_LENGTH,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Dataset for claim verification.\n",
        "\n",
        "        Args:\n",
        "            claims: Dictionary of claims\n",
        "            evidence: Dictionary of evidence passages\n",
        "            tokenizer: Tokenizer for preprocessing\n",
        "            max_length: Maximum sequence length\n",
        "            use_ground_truth: Whether to use ground truth evidence\n",
        "            retrieved_evidence: Dictionary of retrieved evidence IDs (from Task 1)\n",
        "            mix_ratio: Ratio of retrieved evidence to mix in (0.0-1.0)\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "\n",
        "        print(\n",
        "            f\"Creating dataset (use_ground_truth={use_ground_truth}, mix_ratio={mix_ratio})...\"\n",
        "        )\n",
        "\n",
        "        # Set random seed for reproducibility when mixing evidence\n",
        "        random.seed(42)\n",
        "\n",
        "        # Process each claim\n",
        "        for claim_id, claim_data in tqdm(claims.items(), desc=\"Processing claims\"):\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "\n",
        "            # Skip claims without labels (e.g., test set)\n",
        "            if \"claim_label\" not in claim_data:\n",
        "                continue\n",
        "\n",
        "            label = LABEL_MAP.get(claim_data[\"claim_label\"])\n",
        "\n",
        "            # Use ground truth evidence if available and requested\n",
        "            if use_ground_truth and \"evidences\" in claim_data:\n",
        "                for ev_id in claim_data[\"evidences\"]:\n",
        "                    if ev_id in evidence:\n",
        "                        self.samples.append(\n",
        "                            {\n",
        "                                \"claim_id\": claim_id,\n",
        "                                \"claim_text\": claim_text,\n",
        "                                \"evidence_id\": ev_id,\n",
        "                                \"evidence_text\": evidence[ev_id],\n",
        "                                \"label\": label,\n",
        "                                \"is_ground_truth\": True,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "            # Mix in retrieved evidence if requested\n",
        "            if retrieved_evidence and claim_id in retrieved_evidence and mix_ratio > 0:\n",
        "                for ev_id in retrieved_evidence[claim_id]:\n",
        "                    # Skip if already included as ground truth\n",
        "                    if (\n",
        "                        use_ground_truth\n",
        "                        and \"evidences\" in claim_data\n",
        "                        and ev_id in claim_data[\"evidences\"]\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    # Only include with probability equal to mix_ratio\n",
        "                    if random.random() < mix_ratio and ev_id in evidence:\n",
        "                        self.samples.append(\n",
        "                            {\n",
        "                                \"claim_id\": claim_id,\n",
        "                                \"claim_text\": claim_text,\n",
        "                                \"evidence_id\": ev_id,\n",
        "                                \"evidence_text\": evidence[ev_id],\n",
        "                                \"label\": label,\n",
        "                                \"is_ground_truth\": False,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "        print(f\"Created dataset with {len(self.samples)} samples\")\n",
        "\n",
        "        # Print ground truth vs retrieved evidence statistics\n",
        "        ground_truth_count = sum(\n",
        "            1 for sample in self.samples if sample[\"is_ground_truth\"]\n",
        "        )\n",
        "        retrieved_count = len(self.samples) - ground_truth_count\n",
        "        print(f\"Ground truth evidence: {ground_truth_count}\")\n",
        "        print(f\"Retrieved evidence: {retrieved_count}\")\n",
        "        print(f\"Ratio of retrieved to total: {retrieved_count/len(self.samples):.2f}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Tokenize and prepare input sequence\n",
        "        encoded = prepare_input_sequence(\n",
        "            sample[\"claim_text\"],\n",
        "            sample[\"evidence_text\"],\n",
        "            self.tokenizer,\n",
        "            self.max_length,\n",
        "        )\n",
        "\n",
        "        # Convert to tensors\n",
        "        item = {\n",
        "            \"input_ids\": torch.tensor(encoded[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(encoded[\"attention_mask\"]),\n",
        "            \"label\": torch.tensor(sample[\"label\"]),\n",
        "        }\n",
        "\n",
        "        # Add token_type_ids only if not using DistilBERT\n",
        "        if \"token_type_ids\" in encoded:\n",
        "            item[\"token_type_ids\"] = torch.tensor(encoded[\"token_type_ids\"])\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "9-jZ1GMBY6S8"
      },
      "outputs": [],
      "source": [
        "class ClaimVerificationModel(nn.Module):\n",
        "    def __init__(self, num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME):\n",
        "        \"\"\"\n",
        "        Transformer-based model for claim verification.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
        "        self.evidence_attention = nn.Linear(self.transformer.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs\n",
        "            attention_mask: Attention mask\n",
        "            token_type_ids: Optional token type IDs\n",
        "            labels: Optional labels for loss calculation\n",
        "\n",
        "        Returns:\n",
        "            If labels provided: (loss, logits)\n",
        "            If labels not provided: logits\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if 'distilbert' in VERIFICATION_MODEL_NAME.lower():\n",
        "            outputs = self.transformer(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "        else:\n",
        "            outputs = self.transformer(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "        # Get CLS token representation\n",
        "        cls_output = outputs[0][:, 0, :]  # CLS token output\n",
        "        # Calculate attention scores - apply consistently regardless of training mode\n",
        "        # attention_scores = self.evidence_attention(cls_output)\n",
        "        # # Apply the attention as a weighting factor\n",
        "        # cls_output = cls_output * torch.sigmoid(attention_scores)\n",
        "\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        # Calculate loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits\n",
        "\n",
        "    def get_attention_score(self, cls_output):\n",
        "        \"\"\"Calculate attention score for evidence weighting\"\"\"\n",
        "        return torch.sigmoid(self.evidence_attention(cls_output))\n",
        "\n",
        "    def predict_with_evidence_set(self, claim_text, evidence_texts, tokenizer):\n",
        "        \"\"\"\n",
        "        Process multiple evidence passages for one claim and make a prediction.\n",
        "\n",
        "        Args:\n",
        "            claim_text: The claim text\n",
        "            evidence_texts: List of evidence texts\n",
        "            tokenizer: Tokenizer for encoding\n",
        "\n",
        "        Returns:\n",
        "            Probability distribution over the 4 classes\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        all_logits = []\n",
        "        all_attention = []\n",
        "\n",
        "        # Default if no evidence\n",
        "        if not evidence_texts:\n",
        "            print(f\"No evidence passages provided.\")\n",
        "            return torch.tensor([0, 0, 1, 0], device=device)  # Default to NOT_ENOUGH_INFO\n",
        "\n",
        "        # Process each evidence passage\n",
        "        for evidence_text in evidence_texts:\n",
        "            # Prepare input\n",
        "            encoded = prepare_input_sequence(claim_text, evidence_text, tokenizer, VERIFICATION_MAX_LENGTH)\n",
        "            input_ids = torch.tensor([encoded[\"input_ids\"]]).to(device)\n",
        "            attention_mask = torch.tensor([encoded[\"attention_mask\"]]).to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            with torch.no_grad():\n",
        "                if 'distilbert' in VERIFICATION_MODEL_NAME.lower():\n",
        "                    outputs = self.transformer(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask\n",
        "                    )\n",
        "                else:\n",
        "                    token_type_ids = torch.tensor([encoded[\"token_type_ids\"]]).to(device) if \"token_type_ids\" in encoded else None\n",
        "                    outputs = self.transformer(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        token_type_ids=token_type_ids\n",
        "                    )\n",
        "\n",
        "                cls_output = outputs[0][:, 0, :]\n",
        "\n",
        "                # Get attention score\n",
        "                attention_score = self.get_attention_score(cls_output)\n",
        "\n",
        "                # Get logits\n",
        "                logits = self.classifier(self.dropout(cls_output))\n",
        "\n",
        "                all_logits.append(logits)\n",
        "                all_attention.append(attention_score)\n",
        "\n",
        "        # Stack results\n",
        "        stacked_logits = torch.cat(all_logits, dim=0)\n",
        "        stacked_attention = torch.cat(all_attention, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "        # Normalize attention\n",
        "        norm_attention = F.softmax(stacked_attention.squeeze(-1), dim=0)\n",
        "\n",
        "        # Weighted average of logits\n",
        "        weighted_logits = torch.sum(stacked_logits * norm_attention.unsqueeze(-1), dim=0)\n",
        "\n",
        "        # Return probabilities\n",
        "        return F.softmax(weighted_logits, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "-UZYQQPtY6S9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def get_loss_function(labels=None, use_class_weights=False):\n",
        "    \"\"\"Get loss function with optional class weighting.\"\"\"\n",
        "    if use_class_weights and labels is not None:\n",
        "        # Calculate class weights\n",
        "        class_counts = torch.bincount(labels)\n",
        "        class_weights = len(labels) / (len(class_counts) * class_counts)\n",
        "        class_weights = class_weights.to(device)\n",
        "\n",
        "        print(f\"Using class weights: {class_weights}\")\n",
        "        return nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler=None):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "            loss, logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        else:\n",
        "            loss, logits = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "                loss, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            else:\n",
        "                loss, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Option 2: Calculate F1 score (macro-averaged)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    # Option 3: Calculate balanced accuracy\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, target_names=list(LABEL_MAP.keys()), output_dict=True)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return {\n",
        "        \"loss\": total_loss / len(dataloader),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_score\": f1,  # Added F1 score\n",
        "        \"balanced_accuracy\": balanced_acc,  # Added balanced accuracy\n",
        "        \"report\": report,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, num_epochs=VERIFICATION_NUM_EPOCHS,\n",
        "                batch_size=VERIFICATION_BATCH_SIZE, learning_rate=VERIFICATION_LEARNING_RATE,\n",
        "                use_class_weights=USE_CLASS_WEIGHTS, save_path=FINAL_MODEL_PATH):\n",
        "    \"\"\"\n",
        "    Train the claim verification model.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_dataset: Training dataset\n",
        "        val_dataset: Validation dataset\n",
        "        num_epochs: Number of training epochs\n",
        "        batch_size: Batch size\n",
        "        learning_rate: Learning rate\n",
        "        use_class_weights: Whether to use class weighting\n",
        "        save_path: Path to save the best model\n",
        "\n",
        "    Returns:\n",
        "        The trained model and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Get loss function\n",
        "    if use_class_weights:\n",
        "        all_labels = torch.tensor([sample[\"label\"] for i, sample in enumerate(train_dataset)])\n",
        "        loss_fn = get_loss_function(all_labels, use_class_weights=True)\n",
        "    else:\n",
        "        loss_fn = get_loss_function()\n",
        "\n",
        "     # Initialize tracking variables\n",
        "    best_val_accuracy = 0.0\n",
        "    best_val_f1 = 0.0  # For Option 2\n",
        "    best_val_balanced_acc = 0.0  # For Option 3\n",
        "    best_epoch = 0\n",
        "    training_stats = []\n",
        "\n",
        "      # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer)\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        val_metrics = evaluate(model, val_loader)\n",
        "        print(f\"Validation loss: {val_metrics['loss']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"F1 Score (macro): {val_metrics['f1_score']:.4f}, Balanced Accuracy: {val_metrics['balanced_accuracy']:.4f}\")\n",
        "\n",
        "        # Print detailed metrics\n",
        "        print(\"\\nValidation Report:\")\n",
        "        for label, metrics in val_metrics['report'].items():\n",
        "            if label in LABEL_MAP_REVERSE:\n",
        "                print(f\"  {LABEL_MAP_REVERSE[int(label)]}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1-score']:.4f}\")\n",
        "\n",
        "        # Save stats\n",
        "        training_stats.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_metrics[\"loss\"],\n",
        "            \"val_accuracy\": val_metrics[\"accuracy\"],\n",
        "            \"val_f1_score\": val_metrics[\"f1_score\"],  # Added F1 score\n",
        "            \"val_balanced_accuracy\": val_metrics[\"balanced_accuracy\"],  # Added balanced accuracy\n",
        "            \"val_report\": val_metrics[\"report\"]\n",
        "        })\n",
        "\n",
        "        # Option 1 : Save best model based on accuracy\n",
        "        # if val_metrics[\"accuracy\"] > best_val_accuracy:\n",
        "        #    best_val_accuracy = val_metrics[\"accuracy\"]\n",
        "        #    best_epoch = epoch + 1\n",
        "        #    torch.save(model.state_dict(), save_path)\n",
        "        #    print(f\"New best model saved with accuracy {best_val_accuracy:.4f}\")\n",
        "\n",
        "        # Option 2: Save best model based on F1 score\n",
        "        if val_metrics[\"f1_score\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"f1_score\"]\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"New best model saved with F1 score {best_val_f1:.4f}\")\n",
        "\n",
        "        # Option 3: Save best model based on balanced accuracy\n",
        "        # if val_metrics[\"balanced_accuracy\"] > best_val_balanced_acc:\n",
        "        #    best_val_balanced_acc = val_metrics[\"balanced_accuracy\"]\n",
        "        #    best_epoch = epoch + 1\n",
        "        #    torch.save(model.state_dict(), save_path)\n",
        "        #    print(f\"New best model saved with balanced accuracy {best_val_balanced_acc:.4f}\")\n",
        "\n",
        "    # Print training summary\n",
        "    if best_val_f1 > 0:  # If using Option 2\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with F1 score {best_val_f1:.4f}\")\n",
        "    elif best_val_balanced_acc > 0:  # If using Option 3\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with balanced accuracy {best_val_balanced_acc:.4f}\")\n",
        "    else:  # If using original Option 1\n",
        "        print(f\"\\nTraining complete. Best model from epoch {best_epoch} with accuracy {best_val_accuracy:.4f}\")\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(save_path, weights_only=True))  # Added weights_only=True\n",
        "\n",
        "    return model, training_stats\n",
        "\n",
        "def visualize_confusion_matrix(confusion_matrix, classes):\n",
        "    \"\"\"\n",
        "    Visualize the confusion matrix.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "UVL1u9lmY6S-"
      },
      "outputs": [],
      "source": [
        "def train_verification_model(train_claims, dev_claims, evidence, retriever=None,\n",
        "                            use_mixed_evidence=USE_MIXED_EVIDENCE,\n",
        "                            use_class_weights=USE_CLASS_WEIGHTS):\n",
        "    \"\"\"\n",
        "    Train the verification model with configurable training phases.\n",
        "\n",
        "    Args:\n",
        "        train_claims: Training claims\n",
        "        dev_claims: Development claims\n",
        "        evidence: Evidence corpus\n",
        "        retriever: Optional retriever for getting training evidence\n",
        "        use_mixed_evidence: Whether to use Phase 2 (mixed evidence training)\n",
        "        use_class_weights: Whether to use class weighting\n",
        "\n",
        "    Returns:\n",
        "        Trained model and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    model = ClaimVerificationModel(num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME)\n",
        "    model.to(device)\n",
        "\n",
        "    # Phase 1: Train on ground truth evidence\n",
        "    print(\"\\n=== Phase 1: Training on ground truth evidence ===\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ClaimVerificationDataset(\n",
        "        claims=train_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    val_dataset = ClaimVerificationDataset(\n",
        "        claims=dev_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    # Train for Phase 1\n",
        "    model, phase1_stats = train_model(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        use_class_weights=use_class_weights,\n",
        "        save_path=PHASE1_MODEL_PATH\n",
        "    )\n",
        "\n",
        "    phase1_accuracy = phase1_stats[-1][\"val_accuracy\"]\n",
        "    print(f\"Phase 1 Model Accuracy: {phase1_accuracy:.4f}\")\n",
        "\n",
        "    # Phase 2: Mix ground truth and retrieved evidence (optional)\n",
        "    if use_mixed_evidence and retriever is not None:\n",
        "        print(\"\\n=== Phase 2: Training with mixed evidence ===\")\n",
        "\n",
        "        # Get retrieved evidence for training set\n",
        "        print(\"Retrieving evidence for training claims...\")\n",
        "        train_retrieved = {}\n",
        "        for claim_id, claim_data in tqdm(train_claims.items()):\n",
        "            # Skip if no ground truth label\n",
        "            if \"claim_label\" not in claim_data:\n",
        "                continue\n",
        "\n",
        "            # Get claim text\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "\n",
        "            # Retrieve evidence\n",
        "            evidence_ids, _ = retriever.retrieve(claim_text)\n",
        "            train_retrieved[claim_id] = evidence_ids\n",
        "\n",
        "        # Create mixed dataset\n",
        "        mixed_train_dataset = ClaimVerificationDataset(\n",
        "            claims=train_claims,\n",
        "            evidence=evidence,\n",
        "            tokenizer=tokenizer,\n",
        "            use_ground_truth=True,\n",
        "            retrieved_evidence=train_retrieved,\n",
        "            mix_ratio=0.5\n",
        "        )\n",
        "\n",
        "        # Continue training from Phase 1 model\n",
        "        model.load_state_dict(torch.load(PHASE1_MODEL_PATH,weights_only=True))\n",
        "\n",
        "        # Train for Phase 2\n",
        "        model, phase2_stats = train_model(\n",
        "            model=model,\n",
        "            train_dataset=mixed_train_dataset,\n",
        "            val_dataset=val_dataset,\n",
        "            num_epochs=5,  # Fewer epochs for Phase 2\n",
        "            use_class_weights=use_class_weights,\n",
        "            save_path=PHASE2_MODEL_PATH\n",
        "        )\n",
        "\n",
        "        phase2_accuracy = phase2_stats[-1][\"val_accuracy\"]\n",
        "        print(f\"Phase 2 Model Accuracy: {phase2_accuracy:.4f}\")\n",
        "        print(f\"Improvement: {phase2_accuracy - phase1_accuracy:.4f}\")\n",
        "\n",
        "        return model, {\"phase1\": phase1_stats, \"phase2\": phase2_stats}\n",
        "\n",
        "    return model, {\"phase1\": phase1_stats}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "9sEVeQI_Y6S_"
      },
      "outputs": [],
      "source": [
        "def predict_claims(model, claims, evidence, retrieved_evidence, tokenizer):\n",
        "    \"\"\"\n",
        "    Make predictions on claims using retrieved evidence.\n",
        "\n",
        "    Args:\n",
        "        model: Trained verification model\n",
        "        claims: Dictionary of claims\n",
        "        evidence: Evidence corpus\n",
        "        retrieved_evidence: Dictionary mapping claim IDs to lists of evidence IDs\n",
        "        tokenizer: Tokenizer for preprocessing\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping claim IDs to predicted labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = {}\n",
        "\n",
        "    print(f\"Making predictions for {len(claims)} claims...\")\n",
        "    for claim_id, claim_data in tqdm(claims.items()):\n",
        "        claim_text = claim_data[\"claim_text\"]\n",
        "\n",
        "        # Get evidence texts\n",
        "        evidence_ids = retrieved_evidence.get(claim_id, [])\n",
        "        evidence_texts = [evidence[eid] for eid in evidence_ids if eid in evidence]\n",
        "\n",
        "        # Skip if no evidence (default to NOT_ENOUGH_INFO)\n",
        "        if not evidence_texts:\n",
        "            print(f\"No evidence passages provided HOWWWWW.\")\n",
        "            predictions[claim_id] = \"NOT_ENOUGH_INFO\"\n",
        "            continue\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            probs = model.predict_with_evidence_set(claim_text, evidence_texts, tokenizer)\n",
        "\n",
        "        # Get class with highest probability\n",
        "        label_id = torch.argmax(probs).item()\n",
        "        predictions[claim_id] = LABEL_MAP_REVERSE[label_id]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def update_predictions_file(predictions_path, predictions):\n",
        "    \"\"\"Update predictions file with claim labels.\"\"\"\n",
        "    # Load current predictions\n",
        "    with open(predictions_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Update with new predictions\n",
        "    for claim_id, label in predictions.items():\n",
        "        if claim_id in data:\n",
        "            data[claim_id][\"claim_label\"] = label\n",
        "\n",
        "    # Save updated predictions\n",
        "    with open(predictions_path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    print(f\"Updated predictions saved to {predictions_path}\")\n",
        "\n",
        "def evaluate_predictions(predictions, groundtruth):\n",
        "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Collect all predictions and labels\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for claim_id, claim_data in groundtruth.items():\n",
        "        if claim_id in predictions:\n",
        "            true_label = claim_data[\"claim_label\"]\n",
        "            pred_label = predictions[claim_id]\n",
        "\n",
        "            y_true.append(LABEL_MAP[true_label])\n",
        "            y_pred.append(LABEL_MAP[pred_label])\n",
        "\n",
        "            if pred_label == true_label:\n",
        "                correct += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    # Get detailed report\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    report = classification_report(y_true, y_pred, target_names=list(LABEL_MAP.keys()), output_dict=True)\n",
        "\n",
        "    # Get confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    for label, metrics in report.items():\n",
        "        if label in LABEL_MAP_REVERSE:\n",
        "            print(f\"  {LABEL_MAP_REVERSE[int(label)]}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1-score']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"report\": report,\n",
        "        \"confusion_matrix\": cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JzWB7vP1Y6S_",
        "outputId": "89aa08aa-dc77-4233-9830-b0972efc2f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found retriever in retrieval_results.\n",
            "\n",
            "=== Running Claim Verification (Task 2) ===\n",
            "\n",
            "=== Claim Verification Hyperparameters ===\n",
            "MODEL_NAME: distilbert-base-uncased\n",
            "BATCH_SIZE: 16\n",
            "LEARNING_RATE: 1e-05\n",
            "USE_CLASS_WEIGHTS: True\n",
            "USE_MIXED_EVIDENCE: True\n",
            "Loading /content/drive/MyDrive/data/WithAttentionONON.json...\n",
            "Successfully loaded 154 items.\n",
            "Loading /content/drive/MyDrive/data/test-claims-predictions.json...\n",
            "Successfully loaded 153 items.\n",
            "Checking retriever functionality...\n",
            "Retriever test successful. Found 4 evidence passages.\n",
            "\n",
            "=== Phase 1: Training on ground truth evidence ===\n",
            "Creating dataset (use_ground_truth=True, mix_ratio=0.0)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing claims: 100%|██████████| 1228/1228 [00:00<00:00, 163464.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 4122 samples\n",
            "Ground truth evidence: 4122\n",
            "Retrieved evidence: 0\n",
            "Ratio of retrieved to total: 0.00\n",
            "Creating dataset (use_ground_truth=True, mix_ratio=0.0)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing claims: 100%|██████████| 154/154 [00:00<00:00, 101273.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 491 samples\n",
            "Ground truth evidence: 491\n",
            "Retrieved evidence: 0\n",
            "Ratio of retrieved to total: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using class weights: tensor([0.7673, 2.2549, 0.5339, 2.6288], device='cuda:0')\n",
            "\n",
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 258/258 [03:07<00:00,  1.38it/s, loss=0.717]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 1.1073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.14it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.1886, Accuracy: 0.4969\n",
            "F1 Score (macro): 0.2968, Balanced Accuracy: 0.3344\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.2968\n",
            "\n",
            "Epoch 2/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 258/258 [03:07<00:00,  1.38it/s, loss=0.674]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.4517, Accuracy: 0.4745\n",
            "F1 Score (macro): 0.3210, Balanced Accuracy: 0.3447\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3210\n",
            "\n",
            "Epoch 3/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 258/258 [03:07<00:00,  1.37it/s, loss=0.501]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.3145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.8894, Accuracy: 0.4542\n",
            "F1 Score (macro): 0.3832, Balanced Accuracy: 0.4032\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3832\n",
            "\n",
            "Epoch 4/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 258/258 [03:07<00:00,  1.37it/s, loss=0.0187]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.1395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.2543, Accuracy: 0.4420\n",
            "F1 Score (macro): 0.3673, Balanced Accuracy: 0.3657\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Epoch 5/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 258/258 [03:07<00:00,  1.38it/s, loss=0.0503]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.0743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.4705, Accuracy: 0.4481\n",
            "F1 Score (macro): 0.3655, Balanced Accuracy: 0.3665\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Epoch 6/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 258/258 [03:07<00:00,  1.37it/s, loss=0.00714]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.0403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.7640, Accuracy: 0.4420\n",
            "F1 Score (macro): 0.3801, Balanced Accuracy: 0.3968\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Training complete. Best model from epoch 3 with F1 score 0.3832\n",
            "Phase 1 Model Accuracy: 0.4420\n",
            "\n",
            "=== Phase 2: Training with mixed evidence ===\n",
            "Retrieving evidence for training claims...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1228/1228 [18:42<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset (use_ground_truth=True, mix_ratio=0.5)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing claims: 100%|██████████| 1228/1228 [00:00<00:00, 86850.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 6232 samples\n",
            "Ground truth evidence: 4122\n",
            "Retrieved evidence: 2110\n",
            "Ratio of retrieved to total: 0.34\n",
            "Using class weights: tensor([0.7095, 1.8771, 0.5974, 2.6054], device='cuda:0')\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 390/390 [04:43<00:00,  1.37it/s, loss=0.0974]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.1602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.4078, Accuracy: 0.4358\n",
            "F1 Score (macro): 0.3699, Balanced Accuracy: 0.3751\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3699\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 390/390 [04:43<00:00,  1.37it/s, loss=0.0111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.0568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 3.0152, Accuracy: 0.4257\n",
            "F1 Score (macro): 0.3556, Balanced Accuracy: 0.4052\n",
            "\n",
            "Validation Report:\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 390/390 [04:43<00:00,  1.38it/s, loss=0.00945]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.0328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.8363, Accuracy: 0.4542\n",
            "F1 Score (macro): 0.3845, Balanced Accuracy: 0.4040\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3845\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 390/390 [04:43<00:00,  1.38it/s, loss=0.00219]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.0240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 3.0180, Accuracy: 0.4562\n",
            "F1 Score (macro): 0.3974, Balanced Accuracy: 0.4084\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3974\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 390/390 [04:43<00:00,  1.37it/s, loss=0.000691]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.0138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 31/31 [00:07<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 3.2627, Accuracy: 0.4725\n",
            "F1 Score (macro): 0.3976, Balanced Accuracy: 0.4135\n",
            "\n",
            "Validation Report:\n",
            "New best model saved with F1 score 0.3976\n",
            "\n",
            "Training complete. Best model from epoch 5 with F1 score 0.3976\n",
            "Phase 2 Model Accuracy: 0.4725\n",
            "Improvement: 0.0305\n",
            "\n",
            "Making predictions on development set...\n",
            "Making predictions for 154 claims...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 154/154 [00:10<00:00, 15.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating development predictions:\n",
            "Accuracy: 0.5130\n",
            "\n",
            "Classification Report:\n",
            "Updated predictions saved to /content/drive/MyDrive/data/WithAttentionONON.json\n",
            "\n",
            "Making predictions on test set...\n",
            "Making predictions for 153 claims...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 153/153 [00:10<00:00, 15.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated predictions saved to /content/drive/MyDrive/data/test-claims-predictions.json\n",
            "\n",
            "Claim verification completed!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 2 completed!\n"
          ]
        }
      ],
      "source": [
        "def run_claim_verification(train_claims, dev_claims, test_claims, evidence, retriever=None):\n",
        "    \"\"\"\n",
        "    Run the claim verification pipeline and generate predictions.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Claim Verification Hyperparameters ===\")\n",
        "    print(f\"MODEL_NAME: {VERIFICATION_MODEL_NAME}\")\n",
        "    print(f\"BATCH_SIZE: {VERIFICATION_BATCH_SIZE}\")\n",
        "    print(f\"LEARNING_RATE: {VERIFICATION_LEARNING_RATE}\")\n",
        "    print(f\"USE_CLASS_WEIGHTS: {USE_CLASS_WEIGHTS}\")\n",
        "    print(f\"USE_MIXED_EVIDENCE: {USE_MIXED_EVIDENCE}\")\n",
        "\n",
        "    # Load retrieved evidence from Task 1\n",
        "    dev_predictions = load_json(DEV_PREDICTIONS_PATH)\n",
        "    test_predictions = load_json(TEST_PREDICTIONS_PATH)\n",
        "\n",
        "    # Extract retrieved evidence\n",
        "    dev_retrieved = {claim_id: data[\"evidences\"] for claim_id, data in dev_predictions.items()}\n",
        "    test_retrieved = {claim_id: data[\"evidences\"] for claim_id, data in test_predictions.items()}\n",
        "\n",
        "    # Check if retriever is valid\n",
        "    if retriever is not None:\n",
        "        print(\"Checking retriever functionality...\")\n",
        "        try:\n",
        "            # Test retrieve method on a sample claim\n",
        "            sample_claim_id = next(iter(train_claims))\n",
        "            sample_claim_text = train_claims[sample_claim_id][\"claim_text\"]\n",
        "            sample_evidence, _ = retriever.retrieve(sample_claim_text)\n",
        "            print(f\"Retriever test successful. Found {len(sample_evidence)} evidence passages.\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Retriever test failed: {e}\")\n",
        "\n",
        "    # Train model\n",
        "    model, stats = train_verification_model(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims,\n",
        "        evidence=evidence,\n",
        "        retriever=retriever,\n",
        "        use_mixed_evidence=USE_MIXED_EVIDENCE,\n",
        "        use_class_weights=USE_CLASS_WEIGHTS\n",
        "    )\n",
        "\n",
        "    # Initialize tokenizer for predictions\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "\n",
        "    # Make predictions on dev set\n",
        "    print(\"\\nMaking predictions on development set...\")\n",
        "    dev_pred_labels = predict_claims(model, dev_claims, evidence, dev_retrieved, tokenizer)\n",
        "\n",
        "    # Evaluate dev predictions\n",
        "    print(\"\\nEvaluating development predictions:\")\n",
        "    dev_metrics = evaluate_predictions(dev_pred_labels, dev_claims)\n",
        "\n",
        "    # Update dev predictions file\n",
        "    update_predictions_file(DEV_PREDICTIONS_PATH, dev_pred_labels)\n",
        "\n",
        "    # Make predictions on test set\n",
        "    print(\"\\nMaking predictions on test set...\")\n",
        "    test_pred_labels = predict_claims(model, test_claims, evidence, test_retrieved, tokenizer)\n",
        "\n",
        "    # Update test predictions file\n",
        "    update_predictions_file(TEST_PREDICTIONS_PATH, test_pred_labels)\n",
        "\n",
        "    print(\"\\nClaim verification completed!\")\n",
        "    return model, stats, dev_metrics\n",
        "\n",
        "# Main execution - running task 2\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if we already have retrieval results\n",
        "    if 'retrieval_results' not in locals() and os.path.exists(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')):\n",
        "        print(\"Loading saved retrieval results...\")\n",
        "        retrieval_results = torch.load(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt'),weights_only=True)\n",
        "    # Check if retriever is available\n",
        "    if 'retrieval_results' in locals() and 'retriever' in retrieval_results:\n",
        "        retriever = retrieval_results[\"retriever\"]\n",
        "        print(\"Found retriever in retrieval_results.\")\n",
        "    else:\n",
        "        print(\"Retriever not found in retrieval_results.\")\n",
        "    print(\"\\n=== Running Claim Verification (Task 2) ===\")\n",
        "    model, stats, metrics = run_claim_verification(\n",
        "        train_claims=train_claims,\n",
        "        dev_claims=dev_claims,\n",
        "        test_claims=test_claims,\n",
        "        evidence=evidence,\n",
        "        retriever=retriever\n",
        "    )\n",
        "\n",
        "    # Visualize confusion matrix\n",
        "    visualize_confusion_matrix(\n",
        "        metrics[\"confusion_matrix\"],\n",
        "        list(LABEL_MAP.keys())\n",
        "    )\n",
        "\n",
        "    print(\"\\nTask 2 completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "hblEzunwY6S_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGWzlldNY6S_"
      },
      "source": [
        "#### Diagnose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "A57J0UDSY6S_"
      },
      "outputs": [],
      "source": [
        "def diagnose_dataset(train_claims, dev_claims, evidence):\n",
        "    print(\"\\n=== Dataset Diagnosis ===\")\n",
        "\n",
        "    # Check label distribution in claims\n",
        "    train_labels = {}\n",
        "    for claim_id, claim_data in train_claims.items():\n",
        "        if \"claim_label\" in claim_data:\n",
        "            label = claim_data[\"claim_label\"]\n",
        "            if label not in train_labels:\n",
        "                train_labels[label] = 0\n",
        "            train_labels[label] += 1\n",
        "\n",
        "    print(\"\\nLabel Distribution in Training Claims:\")\n",
        "    total_train = sum(train_labels.values())\n",
        "    for label, count in train_labels.items():\n",
        "        print(f\"  {label}: {count} claims ({count/total_train*100:.2f}%)\")\n",
        "\n",
        "    # Check average number of evidence passages per claim\n",
        "    train_evidence_counts = []\n",
        "    for claim_id, claim_data in train_claims.items():\n",
        "        if \"evidences\" in claim_data:\n",
        "            train_evidence_counts.append(len(claim_data[\"evidences\"]))\n",
        "\n",
        "    print(\"\\nEvidence per Claim in Training Set:\")\n",
        "    if train_evidence_counts:\n",
        "        print(f\"  Average: {np.mean(train_evidence_counts):.2f}\")\n",
        "        print(f\"  Min: {min(train_evidence_counts)}\")\n",
        "        print(f\"  Max: {max(train_evidence_counts)}\")\n",
        "\n",
        "    # Create a test dataset with the actual preprocessing\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "    test_dataset = ClaimVerificationDataset(\n",
        "        claims=train_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    # Check processed sample distribution\n",
        "    sample_labels = {}\n",
        "    for sample in test_dataset.samples:\n",
        "        label = sample[\"label\"]\n",
        "        if label not in sample_labels:\n",
        "            sample_labels[label] = 0\n",
        "        sample_labels[label] += 1\n",
        "\n",
        "    print(\"\\nLabel Distribution in Processed Samples:\")\n",
        "    total_samples = len(test_dataset.samples)\n",
        "    for label, count in sample_labels.items():\n",
        "        label_name = LABEL_MAP_REVERSE.get(label, f\"Unknown ({label})\")\n",
        "        print(f\"  {label_name}: {count} samples ({count/total_samples*100:.2f}%)\")\n",
        "\n",
        "    # Check for data processing issues by examining a few samples\n",
        "    print(\"\\nSample Check (first 3 samples):\")\n",
        "    for i in range(min(3, len(test_dataset))):\n",
        "        sample = test_dataset[i]\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"  Label: {LABEL_MAP_REVERSE.get(sample['label'].item(), 'Unknown')}\")\n",
        "\n",
        "        # Decode the input IDs to check tokenization\n",
        "        input_text = tokenizer.decode(sample[\"input_ids\"])\n",
        "        print(f\"  Tokenized text (truncated): {input_text[:100]}...\")\n",
        "\n",
        "        # Check if special tokens are properly placed\n",
        "        cls_token_id = tokenizer.cls_token_id\n",
        "        sep_token_id = tokenizer.sep_token_id\n",
        "        has_cls = sample[\"input_ids\"][0] == cls_token_id\n",
        "        has_sep = sep_token_id in sample[\"input_ids\"]\n",
        "\n",
        "        print(f\"  Has CLS token at start: {has_cls}\")\n",
        "        print(f\"  Has SEP token: {has_sep}\")\n",
        "\n",
        "        # Check attention mask\n",
        "        valid_tokens = sum(sample[\"attention_mask\"].tolist())\n",
        "        print(f\"  Valid tokens: {valid_tokens}/{len(sample['input_ids'])}\")\n",
        "\n",
        "    return test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "aHTkK4vlY6S_"
      },
      "outputs": [],
      "source": [
        "def diagnose_model_predictions(model, test_dataset):\n",
        "    print(\"\\n=== Model Prediction Diagnosis ===\")\n",
        "\n",
        "    # Create a small subset of the dataset for quick testing\n",
        "    subset_size = min(100, len(test_dataset))\n",
        "    subset_indices = np.random.choice(len(test_dataset), subset_size, replace=False)\n",
        "\n",
        "    # Create a dataloader for the subset\n",
        "    batch_size = 16\n",
        "    subset_sampler = torch.utils.data.SubsetRandomSampler(subset_indices)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=subset_sampler)\n",
        "\n",
        "    # Move model to device and set to evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass (handle models with and without token_type_ids)\n",
        "            if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "                _, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "            else:\n",
        "                _, logits = model(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"label\"]\n",
        "                )\n",
        "\n",
        "            # Get predictions\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "            labels = batch[\"label\"].cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "            # Print some example predictions\n",
        "            if len(all_preds) <= 5:  # Just show the first 5 examples\n",
        "                for i in range(len(preds)):\n",
        "                    print(f\"\\nPrediction example {len(all_preds) - len(preds) + i + 1}:\")\n",
        "                    print(f\"  True label: {LABEL_MAP_REVERSE.get(labels[i], 'Unknown')}\")\n",
        "                    print(f\"  Predicted: {LABEL_MAP_REVERSE.get(preds[i], 'Unknown')}\")\n",
        "                    print(f\"  Probabilities: {probs[i].cpu().numpy().round(3)}\")\n",
        "\n",
        "    # Show prediction distribution\n",
        "    pred_counts = {}\n",
        "    for pred in all_preds:\n",
        "        if pred not in pred_counts:\n",
        "            pred_counts[pred] = 0\n",
        "        pred_counts[pred] += 1\n",
        "\n",
        "    print(\"\\nPrediction Distribution:\")\n",
        "    for pred, count in pred_counts.items():\n",
        "        pred_name = LABEL_MAP_REVERSE.get(pred, f\"Unknown ({pred})\")\n",
        "        print(f\"  {pred_name}: {count} predictions ({count/len(all_preds)*100:.2f}%)\")\n",
        "\n",
        "    # Show prediction vs actual distribution\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    labels = [LABEL_MAP_REVERSE.get(i, f\"Unknown\") for i in range(max(LABEL_MAP.values()) + 1)]\n",
        "\n",
        "    # Print simple text version of confusion matrix\n",
        "    print(\"True \\\\ Pred\", end=\"\")\n",
        "    for label in labels:\n",
        "        print(f\" | {label[:3]}\", end=\"\")\n",
        "    print()\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"{label[:7]}\", end=\"\")\n",
        "        for j in range(len(labels)):\n",
        "            if i < cm.shape[0] and j < cm.shape[1]:\n",
        "                print(f\" | {cm[i, j]:3d}\", end=\"\")\n",
        "            else:\n",
        "                print(\" |   0\", end=\"\")\n",
        "        print()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\nAccuracy on subset: {accuracy:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wiNF2khnY6TA"
      },
      "outputs": [],
      "source": [
        "def diagnose_gradient_flow(model, test_dataset):\n",
        "    print(\"\\n=== Gradient Flow Diagnosis ===\")\n",
        "\n",
        "    # Create a small subset of the dataset for quick testing\n",
        "    subset_size = min(32, len(test_dataset))\n",
        "    subset_indices = np.random.choice(len(test_dataset), subset_size, replace=False)\n",
        "\n",
        "    # Create a dataloader for the subset\n",
        "    batch_size = 16\n",
        "    subset_sampler = torch.utils.data.SubsetRandomSampler(subset_indices)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=subset_sampler)\n",
        "\n",
        "    # Move model to device and set to training mode\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=VERIFICATION_LEARNING_RATE)\n",
        "\n",
        "    # Get a batch\n",
        "    batch = next(iter(test_loader))\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    # Record initial parameter values\n",
        "    initial_params = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            initial_params[name] = param.clone().detach().cpu().numpy()\n",
        "            print(f\"  {name}: shape={param.shape}, mean={param.data.mean().item():.6f}, std={param.data.std().item():.6f}\")\n",
        "\n",
        "    # Forward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "        loss, logits = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            token_type_ids=batch[\"token_type_ids\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "    else:\n",
        "        loss, logits = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "\n",
        "    print(f\"\\nLoss: {loss.item():.4f}\")\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradients\n",
        "    print(\"\\nGradient Magnitudes:\")\n",
        "    zero_grad_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.grad is None:\n",
        "                zero_grad_params.append(name)\n",
        "                print(f\"  {name}: NO GRADIENT\")\n",
        "            else:\n",
        "                grad_mean = param.grad.abs().mean().item()\n",
        "                grad_max = param.grad.abs().max().item()\n",
        "                print(f\"  {name}: mean={grad_mean:.6f}, max={grad_max:.6f}\")\n",
        "\n",
        "    if zero_grad_params:\n",
        "        print(f\"\\nWARNING: {len(zero_grad_params)} parameters have no gradient!\")\n",
        "\n",
        "    # Apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # Check parameter changes\n",
        "    print(\"\\nParameter Changes After One Update:\")\n",
        "    unchanged_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            new_value = param.clone().detach().cpu().numpy()\n",
        "            old_value = initial_params[name]\n",
        "\n",
        "            # Calculate relative change\n",
        "            abs_diff = np.abs(new_value - old_value)\n",
        "            rel_change = np.mean(abs_diff) / (np.mean(np.abs(old_value)) + 1e-9)\n",
        "\n",
        "            change_description = f\"  {name}: relative change={rel_change:.6f}\"\n",
        "\n",
        "            if rel_change == 0:\n",
        "                unchanged_params.append(name)\n",
        "                change_description += \" (NO CHANGE)\"\n",
        "\n",
        "            print(change_description)\n",
        "\n",
        "    if unchanged_params:\n",
        "        print(f\"\\nWARNING: {len(unchanged_params)} parameters did not change after update!\")\n",
        "\n",
        "    return not (zero_grad_params or unchanged_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "85UC0VVyY6TA"
      },
      "outputs": [],
      "source": [
        "def diagnose_mixed_evidence(train_claims, evidence, retriever):\n",
        "    print(\"\\n=== Mixed Evidence Diagnosis ===\")\n",
        "\n",
        "    if retriever is None:\n",
        "        print(\"No retriever provided. Cannot diagnose mixed evidence functionality.\")\n",
        "        return False\n",
        "\n",
        "    # Test retrieval for a few claims\n",
        "    sample_size = 5\n",
        "    sample_claims = dict(list(train_claims.items())[:sample_size])\n",
        "\n",
        "    print(f\"Testing retrieval for {sample_size} sample claims...\")\n",
        "\n",
        "    for claim_id, claim_data in sample_claims.items():\n",
        "        print(f\"\\nClaim: {claim_data['claim_text']}\")\n",
        "\n",
        "        # Get ground truth evidence\n",
        "        if \"evidences\" in claim_data:\n",
        "            ground_truth = claim_data[\"evidences\"]\n",
        "            print(f\"Ground truth evidence count: {len(ground_truth)}\")\n",
        "\n",
        "            # Show first ground truth evidence\n",
        "            if ground_truth:\n",
        "                first_evidence_id = ground_truth[0]\n",
        "                if first_evidence_id in evidence:\n",
        "                    print(f\"Ground truth evidence sample: {evidence[first_evidence_id][:100]}...\")\n",
        "\n",
        "        # Retrieve evidence using retriever\n",
        "        try:\n",
        "            claim_text = claim_data[\"claim_text\"]\n",
        "            retrieved_evidence, scores = retriever.retrieve(claim_text)\n",
        "            print(f\"Retrieved evidence count: {len(retrieved_evidence)}\")\n",
        "\n",
        "            # Show first retrieved evidence\n",
        "            if retrieved_evidence:\n",
        "                first_retrieved_id = retrieved_evidence[0]\n",
        "                if first_retrieved_id in evidence:\n",
        "                    print(f\"Retrieved evidence sample: {evidence[first_retrieved_id][:100]}...\")\n",
        "                    print(f\"Retrieval score: {scores[0]:.4f}\")\n",
        "\n",
        "            # Check overlap with ground truth\n",
        "            if \"evidences\" in claim_data:\n",
        "                overlap = set(retrieved_evidence).intersection(set(ground_truth))\n",
        "                print(f\"Overlap with ground truth: {len(overlap)}/{len(ground_truth)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Create datasets with and without mixed evidence\n",
        "    tokenizer = AutoTokenizer.from_pretrained(VERIFICATION_MODEL_NAME)\n",
        "\n",
        "    # Ground truth only dataset\n",
        "    gt_dataset = ClaimVerificationDataset(\n",
        "        claims=sample_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=None,\n",
        "        mix_ratio=0.0\n",
        "    )\n",
        "\n",
        "    # Get retrieved evidence for sample claims\n",
        "    retrieved_evidence_dict = {}\n",
        "    for claim_id, claim_data in sample_claims.items():\n",
        "        claim_text = claim_data[\"claim_text\"]\n",
        "        evidence_ids, _ = retriever.retrieve(claim_text)\n",
        "        retrieved_evidence_dict[claim_id] = evidence_ids\n",
        "\n",
        "    # Mixed evidence dataset\n",
        "    mixed_dataset = ClaimVerificationDataset(\n",
        "        claims=sample_claims,\n",
        "        evidence=evidence,\n",
        "        tokenizer=tokenizer,\n",
        "        use_ground_truth=True,\n",
        "        retrieved_evidence=retrieved_evidence_dict,\n",
        "        mix_ratio=0.5\n",
        "    )\n",
        "\n",
        "    print(f\"\\nGround truth only dataset size: {len(gt_dataset)}\")\n",
        "    print(f\"Mixed evidence dataset size: {len(mixed_dataset)}\")\n",
        "\n",
        "    # Check if mixed dataset is actually larger\n",
        "    is_working = len(mixed_dataset) > len(gt_dataset)\n",
        "    print(f\"Mixed evidence functionality is {'working' if is_working else 'NOT working properly'}\")\n",
        "\n",
        "    return is_working"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdO00OOkY6TA"
      },
      "source": [
        "#### Diagnose Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "sPj1mdHBY6TA"
      },
      "outputs": [],
      "source": [
        "def run_full_diagnosis(train_claims, dev_claims, evidence, retriever=None):\n",
        "    \"\"\"\n",
        "    Run a full suite of diagnostic tests to identify model issues.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== STARTING FULL DIAGNOSTIC TESTS ===\\n\")\n",
        "\n",
        "    # Step 1: Diagnose the dataset\n",
        "    test_dataset = diagnose_dataset(train_claims, dev_claims, evidence)\n",
        "\n",
        "    # Step 2: Create and diagnose a new model\n",
        "    print(\"\\nInitializing a fresh model for testing...\")\n",
        "    model = ClaimVerificationModel(num_labels=VERIFICATION_NUM_LABELS, model_name=VERIFICATION_MODEL_NAME)\n",
        "    model.to(device)\n",
        "\n",
        "    # Step 3: Check gradient flow\n",
        "    gradient_flows = diagnose_gradient_flow(model, test_dataset)\n",
        "\n",
        "    # Step 4: Train for one epoch to capture initial predictions\n",
        "    print(\"\\nTraining for one epoch to check initial learning...\")\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=VERIFICATION_LEARNING_RATE)\n",
        "    train_loader = DataLoader(test_dataset, batch_size=VERIFICATION_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if 'token_type_ids' in batch and 'distilbert' not in VERIFICATION_MODEL_NAME.lower():\n",
        "            loss, _ = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "        else:\n",
        "            loss, _ = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "    avg_loss /= len(train_loader)\n",
        "    print(f\"Average loss after one epoch: {avg_loss:.4f}\")\n",
        "\n",
        "    # Step 5: Check model predictions\n",
        "    all_preds, all_labels = diagnose_model_predictions(model, test_dataset)\n",
        "\n",
        "    # Step 6: Check mixed evidence functionality if retriever is available\n",
        "    if retriever is not None:\n",
        "        mixed_evidence_works = diagnose_mixed_evidence(train_claims, evidence, retriever)\n",
        "    else:\n",
        "        print(\"\\nSkipping mixed evidence diagnosis (no retriever provided)\")\n",
        "        mixed_evidence_works = None\n",
        "\n",
        "    # Summarize findings\n",
        "    print(\"\\n=== DIAGNOSTIC SUMMARY ===\")\n",
        "\n",
        "    # Check for dataset imbalance\n",
        "    label_counts = {}\n",
        "    for label in all_labels:\n",
        "        if label not in label_counts:\n",
        "            label_counts[label] = 0\n",
        "        label_counts[label] += 1\n",
        "\n",
        "    most_common_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
        "    most_common_pct = label_counts[most_common_label] / len(all_labels) * 100\n",
        "\n",
        "    if most_common_pct > 50:\n",
        "        print(f\"✘ Dataset imbalance detected: {LABEL_MAP_REVERSE.get(most_common_label)} class dominates ({most_common_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"✓ Dataset balance looks reasonable (largest class: {most_common_pct:.1f}%)\")\n",
        "\n",
        "    # Check for prediction uniformity\n",
        "    pred_counts = {}\n",
        "    for pred in all_preds:\n",
        "        if pred not in pred_counts:\n",
        "            pred_counts[pred] = 0\n",
        "        pred_counts[pred] += 1\n",
        "\n",
        "    if len(pred_counts) < len(LABEL_MAP):\n",
        "        print(f\"✘ Model is not predicting all classes (only {len(pred_counts)} out of {len(LABEL_MAP)})\")\n",
        "    else:\n",
        "        print(f\"✓ Model is predicting all {len(LABEL_MAP)} classes\")\n",
        "\n",
        "    most_common_pred = max(pred_counts.items(), key=lambda x: x[1])[0]\n",
        "    most_common_pred_pct = pred_counts[most_common_pred] / len(all_preds) * 100\n",
        "\n",
        "    if most_common_pred_pct > 70:\n",
        "        print(f\"✘ Model is biased toward {LABEL_MAP_REVERSE.get(most_common_pred)} class ({most_common_pred_pct:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"✓ Model prediction distribution looks reasonable\")\n",
        "\n",
        "    # Check gradient flow\n",
        "    if gradient_flows:\n",
        "        print(\"✓ Gradient flow is working correctly\")\n",
        "    else:\n",
        "        print(\"✘ Gradient flow issues detected\")\n",
        "\n",
        "    # Check mixed evidence\n",
        "    if mixed_evidence_works is not None:\n",
        "        if mixed_evidence_works:\n",
        "            print(\"✓ Mixed evidence functionality is working correctly\")\n",
        "        else:\n",
        "            print(\"✘ Mixed evidence functionality is NOT working properly\")\n",
        "\n",
        "    print(\"\\n=== END OF DIAGNOSIS ===\")\n",
        "\n",
        "    return {\n",
        "        \"dataset_imbalance\": most_common_pct > 50,\n",
        "        \"prediction_bias\": most_common_pred_pct > 70,\n",
        "        \"gradient_flow_issues\": not gradient_flows,\n",
        "        \"mixed_evidence_issues\": False if mixed_evidence_works else True\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "umE-x2RNY6TA"
      },
      "outputs": [],
      "source": [
        "# # Run diagnostic tests\n",
        "# if __name__ == \"__main__\":\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import seaborn as sns\n",
        "#     from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "#     # Run full diagnosis\n",
        "#     if 'retrieval_results' not in locals() and os.path.exists(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt')):\n",
        "#         print(\"Loading saved retrieval results...\")\n",
        "#         retrieval_results = torch.load(os.path.join(DRIVE_DATA_PATH, 'retrieval_results.pt'),weights_only=True)\n",
        "#         retriever = retrieval_results.get(\"retriever\") if \"retriever\" in retrieval_results else None\n",
        "#     else:\n",
        "#         retriever = None\n",
        "\n",
        "#     diagnosis_results = run_full_diagnosis(train_claims, dev_claims, evidence, retriever)\n",
        "\n",
        "#     # Based on diagnosis, suggest fixes\n",
        "#     print(\"\\n=== RECOMMENDED FIXES ===\")\n",
        "\n",
        "#     if diagnosis_results[\"dataset_imbalance\"]:\n",
        "#         print(\"1. Apply class weighting by setting USE_CLASS_WEIGHTS = True\")\n",
        "#         print(\"2. Consider oversampling minority classes or downsampling majority classes\")\n",
        "\n",
        "#     if diagnosis_results[\"prediction_bias\"]:\n",
        "#         print(\"1. Reduce learning rate (try 5e-6 instead of 2e-5)\")\n",
        "#         print(\"2. Train for more epochs (5-10 instead of 3)\")\n",
        "#         print(\"3. Consider using focal loss instead of cross-entropy loss\")\n",
        "\n",
        "#     if diagnosis_results[\"gradient_flow_issues\"]:\n",
        "#         print(\"1. Check for NaN values in inputs or gradients\")\n",
        "#         print(\"2. Reduce batch size if you're encountering memory issues\")\n",
        "#         print(\"3. Consider gradient clipping to prevent exploding gradients\")\n",
        "\n",
        "#     if diagnosis_results[\"mixed_evidence_issues\"]:\n",
        "#         print(\"1. Fix the mixed evidence implementation\")\n",
        "#         print(\"2. Check the retriever functionality\")\n",
        "#         print(\"3. Ensure the mix_ratio is properly used in the dataset creation\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
